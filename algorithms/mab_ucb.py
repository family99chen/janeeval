import json
import math
import os
import random
import sys
import tempfile
from typing import Any, Dict, List, Optional, Tuple

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from mainfunction import evaluate_rag, evaluate_rag_multimodal

try:
    from tqdm import tqdm
except Exception:
    tqdm = None


def _load_yaml(path: str) -> Dict[str, Any]:
    if not os.path.isfile(path):
        return {}
    with open(path, "r", encoding="utf-8") as handle:
        data = handle.read().strip()
    if not data:
        return {}
    import yaml

    parsed = yaml.safe_load(data) or {}
    return parsed if isinstance(parsed, dict) else {}


def _dump_yaml(data: Dict[str, Any], path: str) -> None:
    import yaml

    with open(path, "w", encoding="utf-8") as handle:
        yaml.safe_dump(data, handle, sort_keys=False, allow_unicode=True)


def _allowed_values(node: Any) -> List[Any]:
    if not isinstance(node, dict):
        return []
    allowed = node.get("allowed")
    if not isinstance(allowed, list):
        return []
    return [v for v in allowed if v != "..."]


def _deep_update(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    merged = json.loads(json.dumps(base))
    for key, value in override.items():
        if isinstance(value, dict):
            current = merged.get(key)
            if not isinstance(current, dict):
                current = {}
            merged_child = _deep_update(current, value)
            if merged_child:
                merged[key] = merged_child
            else:
                merged.pop(key, None)
            continue
        if isinstance(value, list):
            continue
        merged[key] = value
    return merged


def _override_choices(
    module: str, key: str, algo_cfg: Dict[str, Any]
) -> Optional[List[Any]]:
    if not isinstance(algo_cfg, dict):
        return None
    section = algo_cfg.get(module)
    if not isinstance(section, dict) or key not in section:
        return None
    value = section.get(key)
    if isinstance(value, list):
        return value
    if value is None:
        return None
    return [value]


def _parse_score_weights(text: str) -> Optional[Dict[str, float]]:
    if not text:
        return None
    name_map = {
        "llmaaj": "LLMAAJ",
        "bertf1": "BERTScore-F1",
        "bert": "BERTScore-F1",
        "rougel": "ROUGE-L",
        "f1": "F1",
        "bleu": "BLEU",
        "exactmatch": "ExactMatch",
        "em": "ExactMatch",
    }
    weights: Dict[str, float] = {}
    for raw in text.split(","):
        part = raw.strip().lower()
        if not part:
            continue
        idx = len(part)
        while idx > 0 and (part[idx - 1].isdigit() or part[idx - 1] == "."):
            idx -= 1
        if idx == len(part):
            continue
        name = part[:idx]
        weight_str = part[idx:]
        metric_key = name_map.get(name)
        if not metric_key:
            continue
        try:
            weight = float(weight_str)
        except Exception:
            continue
        weights[metric_key] = weight
    return weights or None


def _score_from_report(
    report: Dict[str, Any],
    preferred: Optional[str],
    weights: Optional[Dict[str, float]],
) -> Tuple[str, float]:
    metrics = report.get("metrics") or {}
    if weights:
        total = 0.0
        denom = 0.0
        for key, weight in weights.items():
            if key not in metrics:
                continue
            try:
                total += float(metrics[key]) * weight
                denom += weight
            except Exception:
                continue
        return "weighted", (total / denom) if denom > 0 else 0.0
    if preferred and preferred in metrics:
        try:
            return preferred, float(metrics[preferred])
        except Exception:
            return preferred, 0.0
    for name in ("LLMAAJ", "BERTScore-F1", "ROUGE-L", "F1", "BLEU"):
        if name in metrics:
            try:
                return name, float(metrics[name])
            except Exception:
                return name, 0.0
    return "LLMAAJ", 0.0


def _write_temp_selection(selection: Dict[str, Any]) -> str:
    fd, path = tempfile.mkstemp(prefix="mab_ucb_selection_", suffix=".yaml")
    os.close(fd)
    _dump_yaml(selection, path)
    return path


def _is_multimodal(algo_cfg: Dict[str, Any]) -> bool:
    return isinstance(algo_cfg, dict) and "clip" in algo_cfg


def _module_forced_on(algo_cfg: Dict[str, Any], module: str) -> bool:
    if not isinstance(algo_cfg, dict):
        return False
    section = algo_cfg.get(module)
    return isinstance(section, dict) and len(section) > 0


def _random_selection(
    search_space: Dict[str, Any],
    algo_cfg: Dict[str, Any],
    rng: random.Random,
) -> Dict[str, Any]:
    selection: Dict[str, Any] = {}
    for module, params in search_space.items():
        if not isinstance(params, dict):
            continue
        is_optional = module in {"rewriter", "reranker", "pruner"}
        if is_optional and not _module_forced_on(algo_cfg, module) and rng.random() < 0.5:
            continue

        selection[module] = {}
        for key, value in params.items():
            choices = _allowed_values(value)
            override = _override_choices(module, key, algo_cfg)
            if override:
                choices = override
            if choices:
                selection[module][key] = rng.choice(choices)
        if not selection[module]:
            selection.pop(module, None)
    return selection


def _build_config_pool(
    search_space: Dict[str, Any],
    algo_cfg: Dict[str, Any],
    eval_metrics: Optional[Dict[str, Any]],
    pool_size: int,
    seed: int,
) -> List[Dict[str, Any]]:
    rng = random.Random(seed)
    pool: List[Dict[str, Any]] = []
    seen: set[str] = set()
    attempts = 0
    max_attempts = max(pool_size * 50, 100)
    while len(pool) < pool_size and attempts < max_attempts:
        attempts += 1
        candidate = _random_selection(search_space, algo_cfg, rng)
        if eval_metrics:
            candidate["eval_metrics"] = eval_metrics
        if algo_cfg:
            candidate = _deep_update(candidate, algo_cfg)
        key = json.dumps(candidate, sort_keys=True, ensure_ascii=False)
        if key in seen:
            continue
        seen.add(key)
        pool.append(candidate)
    return pool


def _evaluate_selection(
    qa_json_path: str,
    corpus_json_path: str,
    selection: Dict[str, Any],
    eval_mode: str,
    preferred_metric: Optional[str],
    score_weights: Optional[Dict[str, float]],
    eval_fn,
) -> Tuple[float, Dict[str, Any]]:
    selection_path = _write_temp_selection(selection)
    try:
        result = eval_fn(
            qa_json_path=qa_json_path,
            corpus_json_path=corpus_json_path,
            config_path=selection_path,
            eval_mode=eval_mode,
        )
    finally:
        os.remove(selection_path)
    report = result.get("eval_report") or {}
    metric_name, score = _score_from_report(report, preferred_metric, score_weights)
    return score, {
        "metric": metric_name,
        "score": score,
        "report": report,
        "outputs": result.get("outputs"),
        "error": result.get("error"),
        "errors": result.get("errors"),
    }


def mab_ucb_search(
    qa_json_path: str,
    corpus_json_path: str,
    template_path: str,
    algo_config_path: str,
    eval_mode: str,
    report_path: str,
    budget: int,
    pool_size: int,
    seed: int,
    score_weights: Optional[Dict[str, float]] = None,
) -> Dict[str, Any]:
    algo_cfg = _load_yaml(algo_config_path)
    use_multimodal = _is_multimodal(algo_cfg)
    eval_fn = evaluate_rag_multimodal if use_multimodal else evaluate_rag
    if use_multimodal:
        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
        template_path = os.path.join(base_dir, "config_multimodal.yaml")
    template = _load_yaml(template_path)
    search_space = template.get("rag_search_space") or {}
    eval_metrics = template.get("eval_metrics")
    preferred_metric = None
    if isinstance(algo_cfg, dict):
        preferred_metric = algo_cfg.get("score_metric") or algo_cfg.get("metric")

    configs = _build_config_pool(search_space, algo_cfg, eval_metrics, pool_size, seed)
    n_arms = len(configs)
    if n_arms == 0 or budget <= 0:
        return {"best_score": 0.0, "best_config": {}, "trials": [], "pool_size": 0}

    counts = [0] * n_arms
    sums = [0.0] * n_arms
    invalid = [False] * n_arms
    trials: List[Dict[str, Any]] = []
    best_score: float = float("-inf")
    best_config: Dict[str, Any] = {}

    def _write_report_snapshot() -> None:
        report_dir = os.path.dirname(report_path)
        if report_dir:
            os.makedirs(report_dir, exist_ok=True)
        snapshot = {
            "best_score": best_score if best_score != float("-inf") else 0.0,
            "best_config": best_config,
            "trials": trials,
            "pool_size": n_arms,
            "budget": budget,
        }
        with open(report_path, "w", encoding="utf-8") as handle:
            json.dump(snapshot, handle, ensure_ascii=False, indent=2)

    bar = tqdm(total=budget, desc="mab-ucb", unit="trial") if tqdm else None
    attempts = 0
    max_attempts = max(10, int(budget) * 50)

    i = 0
    while len(trials) < budget and i < n_arms and attempts < max_attempts:
        attempts += 1
        score, payload = _evaluate_selection(
            qa_json_path,
            corpus_json_path,
            configs[i],
            eval_mode,
            preferred_metric,
            score_weights,
            eval_fn,
        )
        if payload.get("error"):
            invalid[i] = True
            i += 1
            continue
        record = {
            "index": len(trials) + 1,
            "score": payload.get("score"),
            "metric": payload.get("metric"),
            "selection": configs[i],
            "report": payload.get("report"),
            "outputs": payload.get("outputs"),
            "error": payload.get("error"),
            "errors": payload.get("errors"),
        }
        trials.append(record)
        counts[i] += 1
        sums[i] += score
        if score >= best_score:
            best_score = score
            best_config = json.loads(json.dumps(configs[i]))
        _write_report_snapshot()
        if bar:
            bar.update(1)
        i += 1

    while len(trials) < budget and attempts < max_attempts:
        attempts += 1
        ucb_vals = []
        for i in range(n_arms):
            if invalid[i]:
                ucb_vals.append(float("-inf"))
            elif counts[i] == 0:
                ucb_vals.append(float("inf"))
            else:
                mean = sums[i] / counts[i]
                t = max(2, len(trials))
                bonus = math.sqrt(2.0 * math.log(t) / counts[i])
                ucb_vals.append(mean + bonus)

        if all(v == float("-inf") for v in ucb_vals):
            break

        arm = max(range(n_arms), key=lambda i: ucb_vals[i])
        score, payload = _evaluate_selection(
            qa_json_path,
            corpus_json_path,
            configs[arm],
            eval_mode,
            preferred_metric,
            score_weights,
            eval_fn,
        )
        if payload.get("error"):
            invalid[arm] = True
            continue
        record = {
            "index": len(trials) + 1,
            "score": payload.get("score"),
            "metric": payload.get("metric"),
            "selection": configs[arm],
            "report": payload.get("report"),
            "outputs": payload.get("outputs"),
            "error": payload.get("error"),
            "errors": payload.get("errors"),
        }
        trials.append(record)
        counts[arm] += 1
        sums[arm] += score
        if score >= best_score:
            best_score = score
            best_config = json.loads(json.dumps(configs[arm]))
        _write_report_snapshot()
        if bar:
            bar.update(1)

    if bar:
        bar.close()

    result = {
        "best_score": best_score if best_score != float("-inf") else 0.0,
        "best_config": best_config,
        "trials": trials,
        "pool_size": n_arms,
        "budget": budget,
    }
    _write_report_snapshot()
    return result


def main() -> None:
    import argparse

    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    default_template = os.path.join(base_dir, "config.yaml")
    default_algo_config = os.path.join(base_dir, "algorithms", "configforalgo.yaml")
    default_report = os.path.join(base_dir, "outputs", "mab_ucb_report.json")

    parser = argparse.ArgumentParser(description="MAB UCB1 search for RAG.")
    parser.add_argument("--qa_json", required=True, help="Path to QA JSON/JSONL.")
    parser.add_argument("--corpus_json", required=True, help="Path to corpus JSON.")
    parser.add_argument(
        "--config_yaml",
        default=default_template,
        help="Path to config search space template.",
    )
    parser.add_argument(
        "--algo_config_yaml",
        default=default_algo_config,
        help="Path to algo config with real model URLs/keys.",
    )
    parser.add_argument(
        "--eval_mode",
        default="both",
        choices=["avg", "per_item", "both"],
        help="Evaluation mode.",
    )
    parser.add_argument(
        "--report_path",
        default=default_report,
        help="Path to write report JSON.",
    )
    parser.add_argument(
        "--budget",
        type=int,
        default=20,
        help="Number of evaluations to run.",
    )
    parser.add_argument(
        "--pool_size",
        type=int,
        default=50,
        help="Number of candidate configs to build.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed.",
    )
    parser.add_argument(
        "--score_weights",
        default="",
        help="Weighted metrics, e.g. 'bertf11,llmaaj2'.",
    )
    args = parser.parse_args()

    score_weights = _parse_score_weights(args.score_weights)
    result = mab_ucb_search(
        qa_json_path=args.qa_json,
        corpus_json_path=args.corpus_json,
        template_path=args.config_yaml,
        algo_config_path=args.algo_config_yaml,
        eval_mode=args.eval_mode,
        report_path=args.report_path,
        budget=args.budget,
        pool_size=args.pool_size,
        seed=args.seed,
        score_weights=score_weights,
    )
    print("\n[summary] best_score:", result.get("best_score"))
    print("[summary] best_config:", json.dumps(result.get("best_config"), ensure_ascii=False))


if __name__ == "__main__":
    main()
