{
  "best_score": 0.05,
  "best_config": {
    "chunking": {
      "chunk_size": 2048,
      "chunk_overlap": 64
    },
    "retrieve": {
      "model_url": "/home/cz/ragsearch-jane/models/sentence-transformers--all-MiniLM-L6-v2",
      "topk": 3,
      "bm25_weight": 0.25
    },
    "reranker": {
      "model_url": "/home/cz/ragsearch-jane/models/cross-encoder--ms-marco-MiniLM-L-6-v2",
      "topk": 20
    },
    "pruner": {
      "model_url": "https://api.zhizengzeng.com/v1",
      "model_name": "gpt-4o-mini",
      "api_key": "sk-zk2a9670ffd66cd147a52dca250662f1334694f88bae25e4",
      "prompt_template_id": "3"
    },
    "generator": {
      "model_url": "https://api.zhizengzeng.com/v1",
      "model_name": "gpt-4o-mini",
      "api_key": "sk-zk2a9670ffd66cd147a52dca250662f1334694f88bae25e4"
    },
    "eval_metrics": {
      "llmaaj": {
        "model_url": "https://api.zhizengzeng.com/v1",
        "model_name": "gpt-4o-mini",
        "api_key": "sk-zk2a9670ffd66cd147a52dca250662f1334694f88bae25e4"
      },
      "bert": {
        "model_url": "/home/cz/ragsearch-jane/models/bert-base-uncased"
      }
    }
  },
  "trials": [
    {
      "stage": "rewriter:off",
      "score": 0.05,
      "metric": "LLMAAJ",
      "selection": {
        "chunking": {
          "chunk_size": 2048,
          "chunk_overlap": 64
        },
        "retrieve": {
          "model_url": "/home/cz/ragsearch-jane/models/sentence-transformers--all-MiniLM-L6-v2",
          "topk": 3,
          "bm25_weight": 0.25
        },
        "reranker": {
          "model_url": "/home/cz/ragsearch-jane/models/cross-encoder--ms-marco-MiniLM-L-6-v2",
          "topk": 20
        },
        "pruner": {
          "model_url": "https://api.zhizengzeng.com/v1",
          "model_name": "gpt-4o-mini",
          "api_key": "sk-zk2a9670ffd66cd147a52dca250662f1334694f88bae25e4",
          "prompt_template_id": "3"
        },
        "generator": {
          "model_url": "https://api.zhizengzeng.com/v1",
          "model_name": "gpt-4o-mini",
          "api_key": "sk-zk2a9670ffd66cd147a52dca250662f1334694f88bae25e4"
        },
        "eval_metrics": {
          "llmaaj": {
            "model_url": "https://api.zhizengzeng.com/v1",
            "model_name": "gpt-4o-mini",
            "api_key": "sk-zk2a9670ffd66cd147a52dca250662f1334694f88bae25e4"
          },
          "bert": {
            "model_url": "/home/cz/ragsearch-jane/models/bert-base-uncased"
          }
        }
      },
      "report": {
        "metrics": {
          "ExactMatch": 0.0,
          "F1": 0.04142838665150164,
          "ROUGE-L": 0.035134060410366895,
          "BLEU": 0.0035760535017951003,
          "BERTScore-F1": 0.3833897545933723,
          "LLMAAJ": 0.05
        },
        "per_item": [
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.31395888328552246
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.3519063889980316
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.1276595744680851,
            "ROUGE-L": 0.0851063829787234,
            "BLEU": 0.007434360163166208,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.5586894750595093
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.019323671497584537,
            "ROUGE-L": 0.019323671497584537,
            "BLEU": 0.0010872891856749099,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.4087343215942383
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.43265846371650696
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.34194329380989075
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.09090909090909091,
            "ROUGE-L": 0.09090909090909091,
            "BLEU": 0.013120543279239774,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.3507317900657654
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.1,
            "ROUGE-L": 0.1,
            "BLEU": 0.016847111051295396,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.3329399526119232
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.07017543859649122,
            "ROUGE-L": 0.07017543859649122,
            "BLEU": 0.004863083928997274,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.2814684808254242
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.31709104776382446
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.05333333333333334,
            "ROUGE-L": 0.05333333333333334,
            "BLEU": 0.0018980802981805563,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.4201585352420807
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.39385393261909485
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.04,
            "ROUGE-L": 0.04,
            "BLEU": 0.004392487796991638,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.44367799162864685
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.19444444444444445,
            "ROUGE-L": 0.11111111111111112,
            "BLEU": 0.006797010899515826,
            "LLMAAJ": 1.0,
            "BERTScore-F1": 0.5645115375518799
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.40562161803245544
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.05714285714285714,
            "ROUGE-L": 0.05714285714285714,
            "BLEU": 0.0038111203719291505,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.5416703820228577
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.3319658935070038
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.3496529161930084
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.5265601873397827
          },
          {
            "ExactMatch": 0.0,
            "F1": 0.0,
            "ROUGE-L": 0.0,
            "BLEU": 0.0,
            "LLMAAJ": 0.0,
            "BERTScore-F1": 0.0
          }
        ]
      },
      "outputs": [
        {
          "query": "What features are used?",
          "rewritten": "What features are used?",
          "retrieval": [
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__21",
              "document": "often guided by the task definition and the label types. For example, to identify stance towards rumors based on sequential annotations, an algorithm for learning from sequential BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single words, sequences of words, or information about their syntactic structure, to meta-information such as user or network information. Deciding on the features requires experimentation and expert insight and is often called feature engineering. For insight-driven analysis, we are often interested in why a prediction has been made and features that can be interpreted by humans may be preferred. Recent neural network approaches often use simple features as input (such as word embeddings or character sequences), which requires less feature engineering but make interpretation more difficult.\nSupervised models are powerful, but they can latch on to spurious features of the dataset. This is particularly true for datasets that are not well-balanced, and for annotations that are noisy. In our introductory example on hate speech in Reddit BIBREF0 , the annotations are automatically derived from the forum in which each post appears, and indeed, many of the posts in the forums (subreddits) that were banned by Reddit would be perceived by many as hate speech. But even in banned subreddits, not all of the content is hate speech (e.g., some of the top features were self-referential like the name of the subreddit) but a classifier would learn a high weight for these features.\nEven when expert annotations are available on the level of individual posts, spurious features may remain. BIBREF45 produced expert annotations of hate speech on Twitter. They found that one of the strongest features for sexism is the name of an Australian TV show, because people like to post sexist comments about the contestants. If we are trying to make claims about what i",
              "metadata": {
                "chunk_index": 21,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.9429622726221583
            },
            {
              "id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12__6",
              "document": "CREF4 ).\nSelecting Facebook pages\nAlthough page selection is a crucial ingredient of this approach, which we believe calls for further and deeper, dedicated investigation, for the experiments described here we took a rather simple approach. First, we selected the pages that would provide training data based on intuition and availability, then chose different combinations according to results of a basic model run on development data, and eventually tested feature combinations, still on the development set.\nFor the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation BIBREF13 on different combinations of pages. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset (Figure FIGREF9 ), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.\nFeatures\nIn selecting appropriate features, we mainly relied on previous work and intuition. We experimented with different combinations, and all tests were still done on Affective development, using the pages for the best model (B-M) described above as training data. Results are in Table TABREF20 . Future work will further explore the simultaneous selection of features and page combinations.\nWe use a set of basic text-based features to capture the emotion class. These include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, and features related to the presence of negation words, and to the usage of punctuation.\nThis feature is used in all unsupervised models as a source of information, and we mainly include it to assess its contribution, but eventually do not use it in our final model.\nWe used the NRC10 Lexicon because it performed best in the experiments by BIBREF10 , which is built around the emoti",
              "metadata": {
                "chunk_index": 6,
                "source_id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12"
              },
              "score": 0.8207281530636448
            },
            {
              "id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1__3",
              "document": "an be used for parameter estimation.\nIn the framework of GE, this term can be obtained by setting the constraint function $G({\\rm x}, y) = \\frac{1}{C_k} \\vec{I} (y)I(x_k)$ , where $\\vec{I}(y)$ is an indicator vector with 1 at the index corresponding to label $y$ and 0 elsewhere.\nRegularization Terms\nGE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. But as mentioned before, it is often the case that we are not able to provide enough knowledge for some of the classes. For the baseball-hockey classification task, as shown before, GE-FL will predict most of the instances as baseball. In this section, we will show three terms to make the model more robust.\nNeutral features are features that are not informative indicator of any classes, for instance, word player to the baseball-hockey classification task. Such features are usually frequent words across all categories. When we set the preference distribution of the neutral features to be uniform distributed, these neutral features will prevent the model from biasing to the class that has a dominate number of labeled features.\nFormally, given a set of neutral features $K^{^{\\prime }}$ , the uniform distribution is $\\hat{p}_u(y|x_k) = \\frac{1}{|C|}, k \\in K^{^{\\prime }}$ , where $|C|$ is the number of classes. The objective function with the new term becomes\n$$\\mathcal {O}_{NE} = \\mathcal {O} + \\sum _{k \\in K^{^{\\prime }}} KL(\\hat{p}_u(y|x_k) || p_\\theta (y | x_k)).$$   (Eq. 9)\nNote that we do not need manual annotation to provide neutral features. One simple way is to take the most common features as neutral features. Experimental results show that this strategy works successfully.\nAnother way to prevent the model from drifting from the desired direction is to constrain the predicted class distribution on unlabeled data. When lacking knowledge about the class distribution of the data, one feasible",
              "metadata": {
                "chunk_index": 3,
                "source_id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1"
              },
              "score": 0.6323916142843486
            }
          ],
          "reranked": [
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__21",
              "document": "often guided by the task definition and the label types. For example, to identify stance towards rumors based on sequential annotations, an algorithm for learning from sequential BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single words, sequences of words, or information about their syntactic structure, to meta-information such as user or network information. Deciding on the features requires experimentation and expert insight and is often called feature engineering. For insight-driven analysis, we are often interested in why a prediction has been made and features that can be interpreted by humans may be preferred. Recent neural network approaches often use simple features as input (such as word embeddings or character sequences), which requires less feature engineering but make interpretation more difficult.\nSupervised models are powerful, but they can latch on to spurious features of the dataset. This is particularly true for datasets that are not well-balanced, and for annotations that are noisy. In our introductory example on hate speech in Reddit BIBREF0 , the annotations are automatically derived from the forum in which each post appears, and indeed, many of the posts in the forums (subreddits) that were banned by Reddit would be perceived by many as hate speech. But even in banned subreddits, not all of the content is hate speech (e.g., some of the top features were self-referential like the name of the subreddit) but a classifier would learn a high weight for these features.\nEven when expert annotations are available on the level of individual posts, spurious features may remain. BIBREF45 produced expert annotations of hate speech on Twitter. They found that one of the strongest features for sexism is the name of an Australian TV show, because people like to post sexist comments about the contestants. If we are trying to make claims about what i",
              "metadata": {
                "chunk_index": 21,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.9429622726221583
            },
            {
              "id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12__6",
              "document": "CREF4 ).\nSelecting Facebook pages\nAlthough page selection is a crucial ingredient of this approach, which we believe calls for further and deeper, dedicated investigation, for the experiments described here we took a rather simple approach. First, we selected the pages that would provide training data based on intuition and availability, then chose different combinations according to results of a basic model run on development data, and eventually tested feature combinations, still on the development set.\nFor the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation BIBREF13 on different combinations of pages. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset (Figure FIGREF9 ), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.\nFeatures\nIn selecting appropriate features, we mainly relied on previous work and intuition. We experimented with different combinations, and all tests were still done on Affective development, using the pages for the best model (B-M) described above as training data. Results are in Table TABREF20 . Future work will further explore the simultaneous selection of features and page combinations.\nWe use a set of basic text-based features to capture the emotion class. These include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, and features related to the presence of negation words, and to the usage of punctuation.\nThis feature is used in all unsupervised models as a source of information, and we mainly include it to assess its contribution, but eventually do not use it in our final model.\nWe used the NRC10 Lexicon because it performed best in the experiments by BIBREF10 , which is built around the emoti",
              "metadata": {
                "chunk_index": 6,
                "source_id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12"
              },
              "score": 0.8207281530636448
            },
            {
              "id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1__3",
              "document": "an be used for parameter estimation.\nIn the framework of GE, this term can be obtained by setting the constraint function $G({\\rm x}, y) = \\frac{1}{C_k} \\vec{I} (y)I(x_k)$ , where $\\vec{I}(y)$ is an indicator vector with 1 at the index corresponding to label $y$ and 0 elsewhere.\nRegularization Terms\nGE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. But as mentioned before, it is often the case that we are not able to provide enough knowledge for some of the classes. For the baseball-hockey classification task, as shown before, GE-FL will predict most of the instances as baseball. In this section, we will show three terms to make the model more robust.\nNeutral features are features that are not informative indicator of any classes, for instance, word player to the baseball-hockey classification task. Such features are usually frequent words across all categories. When we set the preference distribution of the neutral features to be uniform distributed, these neutral features will prevent the model from biasing to the class that has a dominate number of labeled features.\nFormally, given a set of neutral features $K^{^{\\prime }}$ , the uniform distribution is $\\hat{p}_u(y|x_k) = \\frac{1}{|C|}, k \\in K^{^{\\prime }}$ , where $|C|$ is the number of classes. The objective function with the new term becomes\n$$\\mathcal {O}_{NE} = \\mathcal {O} + \\sum _{k \\in K^{^{\\prime }}} KL(\\hat{p}_u(y|x_k) || p_\\theta (y | x_k)).$$   (Eq. 9)\nNote that we do not need manual annotation to provide neutral features. One simple way is to take the most common features as neutral features. Experimental results show that this strategy works successfully.\nAnother way to prevent the model from drifting from the desired direction is to constrain the predicted class distribution on unlabeled data. When lacking knowledge about the class distribution of the data, one feasible",
              "metadata": {
                "chunk_index": 3,
                "source_id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1"
              },
              "score": 0.6323916142843486
            }
          ],
          "pruned_text": "The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single words, sequences of words, or information about their syntactic structure, to meta-information such as user or network information. Recent neural network approaches often use simple features as input (such as word embeddings or character sequences). \n\nWe use a set of basic text-based features to capture the emotion class. These include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, and features related to the presence of negation words, and to the usage of punctuation. \n\nWe used the NRC10 Lexicon because it performed best in the experiments. \n\nNeutral features are features that are not informative indicators of any classes, for instance, word player to the baseball-hockey classification task. Such features are usually frequent words across all categories. One simple way is to take the most common features as neutral features.",
          "answer": "The features used include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, features related to the presence of negation words, and features related to the usage of punctuation. Additionally, the NRC10 Lexicon was used because it performed best in the experiments."
        },
        {
          "query": "Do they report results only on English data?",
          "rewritten": "Do they report results only on English data?",
          "retrieval": [
            {
              "id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc__3",
              "document": "n the lower half of Table TABREF8, the results are obtained by the translated training data. First, we found that when testing on English and Chinese, translation always degrades the performance (En v.s. En-XX, Zh v.s. Zh-XX). Even though we translate the training data into the same language as testing data, using the untranslated data still yield better results. For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality of data. There are some exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Machine Translation\nTable TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. Because the above statement is true across all the languages, it is a strong evidence that translation degrades the performance.We notice that the translated corpus and untranslated corpus are not the same. This may be another factor that influences the results. Conducting an experiment between un-translated and back-translated data may deal with this problem.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Other Factors\nHere we discuss the case that the training data are translated. We consider each result is affected by at least three factors: (1) training corpus, (2) data size, (3) whether",
              "metadata": {
                "chunk_index": 3,
                "source_id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc"
              },
              "score": 0.8891940378300698
            },
            {
              "id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e__4",
              "document": "sian (“ellipsis (VP)” in Table TABREF15).\nLexical cohesion The test set focuses on reiteration of named entities. Where several translations of a named entity are possible, a model has to prefer consistent translations over inconsistent ones.\nExperimental Setup ::: Data preprocessing\nWe use the publicly available OpenSubtitles2018 corpus BIBREF12 for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by BIBREF11. Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least $0.9$.\nWe gathered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report are for the model trained on all 30m fragments.\nWe use the tokenization provided by the corpus and use multi-bleu.perl on lowercased data to compute BLEU score. We use beam search with a beam of 4.\nSentences were encoded using byte-pair encoding BIBREF13, with source and target vocabularies of about 32000 tokens. Translation pairs were batched together by approximate sequence length. Each training batch contained a set of translation pairs containing approximately 15000 source tokens. It has been shown that Transformer's performance depends heavily on batch size BIBREF14, and we chose a large batch size to ensure the best performance. In training context-aware models, for early stopping we use both convergence in BLEU score on the general development set and scores on the consistency development sets. After training, we average the 5 latest checkpoints.\nExperimental Setup ::: Models\nThe baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models BIBREF15. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of inpu",
              "metadata": {
                "chunk_index": 4,
                "source_id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e"
              },
              "score": 0.6796905846671674
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__21",
              "document": "ived from Wikipedia and Common Crawl corpora. While many concept pairs are direct or approximate translations of English pairs, we can see that the frequency distribution does vary across different languages, and is also related to inherent language properties. For instance, in Finnish and Russian, while we use infinitive forms of all verbs, conjugated verb inflections are often more frequent in raw corpora than the corresponding infinitive forms. The variance can also be partially explained by the difference in monolingual corpora size used to derive the frequency rankings in the first place: absolute vocabulary sizes are expected to fluctuate across different languages. However, it is also important to note that the datasets also contain subsets of lower-frequency and rare words, which can be used for rare word evaluations in multiple languages, in the spirit of Pilehvar:2018emnlp's English rare word dataset.\nCross-Linguistic Differences. Table TABREF23 shows some examples of average similarity scores of English, Spanish, Kiswahili and Welsh concept pairs. Remember that the scores range from 0 to 6: the higher the score, the more similar the participants found the concepts in the pair. The examples from Table TABREF23 show evidence of both the stability of average similarity scores across languages (unlikely – friendly, book – literature, and vanish – disappear), as well as language-specific differences (care – caution). Some differences in similarity scores seem to group languages into clusters. For example, the word pair regular – average has an average similarity score of 4.0 and 4.1 in English and Spanish, respectively, whereas in Kiswahili and Welsh the average similarity score of this pair is 0.5 and 0.8. We analyze this phenomenon in more detail in §SECREF25.\nThere are also examples for each of the four languages having a notably higher or lower similarity score for the same concept pair than the three other languages. For example, large – big in English has an average similarity score of 5.9, whereas Sp",
              "metadata": {
                "chunk_index": 21,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.6315549202222248
            }
          ],
          "reranked": [
            {
              "id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc__3",
              "document": "n the lower half of Table TABREF8, the results are obtained by the translated training data. First, we found that when testing on English and Chinese, translation always degrades the performance (En v.s. En-XX, Zh v.s. Zh-XX). Even though we translate the training data into the same language as testing data, using the untranslated data still yield better results. For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality of data. There are some exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Machine Translation\nTable TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. Because the above statement is true across all the languages, it is a strong evidence that translation degrades the performance.We notice that the translated corpus and untranslated corpus are not the same. This may be another factor that influences the results. Conducting an experiment between un-translated and back-translated data may deal with this problem.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Other Factors\nHere we discuss the case that the training data are translated. We consider each result is affected by at least three factors: (1) training corpus, (2) data size, (3) whether",
              "metadata": {
                "chunk_index": 3,
                "source_id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc"
              },
              "score": 0.8891940378300698
            },
            {
              "id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e__4",
              "document": "sian (“ellipsis (VP)” in Table TABREF15).\nLexical cohesion The test set focuses on reiteration of named entities. Where several translations of a named entity are possible, a model has to prefer consistent translations over inconsistent ones.\nExperimental Setup ::: Data preprocessing\nWe use the publicly available OpenSubtitles2018 corpus BIBREF12 for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by BIBREF11. Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least $0.9$.\nWe gathered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report are for the model trained on all 30m fragments.\nWe use the tokenization provided by the corpus and use multi-bleu.perl on lowercased data to compute BLEU score. We use beam search with a beam of 4.\nSentences were encoded using byte-pair encoding BIBREF13, with source and target vocabularies of about 32000 tokens. Translation pairs were batched together by approximate sequence length. Each training batch contained a set of translation pairs containing approximately 15000 source tokens. It has been shown that Transformer's performance depends heavily on batch size BIBREF14, and we chose a large batch size to ensure the best performance. In training context-aware models, for early stopping we use both convergence in BLEU score on the general development set and scores on the consistency development sets. After training, we average the 5 latest checkpoints.\nExperimental Setup ::: Models\nThe baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models BIBREF15. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of inpu",
              "metadata": {
                "chunk_index": 4,
                "source_id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e"
              },
              "score": 0.6796905846671674
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__21",
              "document": "ived from Wikipedia and Common Crawl corpora. While many concept pairs are direct or approximate translations of English pairs, we can see that the frequency distribution does vary across different languages, and is also related to inherent language properties. For instance, in Finnish and Russian, while we use infinitive forms of all verbs, conjugated verb inflections are often more frequent in raw corpora than the corresponding infinitive forms. The variance can also be partially explained by the difference in monolingual corpora size used to derive the frequency rankings in the first place: absolute vocabulary sizes are expected to fluctuate across different languages. However, it is also important to note that the datasets also contain subsets of lower-frequency and rare words, which can be used for rare word evaluations in multiple languages, in the spirit of Pilehvar:2018emnlp's English rare word dataset.\nCross-Linguistic Differences. Table TABREF23 shows some examples of average similarity scores of English, Spanish, Kiswahili and Welsh concept pairs. Remember that the scores range from 0 to 6: the higher the score, the more similar the participants found the concepts in the pair. The examples from Table TABREF23 show evidence of both the stability of average similarity scores across languages (unlikely – friendly, book – literature, and vanish – disappear), as well as language-specific differences (care – caution). Some differences in similarity scores seem to group languages into clusters. For example, the word pair regular – average has an average similarity score of 4.0 and 4.1 in English and Spanish, respectively, whereas in Kiswahili and Welsh the average similarity score of this pair is 0.5 and 0.8. We analyze this phenomenon in more detail in §SECREF25.\nThere are also examples for each of the four languages having a notably higher or lower similarity score for the same concept pair than the three other languages. For example, large – big in English has an average similarity score of 5.9, whereas Sp",
              "metadata": {
                "chunk_index": 21,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.6315549202222248
            }
          ],
          "pruned_text": "In the lower half of Table TABREF8, the results are obtained by the translated training data. Translation always degrades the performance (En v.s. En-XX, Zh v.s. Zh-XX). When testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality of data. There are exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improves performance on Korean. The English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not. \n\nTable TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. The translated corpus and untranslated corpus are not the same, which may influence the results. \n\nWe use the publicly available OpenSubtitles2018 corpus for English and Russian. Our MT system is trained on 6m instances. We gathered 30m groups of 4 consecutive sentences as our monolingual data. The main results we report are for the model trained on all 30m fragments. \n\nThe baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. The number of layers is $N=6$ with $h = 8$ parallel attention layers. \n\nTable TABREF23 shows some examples of average similarity scores of English, Spanish, Kiswahili and Welsh concept pairs. The scores range from 0 to 6: the higher the score, the more similar the participants found the concepts in the pair. Some differences in similarity scores seem to group languages into clusters.",
          "answer": "No, they do not report results only on English data. They also report results for other languages such as Chinese, Korean, Spanish, Kiswahili, and Welsh."
        },
        {
          "query": "What type of evaluation is proposed for this task?",
          "rewritten": "What type of evaluation is proposed for this task?",
          "retrieval": [
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__14",
              "document": "tent may produce disfluent or ungrammatical output. Specifically, we followed the Best-Worst Scaling BIBREF34 method where participants were presented with the output of two systems (and the original document) and asked to decide which one was better according to the criteria of Informativeness, Fluency, and Succinctness.\nBoth types of evaluation were conducted on the Amazon Mechanical Turk platform. For the CNN/DailyMail and NYT datasets we used the same documents (20 in total) and questions from previous work BIBREF8, BIBREF18. For XSum, we randomly selected 20 documents (and their questions) from the release of BIBREF22. We elicited 3 responses per HIT. With regard to QA evaluation, we adopted the scoring mechanism from BIBREF33; correct answers were marked with a score of one, partially correct answers with 0.5, and zero otherwise. For quality-based evaluation, the rating of each system was computed as the percentage of times it was chosen as better minus the times it was selected as worse. Ratings thus range from -1 (worst) to 1 (best).\nResults for extractive and abstractive systems are shown in Tables TABREF37 and TABREF38, respectively. We compared the best performing BertSum model in each setting (extractive or abstractive) against various state-of-the-art systems (whose output is publicly available), the Lead baseline, and the Gold standard as an upper bound. As shown in both tables participants overwhelmingly prefer the output of our model against comparison systems across datasets and evaluation paradigms. All differences between BertSum and comparison models are statistically significant ($p<0.05$), with the exception of TConvS2S (see Table TABREF38; XSum) in the QA evaluation setting.\nConclusions\nIn this paper, we showcased how pretrained Bert can be usefully applied in text summarization. We introduced a novel document-level encoder and proposed a general framework for both abstractive and extractive summarization. Experimental results across three datasets show that our model achieves state-of-the-",
              "metadata": {
                "chunk_index": 14,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.9524248495172718
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__25",
              "document": "del building. Applying this circular, iterative process to 450 18th-century novels written in three languages, Piper was able to uncover a new form of “conversional novel” that was not previously captured in “literary history's received critical categories” BIBREF48 .\nAlong similar lines, we can subject both the machine-generated output and the human annotations to another round of content validation. That is, take a stratified random sample, selecting observations from the full range of scores, and ask: Do these make sense in light of the systematized concept? If not, what seems to be missing? Or is something extraneous being captured? This is primarily a qualitative process that requires returning to theory and interrogating the systematized concept, indicators, and scores together. This type of validation is rarely done in NLP, but it is especially important when it is difficult to assess what drives a given machine learning model. If there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted.\nOther types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth.\nBesides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power ",
              "metadata": {
                "chunk_index": 25,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7569126146939921
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__2",
              "document": ", the extraction model presented in that work is limited to the closed-domain setting of basketball game tables and summaries. In particular, they assume that each table has exactly the same set of attributes for each entity, and that the entities can be identified in the text via string matching. These assumptions are not valid for the open-domain WikiBio dataset, and hence we train our own extraction model to replicate their evaluation scheme.\nOur extraction system is a pointer-generator network BIBREF19 , which learns to produce a linearized version of the table from the text. The network learns which attributes need to be populated in the output table, along with their values. It is trained on the training set of WikiBio. At test time we parsed the output strings into a set of (attribute, value) tuples and compare it to the ground truth table. The F-score of this text-to-table system was INLINEFORM0 , which is comparable to other challenging open-domain settings BIBREF20 . More details are included in the Appendix SECREF52 .\nGiven this information extraction system, we consider the following metrics for evaluation, along the lines of BIBREF1 . Content Selection (CS): F-score for the (attribute, value) pairs extracted from the generated text compared to those extracted from the reference. Relation Generation (RG): Precision for the (attribute, value) pairs extracted from the generated text compared to those in the ground truth table. RG-F: Since our task emphasizes the recall of information from the table as well, we consider another variant which computes the F-score of the extracted pairs to those in the table. We omit the content ordering metric, since our extraction system does not align records to the input text.\nExperiments & Results\nIn this section we compare several automatic evaluation metrics by checking their correlation with the scores assigned by humans to table-to-text models. Specifically, given INLINEFORM0 models INLINEFORM1 , and their outputs on an evaluation set, we show these generated text",
              "metadata": {
                "chunk_index": 2,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.7326159633353356
            }
          ],
          "reranked": [
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__14",
              "document": "tent may produce disfluent or ungrammatical output. Specifically, we followed the Best-Worst Scaling BIBREF34 method where participants were presented with the output of two systems (and the original document) and asked to decide which one was better according to the criteria of Informativeness, Fluency, and Succinctness.\nBoth types of evaluation were conducted on the Amazon Mechanical Turk platform. For the CNN/DailyMail and NYT datasets we used the same documents (20 in total) and questions from previous work BIBREF8, BIBREF18. For XSum, we randomly selected 20 documents (and their questions) from the release of BIBREF22. We elicited 3 responses per HIT. With regard to QA evaluation, we adopted the scoring mechanism from BIBREF33; correct answers were marked with a score of one, partially correct answers with 0.5, and zero otherwise. For quality-based evaluation, the rating of each system was computed as the percentage of times it was chosen as better minus the times it was selected as worse. Ratings thus range from -1 (worst) to 1 (best).\nResults for extractive and abstractive systems are shown in Tables TABREF37 and TABREF38, respectively. We compared the best performing BertSum model in each setting (extractive or abstractive) against various state-of-the-art systems (whose output is publicly available), the Lead baseline, and the Gold standard as an upper bound. As shown in both tables participants overwhelmingly prefer the output of our model against comparison systems across datasets and evaluation paradigms. All differences between BertSum and comparison models are statistically significant ($p<0.05$), with the exception of TConvS2S (see Table TABREF38; XSum) in the QA evaluation setting.\nConclusions\nIn this paper, we showcased how pretrained Bert can be usefully applied in text summarization. We introduced a novel document-level encoder and proposed a general framework for both abstractive and extractive summarization. Experimental results across three datasets show that our model achieves state-of-the-",
              "metadata": {
                "chunk_index": 14,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.9524248495172718
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__25",
              "document": "del building. Applying this circular, iterative process to 450 18th-century novels written in three languages, Piper was able to uncover a new form of “conversional novel” that was not previously captured in “literary history's received critical categories” BIBREF48 .\nAlong similar lines, we can subject both the machine-generated output and the human annotations to another round of content validation. That is, take a stratified random sample, selecting observations from the full range of scores, and ask: Do these make sense in light of the systematized concept? If not, what seems to be missing? Or is something extraneous being captured? This is primarily a qualitative process that requires returning to theory and interrogating the systematized concept, indicators, and scores together. This type of validation is rarely done in NLP, but it is especially important when it is difficult to assess what drives a given machine learning model. If there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted.\nOther types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth.\nBesides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power ",
              "metadata": {
                "chunk_index": 25,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7569126146939921
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__2",
              "document": ", the extraction model presented in that work is limited to the closed-domain setting of basketball game tables and summaries. In particular, they assume that each table has exactly the same set of attributes for each entity, and that the entities can be identified in the text via string matching. These assumptions are not valid for the open-domain WikiBio dataset, and hence we train our own extraction model to replicate their evaluation scheme.\nOur extraction system is a pointer-generator network BIBREF19 , which learns to produce a linearized version of the table from the text. The network learns which attributes need to be populated in the output table, along with their values. It is trained on the training set of WikiBio. At test time we parsed the output strings into a set of (attribute, value) tuples and compare it to the ground truth table. The F-score of this text-to-table system was INLINEFORM0 , which is comparable to other challenging open-domain settings BIBREF20 . More details are included in the Appendix SECREF52 .\nGiven this information extraction system, we consider the following metrics for evaluation, along the lines of BIBREF1 . Content Selection (CS): F-score for the (attribute, value) pairs extracted from the generated text compared to those extracted from the reference. Relation Generation (RG): Precision for the (attribute, value) pairs extracted from the generated text compared to those in the ground truth table. RG-F: Since our task emphasizes the recall of information from the table as well, we consider another variant which computes the F-score of the extracted pairs to those in the table. We omit the content ordering metric, since our extraction system does not align records to the input text.\nExperiments & Results\nIn this section we compare several automatic evaluation metrics by checking their correlation with the scores assigned by humans to table-to-text models. Specifically, given INLINEFORM0 models INLINEFORM1 , and their outputs on an evaluation set, we show these generated text",
              "metadata": {
                "chunk_index": 2,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.7326159633353356
            }
          ],
          "pruned_text": "We followed the Best-Worst Scaling method where participants were presented with the output of two systems and asked to decide which one was better according to the criteria of Informativeness, Fluency, and Succinctness. Both types of evaluation were conducted on the Amazon Mechanical Turk platform. We elicited 3 responses per HIT. For QA evaluation, correct answers were marked with a score of one, partially correct answers with 0.5, and zero otherwise. For quality-based evaluation, the rating of each system was computed as the percentage of times it was chosen as better minus the times it was selected as worse. Ratings thus range from -1 (worst) to 1 (best). We compared the best performing BertSum model against various state-of-the-art systems, the Lead baseline, and the Gold standard. All differences between BertSum and comparison models are statistically significant ($p<0.05$), with the exception of TConvS2S in the QA evaluation setting. \n\nWe can subject both the machine-generated output and the human annotations to another round of content validation. This type of validation is rarely done in NLP, but it is especially important when it is difficult to assess what drives a given machine learning model. Other types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth. \n\nWe consider the following metrics for evaluation: Content Selection (CS): F-score for the (attribute, value) pairs extracted from the generated text compared to those extracted from the reference. Relation Generation (RG): Precision for the (attribute, value) pairs extracted from the generated text compared to those in the ground truth table. RG-F: Another variant which computes the F-score of the extracted pairs to those in the table. \n\nIn this section we compare several automatic evaluation metrics by checking their correlation with the scores assigned by humans to table-to-text models.",
          "answer": "The proposed type of evaluation for this task includes Best-Worst Scaling, QA evaluation, and quality-based evaluation. Additionally, it mentions content validation and various metrics such as Content Selection (CS) and Relation Generation (RG)."
        },
        {
          "query": "what were the baselines?",
          "rewritten": "what were the baselines?",
          "retrieval": [
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__7",
              "document": "ce pair was judged by 3 independent workers, and we used the majority vote as the label for that pair. Overall, only INLINEFORM0 of the references were labeled as entailed by the table. Fleiss' INLINEFORM1 was INLINEFORM2 , which indicates a fair agreement. We found the workers sometimes disagreed on what information can be reasonably entailed by the table.\nFigure FIGREF40 shows the correlations as we vary the percent of entailed examples in the evaluation set of WikiBio. Each point is obtained by fixing the desired proportion of entailed examples, and sampling subsets from the full set which satisfy this proportion. PARENT and RG-F remain stable and show a high correlation across the entire range, whereas BLEU and BLEU-T vary a lot. In the hyperparams category, the latter two have the worst correlation when the evaluation set contains only entailed examples, which may seem surprising. However, on closer examination we found that this subset tends to omit a lot of information from the tables. Systems which produce more information than these references are penalized by BLEU, but not in the human evaluation. PARENT overcomes this issue by measuring recall against the table in addition to the reference.\nWe check how different components in the computation of PARENT contribute to its correlation to human judgments. Specifically, we remove the probability INLINEFORM0 of an n-gram INLINEFORM1 being entailed by the table from Eqs. EQREF19 and EQREF23 . The average correlation for PARENT-W drops to INLINEFORM5 in this case. We also try a variant of PARENT with INLINEFORM6 , which removes the contribution of Table Recall (Eq. EQREF22 ). The average correlation is INLINEFORM7 in this case. With these components, the correlation is INLINEFORM8 , showing that they are crucial to the performance of PARENT.\nBIBREF28 point out that hill-climbing on an automatic metric is meaningless if that metric has a low instance-level correlation to human judgments. In Table TABREF46 we show the average accuracy of the metrics in making th",
              "metadata": {
                "chunk_index": 7,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8236891426401063
            },
            {
              "id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a__1",
              "document": " variance in results. Multi-task learning, paired with multilingual training and subsequent monolingual finetuning, scored highest for five out of seven languages, improving accuracy by another 9.86% on average.\nSystem Description\nOur system is a modification of the provided CoNLL–SIGMORPHON 2018 baseline system, so we begin this section with a reiteration of the baseline system architecture, followed by a description of the three augmentations we introduce.\nBaseline\nThe CoNLL–SIGMORPHON 2018 baseline is described as follows:\nThe system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.\nTo that we add a few details regarding model size and training schedule:\nthe number of LSTM layers is one;\nembedding size, LSTM layer size and attention layer size is 100;\nmodels are trained for 20 epochs;\non every epoch, training data is subsampled at a rate of 0.3;\nLSTM dropout is applied at a rate 0.3;\ncontext word forms are randomly dropped at a rate of 0.1;\nthe Adam optimiser is used, with a default learning rate of 0.001; and\ntrained models are evaluated on the development data (the data for the shared task comes already split in train and dev sets).\nOur system\nHere we compare and contrast our system to the baseline system. A diagram of our system is shown in Figure FIGREF4 .\n",
              "metadata": {
                "chunk_index": 1,
                "source_id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a"
              },
              "score": 0.7505329065631146
            },
            {
              "id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e__7",
              "document": " translations: baseline context-agnostic one and the one corrected by the DocRepair model. Translations were presented in random order with no indication which model they came from. The task is to pick one of the three options: (1) the first translation is better, (2) the second translation is better, (3) the translations are of equal quality. The annotators were asked to avoid the third answer if they are able to give preference to one of the translations. No other guidelines were given.\nThe results are provided in Table TABREF30. In about $52\\%$ of the cases annotators marked translations as having equal quality. Among the cases where one of the translations was marked better than the other, the DocRepair translation was marked better in $73\\%$ of the cases. This shows a strong preference of the annotators for corrected translations over the baseline ones.\nVarying Training Data\nIn this section, we discuss the influence of the training data chosen for document-level models. In all experiments, we used the DocRepair model.\nVarying Training Data ::: The amount of training data\nTable TABREF33 provides BLEU and consistency scores for the DocRepair model trained on different amount of data. We see that even when using a dataset of moderate size (e.g., 5m fragments) we can achieve performance comparable to the model trained on a large amount of data (30m fragments). Moreover, we notice that deixis scores are less sensitive to the amount of training data than lexical cohesion and ellipsis scores. The reason might be that, as we observed in our previous work BIBREF11, inconsistencies in translations due to the presence of deictic words and phrases are more frequent in this dataset than other types of inconsistencies. Also, as we show in Section SECREF7, this is the phenomenon the model learns faster in training.\nVarying Training Data ::: One-way vs round-trip translations\nIn this section, we discuss the limitations of using only monolingual data to model inconsistencies between sentence-level translations. In Section SE",
              "metadata": {
                "chunk_index": 7,
                "source_id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e"
              },
              "score": 0.7117218891141112
            }
          ],
          "reranked": [
            {
              "id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a__1",
              "document": " variance in results. Multi-task learning, paired with multilingual training and subsequent monolingual finetuning, scored highest for five out of seven languages, improving accuracy by another 9.86% on average.\nSystem Description\nOur system is a modification of the provided CoNLL–SIGMORPHON 2018 baseline system, so we begin this section with a reiteration of the baseline system architecture, followed by a description of the three augmentations we introduce.\nBaseline\nThe CoNLL–SIGMORPHON 2018 baseline is described as follows:\nThe system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.\nTo that we add a few details regarding model size and training schedule:\nthe number of LSTM layers is one;\nembedding size, LSTM layer size and attention layer size is 100;\nmodels are trained for 20 epochs;\non every epoch, training data is subsampled at a rate of 0.3;\nLSTM dropout is applied at a rate 0.3;\ncontext word forms are randomly dropped at a rate of 0.1;\nthe Adam optimiser is used, with a default learning rate of 0.001; and\ntrained models are evaluated on the development data (the data for the shared task comes already split in train and dev sets).\nOur system\nHere we compare and contrast our system to the baseline system. A diagram of our system is shown in Figure FIGREF4 .\n",
              "metadata": {
                "chunk_index": 1,
                "source_id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a"
              },
              "score": 0.7505329065631146
            },
            {
              "id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e__7",
              "document": " translations: baseline context-agnostic one and the one corrected by the DocRepair model. Translations were presented in random order with no indication which model they came from. The task is to pick one of the three options: (1) the first translation is better, (2) the second translation is better, (3) the translations are of equal quality. The annotators were asked to avoid the third answer if they are able to give preference to one of the translations. No other guidelines were given.\nThe results are provided in Table TABREF30. In about $52\\%$ of the cases annotators marked translations as having equal quality. Among the cases where one of the translations was marked better than the other, the DocRepair translation was marked better in $73\\%$ of the cases. This shows a strong preference of the annotators for corrected translations over the baseline ones.\nVarying Training Data\nIn this section, we discuss the influence of the training data chosen for document-level models. In all experiments, we used the DocRepair model.\nVarying Training Data ::: The amount of training data\nTable TABREF33 provides BLEU and consistency scores for the DocRepair model trained on different amount of data. We see that even when using a dataset of moderate size (e.g., 5m fragments) we can achieve performance comparable to the model trained on a large amount of data (30m fragments). Moreover, we notice that deixis scores are less sensitive to the amount of training data than lexical cohesion and ellipsis scores. The reason might be that, as we observed in our previous work BIBREF11, inconsistencies in translations due to the presence of deictic words and phrases are more frequent in this dataset than other types of inconsistencies. Also, as we show in Section SECREF7, this is the phenomenon the model learns faster in training.\nVarying Training Data ::: One-way vs round-trip translations\nIn this section, we discuss the limitations of using only monolingual data to model inconsistencies between sentence-level translations. In Section SE",
              "metadata": {
                "chunk_index": 7,
                "source_id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e"
              },
              "score": 0.7117218891141112
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__7",
              "document": "ce pair was judged by 3 independent workers, and we used the majority vote as the label for that pair. Overall, only INLINEFORM0 of the references were labeled as entailed by the table. Fleiss' INLINEFORM1 was INLINEFORM2 , which indicates a fair agreement. We found the workers sometimes disagreed on what information can be reasonably entailed by the table.\nFigure FIGREF40 shows the correlations as we vary the percent of entailed examples in the evaluation set of WikiBio. Each point is obtained by fixing the desired proportion of entailed examples, and sampling subsets from the full set which satisfy this proportion. PARENT and RG-F remain stable and show a high correlation across the entire range, whereas BLEU and BLEU-T vary a lot. In the hyperparams category, the latter two have the worst correlation when the evaluation set contains only entailed examples, which may seem surprising. However, on closer examination we found that this subset tends to omit a lot of information from the tables. Systems which produce more information than these references are penalized by BLEU, but not in the human evaluation. PARENT overcomes this issue by measuring recall against the table in addition to the reference.\nWe check how different components in the computation of PARENT contribute to its correlation to human judgments. Specifically, we remove the probability INLINEFORM0 of an n-gram INLINEFORM1 being entailed by the table from Eqs. EQREF19 and EQREF23 . The average correlation for PARENT-W drops to INLINEFORM5 in this case. We also try a variant of PARENT with INLINEFORM6 , which removes the contribution of Table Recall (Eq. EQREF22 ). The average correlation is INLINEFORM7 in this case. With these components, the correlation is INLINEFORM8 , showing that they are crucial to the performance of PARENT.\nBIBREF28 point out that hill-climbing on an automatic metric is meaningless if that metric has a low instance-level correlation to human judgments. In Table TABREF46 we show the average accuracy of the metrics in making th",
              "metadata": {
                "chunk_index": 7,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8236891426401063
            }
          ],
          "pruned_text": "The CoNLL–SIGMORPHON 2018 baseline is described as follows: The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma. The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism. The number of LSTM layers is one; embedding size, LSTM layer size and attention layer size is 100; models are trained for 20 epochs; on every epoch, training data is subsampled at a rate of 0.3; LSTM dropout is applied at a rate 0.3; context word forms are randomly dropped at a rate of 0.1; the Adam optimiser is used, with a default learning rate of 0.001; and trained models are evaluated on the development data.",
          "answer": "The baselines for the CoNLL–SIGMORPHON 2018 are described as follows: \n\n1. **Track 1**: The system uses an encoder-decoder on character sequences, taking a lemma as input and generating a word form. It considers the lemma, word form, and the morphological feature (MSD) of the previous and following words as context. \n\n2. **Track 2**: The system only considers the word forms of the previous and next words as context.\n\nThe baseline system concatenates embeddings for context word forms, lemmas, and MSDs into a context vector. It computes character embeddings for each character in the input lemma, concatenates these with a copy of the context vector, and encodes the resulting sequence of vectors using an LSTM encoder. An LSTM decoder then generates the characters in the output word form using encoder states and an attention mechanism.\n\nAdditional details include:\n- One LSTM layer\n- Embedding size, LSTM layer size, and attention layer size of 100\n- Models trained for 20 epochs\n- Training data subsampled at a rate of 0.3\n- LSTM dropout applied at a rate of 0.3\n- Context word forms randomly dropped at a rate of 0.1\n- Adam optimizer with a default learning rate of 0.001\n- Models evaluated on development data."
        },
        {
          "query": "What background do they have?",
          "rewritten": "What background do they have?",
          "retrieval": [
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__8",
              "document": "place. They might tell us: Is this the central archive, or a peripheral one? What makes it unusual? Or they might tell us how certain underrepresented communities use a social media platform and advise us on strategies for ensuring our data collection includes their perspectives.\nHowever, when it is practically infeasible to navigate the data in this way—for instance, when we cannot determine what is missing from Twitter's Streaming API or what webpages are left out of the Internet Archive—we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.\nCompiling data\nAfter identifying the data source(s), the next step is compiling the data. This step is fundamental: if the sources cannot support a convincing result, no result will be convincing. In many cases, this involves defining a “core\" set of documents and a “comparison\" set. We often have a specific set of documents in mind: an author's work, a particular journal, a time period. But if we want to say that this “core\" set has some distinctive property, we need a “comparison\" set. Expanding the collection beyond the documents that we would immediately think of has the beneficial effect of increasing our sample size. Having more sources increases the chance that we will notice something consistent across many individually varying contexts.\nComparing sets of documents can sometimes support causal inference, presented as a contrast between a treatment group and a control. In BIBREF0 , the treatment consisted of the text written in the two forums that were eventually closed by Reddit. However, identifying a control group required a considerable amount of time and effort. Reddit is a diverse platform, with a wide variety of interactional and linguistic styles; it would be pointless to compare hate speech forums against forums dedicated to, say, pictures of wrecked bicycles. Chandrase",
              "metadata": {
                "chunk_index": 8,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.9295875739641062
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__5",
              "document": "We can go to the canonical archive or open up something that nobody has studied before. For example, we might focus on major historical moments (French Revolution, post-Milosevic Serbia) or critical epochs (Britain entering the Victorian era, the transition from Latin to proto-Romance). Or, we could look for records of how people conducted science, wrote and consumed literature, and worked out their philosophies.\nA growing number of researchers work with born-digital sources or data. Born-digital data, e.g., from social media, generally do not involve direct elicitation from participants and therefore enable unobtrusive measurements BIBREF5 , BIBREF6 . In contrast, methods like surveys sometimes elicit altered responses from participants, who might adapt their responses to what they think is expected. Moreover, born-digital data is often massive, enabling large-scale studies of language and behavior in a variety of social contexts.\nStill, many scholars in the social sciences and humanities work with multiple data sources. The variety of sources typically used means that more than one data collection method is often required. For example, a project examining coverage of a UK General Election, could draw data from traditional media, web archives, Twitter and Facebook, campaign manifestos, etc. and might combine textual analysis of these materials with surveys, laboratory experiments, or field observations offline. In contrast, many computational studies based on born-digital data have focused on one specific source, such as Twitter.\nThe use of born-digital data raises ethical concerns. Although early studies often treated privacy as a binary construct, many now acknowledge its complexity BIBREF7 . Conversations on private matters can be posted online, visible for all, but social norms regarding what should be considered public information may differ from the data's explicit visibility settings. Often no informed consent has been obtained, raising concerns and challenges regarding publishing content and potentially ",
              "metadata": {
                "chunk_index": 5,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7970309751391784
            },
            {
              "id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12__6",
              "document": "CREF4 ).\nSelecting Facebook pages\nAlthough page selection is a crucial ingredient of this approach, which we believe calls for further and deeper, dedicated investigation, for the experiments described here we took a rather simple approach. First, we selected the pages that would provide training data based on intuition and availability, then chose different combinations according to results of a basic model run on development data, and eventually tested feature combinations, still on the development set.\nFor the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation BIBREF13 on different combinations of pages. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset (Figure FIGREF9 ), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.\nFeatures\nIn selecting appropriate features, we mainly relied on previous work and intuition. We experimented with different combinations, and all tests were still done on Affective development, using the pages for the best model (B-M) described above as training data. Results are in Table TABREF20 . Future work will further explore the simultaneous selection of features and page combinations.\nWe use a set of basic text-based features to capture the emotion class. These include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, and features related to the presence of negation words, and to the usage of punctuation.\nThis feature is used in all unsupervised models as a source of information, and we mainly include it to assess its contribution, but eventually do not use it in our final model.\nWe used the NRC10 Lexicon because it performed best in the experiments by BIBREF10 , which is built around the emoti",
              "metadata": {
                "chunk_index": 6,
                "source_id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12"
              },
              "score": 0.7562465131070067
            }
          ],
          "reranked": [
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__5",
              "document": "We can go to the canonical archive or open up something that nobody has studied before. For example, we might focus on major historical moments (French Revolution, post-Milosevic Serbia) or critical epochs (Britain entering the Victorian era, the transition from Latin to proto-Romance). Or, we could look for records of how people conducted science, wrote and consumed literature, and worked out their philosophies.\nA growing number of researchers work with born-digital sources or data. Born-digital data, e.g., from social media, generally do not involve direct elicitation from participants and therefore enable unobtrusive measurements BIBREF5 , BIBREF6 . In contrast, methods like surveys sometimes elicit altered responses from participants, who might adapt their responses to what they think is expected. Moreover, born-digital data is often massive, enabling large-scale studies of language and behavior in a variety of social contexts.\nStill, many scholars in the social sciences and humanities work with multiple data sources. The variety of sources typically used means that more than one data collection method is often required. For example, a project examining coverage of a UK General Election, could draw data from traditional media, web archives, Twitter and Facebook, campaign manifestos, etc. and might combine textual analysis of these materials with surveys, laboratory experiments, or field observations offline. In contrast, many computational studies based on born-digital data have focused on one specific source, such as Twitter.\nThe use of born-digital data raises ethical concerns. Although early studies often treated privacy as a binary construct, many now acknowledge its complexity BIBREF7 . Conversations on private matters can be posted online, visible for all, but social norms regarding what should be considered public information may differ from the data's explicit visibility settings. Often no informed consent has been obtained, raising concerns and challenges regarding publishing content and potentially ",
              "metadata": {
                "chunk_index": 5,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7970309751391784
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__8",
              "document": "place. They might tell us: Is this the central archive, or a peripheral one? What makes it unusual? Or they might tell us how certain underrepresented communities use a social media platform and advise us on strategies for ensuring our data collection includes their perspectives.\nHowever, when it is practically infeasible to navigate the data in this way—for instance, when we cannot determine what is missing from Twitter's Streaming API or what webpages are left out of the Internet Archive—we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.\nCompiling data\nAfter identifying the data source(s), the next step is compiling the data. This step is fundamental: if the sources cannot support a convincing result, no result will be convincing. In many cases, this involves defining a “core\" set of documents and a “comparison\" set. We often have a specific set of documents in mind: an author's work, a particular journal, a time period. But if we want to say that this “core\" set has some distinctive property, we need a “comparison\" set. Expanding the collection beyond the documents that we would immediately think of has the beneficial effect of increasing our sample size. Having more sources increases the chance that we will notice something consistent across many individually varying contexts.\nComparing sets of documents can sometimes support causal inference, presented as a contrast between a treatment group and a control. In BIBREF0 , the treatment consisted of the text written in the two forums that were eventually closed by Reddit. However, identifying a control group required a considerable amount of time and effort. Reddit is a diverse platform, with a wide variety of interactional and linguistic styles; it would be pointless to compare hate speech forums against forums dedicated to, say, pictures of wrecked bicycles. Chandrase",
              "metadata": {
                "chunk_index": 8,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.9295875739641062
            },
            {
              "id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12__6",
              "document": "CREF4 ).\nSelecting Facebook pages\nAlthough page selection is a crucial ingredient of this approach, which we believe calls for further and deeper, dedicated investigation, for the experiments described here we took a rather simple approach. First, we selected the pages that would provide training data based on intuition and availability, then chose different combinations according to results of a basic model run on development data, and eventually tested feature combinations, still on the development set.\nFor the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation BIBREF13 on different combinations of pages. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset (Figure FIGREF9 ), we selected a best model (B-M), namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data. Time and The Guardian perform well on most emotions but Disney helps to boost the performance for the Joy class.\nFeatures\nIn selecting appropriate features, we mainly relied on previous work and intuition. We experimented with different combinations, and all tests were still done on Affective development, using the pages for the best model (B-M) described above as training data. Results are in Table TABREF20 . Future work will further explore the simultaneous selection of features and page combinations.\nWe use a set of basic text-based features to capture the emotion class. These include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, and features related to the presence of negation words, and to the usage of punctuation.\nThis feature is used in all unsupervised models as a source of information, and we mainly include it to assess its contribution, but eventually do not use it in our final model.\nWe used the NRC10 Lexicon because it performed best in the experiments by BIBREF10 , which is built around the emoti",
              "metadata": {
                "chunk_index": 6,
                "source_id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12"
              },
              "score": 0.7562465131070067
            }
          ],
          "pruned_text": "We can go to the canonical archive or open up something that nobody has studied before. A growing number of researchers work with born-digital sources or data. Born-digital data, e.g., from social media, generally do not involve direct elicitation from participants and therefore enable unobtrusive measurements. In contrast, methods like surveys sometimes elicit altered responses from participants. Moreover, born-digital data is often massive, enabling large-scale studies of language and behavior in a variety of social contexts.\n\nStill, many scholars in the social sciences and humanities work with multiple data sources. The variety of sources typically used means that more than one data collection method is often required. For example, a project examining coverage of a UK General Election could draw data from traditional media, web archives, Twitter and Facebook, campaign manifestos, etc. and might combine textual analysis of these materials with surveys, laboratory experiments, or field observations offline.\n\nThe use of born-digital data raises ethical concerns. Although early studies often treated privacy as a binary construct, many now acknowledge its complexity. Conversations on private matters can be posted online, visible for all, but social norms regarding what should be considered public information may differ from the data's explicit visibility settings. Often no informed consent has been obtained, raising concerns and challenges regarding publishing content.\n\nHowever, when it is practically infeasible to navigate the data in this way, we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.\n\nAfter identifying the data source(s), the next step is compiling the data. This step is fundamental: if the sources cannot support a convincing result, no result will be convincing. In many cases, this involves defining a “core\" set of documents and a “comparison\" set. We often have a specific set of documents in mind: an author's work, a particular journal, a time period. But if we want to say that this “core\" set has some distinctive property, we need a “comparison\" set. \n\nComparing sets of documents can sometimes support causal inference, presented as a contrast between a treatment group and a control. Selecting Facebook pages is a crucial ingredient of this approach. For the experiments described here we took a rather simple approach. First, we selected the pages that would provide training data based on intuition and availability, then chose different combinations according to results of a basic model run on development data.\n\nFor the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters. Based on results of the attempted combinations as well as on the distribution of emotions in the development dataset, we selected a best model, namely the combined set of Time, The Guardian and Disney, which yields the highest results on development data.\n\nIn selecting appropriate features, we mainly relied on previous work and intuition. We experimented with different combinations, and all tests were still done on Affective development, using the pages for the best model as training data. We use a set of basic text-based features to capture the emotion class. These include a tf-idf bag-of-words feature, word (2-3) and character (2-5) ngrams, and features related to the presence of negation words, and to the usage of punctuation.",
          "answer": "I do not know."
        },
        {
          "query": "by how much did their model improve?",
          "rewritten": "by how much did their model improve?",
          "retrieval": [
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__4",
              "document": "tegory produce high quality fluent texts, and differ primarily on the quantity and accuracy of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance. This is an important use-case as metrics are often used to tune hyperparameters of a model.\nHuman Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24 . However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0 , for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22 , which assigns a score to each model based on how many times it was preferred over an alternative.\nThe data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.\nCompared Metrics\nText only: We compare BLEU BIBREF8 , ROUGE BIBREF9 , METEOR BIBREF18 , CIDEr and CIDEr-D BIBREF25 using their publicly available implementations.\nInformation Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4 .\nT",
              "metadata": {
                "chunk_index": 4,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8759912114155036
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__26",
              "document": ", it can be more useful to focus on improving explanatory power than making small improvements in predictive performance.\nAnalysis\nIn this phase, we use our models to explore or answer our research questions. For example, given a topic model we can look at the connection between topics and metadata elements. Tags such as “hate speech\" or metadata information imply a certain way of organizing the collection. Computational models provide another organization, which may differ in ways that provide more insight into how these categories manifest themselves, or fail to do so.\nMoreover, when using a supervised approach, the “errors”, i.e. disagreement between the system output and human-provided labels, can point towards interesting cases for closer analysis and help us reflect on our conceptualizations. In the words of BIBREF2 , they can be “opportunities for interpretation”. Other types of “failures” can be insightful as well. Sometimes there is a “dog that didn't bark” BIBREF49 –i.e., something that everyone thinks we should have found, but we did not. Or, sometimes the failures are telling us about the existence of something in the data that nobody noticed, or thought important, until then (e.g., the large number of travel journals in Darwin's reading lists).\nComputational text analysis is not a replacement for but rather an addition to the approaches one can take to analyze social and cultural phenomena using textual data. By moving back and forth between large-scale computational analyses and small-scale qualitative analyses, we can combine their strengths so that we can identify large-scale and long-term trends, but also tell individual stories. For example, the Reddit study on hate speech BIBREF0 raised various follow-up questions: Can we distinguish hate speech from people talking about hate speech? Did people find new ways to express hate speech? If so, did the total amount of online hate speech decrease after all? As possible next steps, a qualitative discourse analyst might examine a smaller corpus to inves",
              "metadata": {
                "chunk_index": 26,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7614169588175116
            },
            {
              "id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc__3",
              "document": "n the lower half of Table TABREF8, the results are obtained by the translated training data. First, we found that when testing on English and Chinese, translation always degrades the performance (En v.s. En-XX, Zh v.s. Zh-XX). Even though we translate the training data into the same language as testing data, using the untranslated data still yield better results. For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality of data. There are some exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Machine Translation\nTable TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. Because the above statement is true across all the languages, it is a strong evidence that translation degrades the performance.We notice that the translated corpus and untranslated corpus are not the same. This may be another factor that influences the results. Conducting an experiment between un-translated and back-translated data may deal with this problem.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Other Factors\nHere we discuss the case that the training data are translated. We consider each result is affected by at least three factors: (1) training corpus, (2) data size, (3) whether",
              "metadata": {
                "chunk_index": 3,
                "source_id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc"
              },
              "score": 0.7218258691385036
            }
          ],
          "reranked": [
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__26",
              "document": ", it can be more useful to focus on improving explanatory power than making small improvements in predictive performance.\nAnalysis\nIn this phase, we use our models to explore or answer our research questions. For example, given a topic model we can look at the connection between topics and metadata elements. Tags such as “hate speech\" or metadata information imply a certain way of organizing the collection. Computational models provide another organization, which may differ in ways that provide more insight into how these categories manifest themselves, or fail to do so.\nMoreover, when using a supervised approach, the “errors”, i.e. disagreement between the system output and human-provided labels, can point towards interesting cases for closer analysis and help us reflect on our conceptualizations. In the words of BIBREF2 , they can be “opportunities for interpretation”. Other types of “failures” can be insightful as well. Sometimes there is a “dog that didn't bark” BIBREF49 –i.e., something that everyone thinks we should have found, but we did not. Or, sometimes the failures are telling us about the existence of something in the data that nobody noticed, or thought important, until then (e.g., the large number of travel journals in Darwin's reading lists).\nComputational text analysis is not a replacement for but rather an addition to the approaches one can take to analyze social and cultural phenomena using textual data. By moving back and forth between large-scale computational analyses and small-scale qualitative analyses, we can combine their strengths so that we can identify large-scale and long-term trends, but also tell individual stories. For example, the Reddit study on hate speech BIBREF0 raised various follow-up questions: Can we distinguish hate speech from people talking about hate speech? Did people find new ways to express hate speech? If so, did the total amount of online hate speech decrease after all? As possible next steps, a qualitative discourse analyst might examine a smaller corpus to inves",
              "metadata": {
                "chunk_index": 26,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7614169588175116
            },
            {
              "id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc__3",
              "document": "n the lower half of Table TABREF8, the results are obtained by the translated training data. First, we found that when testing on English and Chinese, translation always degrades the performance (En v.s. En-XX, Zh v.s. Zh-XX). Even though we translate the training data into the same language as testing data, using the untranslated data still yield better results. For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality of data. There are some exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Machine Translation\nTable TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. Because the above statement is true across all the languages, it is a strong evidence that translation degrades the performance.We notice that the translated corpus and untranslated corpus are not the same. This may be another factor that influences the results. Conducting an experiment between un-translated and back-translated data may deal with this problem.\nZero-shot Transfer with Multi-BERT ::: Discussion ::: The Effect of Other Factors\nHere we discuss the case that the training data are translated. We consider each result is affected by at least three factors: (1) training corpus, (2) data size, (3) whether",
              "metadata": {
                "chunk_index": 3,
                "source_id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc"
              },
              "score": 0.7218258691385036
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__4",
              "document": "tegory produce high quality fluent texts, and differ primarily on the quantity and accuracy of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance. This is an important use-case as metrics are often used to tune hyperparameters of a model.\nHuman Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24 . However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0 , for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22 , which assigns a score to each model based on how many times it was preferred over an alternative.\nThe data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.\nCompared Metrics\nText only: We compare BLEU BIBREF8 , ROUGE BIBREF9 , METEOR BIBREF18 , CIDEr and CIDEr-D BIBREF25 using their publicly available implementations.\nInformation Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4 .\nT",
              "metadata": {
                "chunk_index": 4,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8759912114155036
            }
          ],
          "pruned_text": "In the lower half of Table TABREF8, the results are obtained by the translated training data. First, we found that when testing on English and Chinese, translation always degrades the performance (En v.s. En-XX, Zh v.s. Zh-XX). Even though we translate the training data into the same language as testing data, using the untranslated data still yield better results. For example, when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8, while the F1 score is only 44.1 for the model training on Zh-En. This shows that translation degrades the quality of data. There are some exceptions when testing on Korean. Translating the English training data into Chinese, Japanese and Korean still improve the performance on Korean. We also found that when translated into the same language, the English training data is always better than the Chinese data (En-XX v.s. Zh-XX), with only one exception (En-Fr v.s. Zh-Fr when testing on KorQuAD). This may be because we have less Chinese training data than English. These results show that the quality and the size of dataset are much more important than whether the training and testing are in the same language or not.\n\nTable TABREF8 shows that fine-tuning on un-translated target language data achieves much better performance than data translated into the target language. Because the above statement is true across all the languages, it is a strong evidence that translation degrades the performance. We notice that the translated corpus and untranslated corpus are not the same. This may be another factor that influences the results. Conducting an experiment between un-translated and back-translated data may deal with this problem.\n\nHuman Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Paired comparisons have been shown to be superior to rating scales for comparing generated texts. However, for measuring correlation the comparisons need to be aggregated into real-valued scores for each of the models. For this, we use Thurstone's method, which assigns a score to each model based on how many times it was preferred over an alternative.\n\nThe data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance. In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.",
          "answer": "I do not know."
        },
        {
          "query": "which chinese datasets were used?",
          "rewritten": "which chinese datasets were used?",
          "retrieval": [
            {
              "id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a__8",
              "document": " entity types. We use the standard train/dev/test split of CoNLL2012 shared task.\nChinese MSRA performs as a Chinese benchmark dataset containing 3 entity types. Data in MSRA is collected from news domain. Since the development set is not provided in the original MSRA dataset, we randomly split the training set into training and development splits by 9:1. We use the official test set for evaluation.\nChinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain, which has 18 entity types. In this paper, we take the same data split as wu2019glyce did.\nExperiments ::: Named Entity Recognition ::: Baselines\nWe use the following baselines:\nELMo: a tagging model from peters2018deep.\nLattice-LSTM: lattice2018zhang constructs a word-character lattice, only used in Chinese datasets.\nCVT: from kevin2018cross, which uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.\nBert-Tagger: devlin2018bert treats NER as a tagging task.\nGlyce-BERT: wu2019glyce combines glyph information with BERT pretraining.\nBERT-MRC: The current SOTA model for both Chinese and English NER datasets proposed by xiaoya2019ner, which formulate NER as machine reading comprehension task.\nExperiments ::: Named Entity Recognition ::: Results\nTable shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.\nExperiments ::: Machine Reading Comprehension\nMachine reading comprehension (MRC) BIBREF39, BIBREF40, BIBREF41, BIBREF40, BIBREF42, BIBREF15 has become a central task in natural language understanding. MRC in the SQuAD-style is to predict the answer span in the passage given a question and the passage. In this paper, we choose the SQuAD-style MRC task",
              "metadata": {
                "chunk_index": 8,
                "source_id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a"
              },
              "score": 0.8701809068834786
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__15",
              "document": "he language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family). Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex. We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project.\nThe work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.\nMulti-SimLex: Translation and Annotation ::: Word Pair Translation\nTranslators for each target language were instructed to find direct or approximate translations for the 1,888 word pairs that satisfy the following rules. (1) All pairs in the translated set must be unique (i.e., no duplicate pairs); (2) Translating two words from the same English pair into the same word in the target language is not allowed (e.g., it is not allowed to translate car and automobile to the same Spanish word coche). (3) The translated pairs must preserve the semantic relations between the two words when possible. This means that, when multiple translations are possible, the translation that best conveys the semantic relation between the two words found in the original English pair is selected. (4) If it is not possible to use a single-word translation in the target language, then a multi-word expression (MWE) can be us",
              "metadata": {
                "chunk_index": 15,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.6713421014803385
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__1",
              "document": "tics (see §SECREF2), but has also been shown to benefit a range of language understanding tasks in NLP. Examples include dialog state tracking BIBREF16, BIBREF17, spoken language understanding BIBREF18, BIBREF19, text simplification BIBREF20, BIBREF21, BIBREF22, dictionary and thesaurus construction BIBREF23, BIBREF24.\nDespite the proven usefulness of semantic similarity datasets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian BIBREF25, whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish BIBREF26, 500 in Farsi BIBREF27, or 300 in Finnish BIBREF28) and coverage (e.g., all datasets which originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity dataset spanning 1,888 concept pairs (see §SECREF4).\nMost importantly, semantic similarity datasets in different languages have been created using heterogeneous construction procedures with different guidelines for translation and annotation, as well as different rating scales. For instance, some datasets were obtained by directly translating the English SimLex-999 in its entirety BIBREF25, BIBREF16 or in part BIBREF28. Other datasets were created from scratch BIBREF26 and yet others sampled English concept pairs differently from SimLex-999 and then translated and reannotated them in target languages BIBREF27. This heterogeneity makes these datasets incomparable and precludes systematic cross-linguistic analyses. In this article, consolidating the lessons learned from previous dataset construction paradigms, we propose a carefully designed translation and annotation protocol for developing monolingual Multi-SimLex datasets with aligned concept pairs for typologically diverse languages. We apply this ",
              "metadata": {
                "chunk_index": 1,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.655289007432375
            }
          ],
          "reranked": [
            {
              "id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a__8",
              "document": " entity types. We use the standard train/dev/test split of CoNLL2012 shared task.\nChinese MSRA performs as a Chinese benchmark dataset containing 3 entity types. Data in MSRA is collected from news domain. Since the development set is not provided in the original MSRA dataset, we randomly split the training set into training and development splits by 9:1. We use the official test set for evaluation.\nChinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain, which has 18 entity types. In this paper, we take the same data split as wu2019glyce did.\nExperiments ::: Named Entity Recognition ::: Baselines\nWe use the following baselines:\nELMo: a tagging model from peters2018deep.\nLattice-LSTM: lattice2018zhang constructs a word-character lattice, only used in Chinese datasets.\nCVT: from kevin2018cross, which uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.\nBert-Tagger: devlin2018bert treats NER as a tagging task.\nGlyce-BERT: wu2019glyce combines glyph information with BERT pretraining.\nBERT-MRC: The current SOTA model for both Chinese and English NER datasets proposed by xiaoya2019ner, which formulate NER as machine reading comprehension task.\nExperiments ::: Named Entity Recognition ::: Results\nTable shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.\nExperiments ::: Machine Reading Comprehension\nMachine reading comprehension (MRC) BIBREF39, BIBREF40, BIBREF41, BIBREF40, BIBREF42, BIBREF15 has become a central task in natural language understanding. MRC in the SQuAD-style is to predict the answer span in the passage given a question and the passage. In this paper, we choose the SQuAD-style MRC task",
              "metadata": {
                "chunk_index": 8,
                "source_id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a"
              },
              "score": 0.8701809068834786
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__15",
              "document": "he language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family). Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex. We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project.\nThe work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.\nMulti-SimLex: Translation and Annotation ::: Word Pair Translation\nTranslators for each target language were instructed to find direct or approximate translations for the 1,888 word pairs that satisfy the following rules. (1) All pairs in the translated set must be unique (i.e., no duplicate pairs); (2) Translating two words from the same English pair into the same word in the target language is not allowed (e.g., it is not allowed to translate car and automobile to the same Spanish word coche). (3) The translated pairs must preserve the semantic relations between the two words when possible. This means that, when multiple translations are possible, the translation that best conveys the semantic relation between the two words found in the original English pair is selected. (4) If it is not possible to use a single-word translation in the target language, then a multi-word expression (MWE) can be us",
              "metadata": {
                "chunk_index": 15,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.6713421014803385
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__1",
              "document": "tics (see §SECREF2), but has also been shown to benefit a range of language understanding tasks in NLP. Examples include dialog state tracking BIBREF16, BIBREF17, spoken language understanding BIBREF18, BIBREF19, text simplification BIBREF20, BIBREF21, BIBREF22, dictionary and thesaurus construction BIBREF23, BIBREF24.\nDespite the proven usefulness of semantic similarity datasets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian BIBREF25, whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish BIBREF26, 500 in Farsi BIBREF27, or 300 in Finnish BIBREF28) and coverage (e.g., all datasets which originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity dataset spanning 1,888 concept pairs (see §SECREF4).\nMost importantly, semantic similarity datasets in different languages have been created using heterogeneous construction procedures with different guidelines for translation and annotation, as well as different rating scales. For instance, some datasets were obtained by directly translating the English SimLex-999 in its entirety BIBREF25, BIBREF16 or in part BIBREF28. Other datasets were created from scratch BIBREF26 and yet others sampled English concept pairs differently from SimLex-999 and then translated and reannotated them in target languages BIBREF27. This heterogeneity makes these datasets incomparable and precludes systematic cross-linguistic analyses. In this article, consolidating the lessons learned from previous dataset construction paradigms, we propose a carefully designed translation and annotation protocol for developing monolingual Multi-SimLex datasets with aligned concept pairs for typologically diverse languages. We apply this ",
              "metadata": {
                "chunk_index": 1,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.655289007432375
            }
          ],
          "pruned_text": "Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types. Data in MSRA is collected from news domain. Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain, which has 18 entity types. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively.",
          "answer": "The Chinese datasets used are MSRA and OntoNotes4.0."
        },
        {
          "query": "What architecture does the encoder have?",
          "rewritten": "What architecture does the encoder have?",
          "retrieval": [
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__6",
              "document": ":\nwhere $h^0=\\mathrm {PosEmb}(T)$; $T$ denotes the sentence vectors output by BertSum, and function $\\mathrm {PosEmb}$ adds sinusoid positional embeddings BIBREF3 to $T$, indicating the position of each sentence.\nThe final output layer is a sigmoid classifier:\nwhere $h^L_i$ is the vector for $sent_i$ from the top layer (the $L$-th layer ) of the Transformer. In experiments, we implemented Transformers with $L=1, 2, 3$ and found that a Transformer with $L=2$ performed best. We name this model BertSumExt.\nThe loss of the model is the binary classification entropy of prediction $\\hat{y}_i$ against gold label $y_i$. Inter-sentence Transformer layers are jointly fine-tuned with BertSum. We use the Adam optimizer with $\\beta _1=0.9$, and $\\beta _2=0.999$). Our learning rate schedule follows BIBREF3 with warming-up ($ \\operatorname{\\operatorname{warmup}}=10,000$):\nFine-tuning Bert for Summarization ::: Abstractive Summarization\nWe use a standard encoder-decoder framework for abstractive summarization BIBREF6. The encoder is the pretrained BertSum and the decoder is a 6-layered Transformer initialized randomly. It is conceivable that there is a mismatch between the encoder and the decoder, since the former is pretrained while the latter must be trained from scratch. This can make fine-tuning unstable; for example, the encoder might overfit the data while the decoder underfits, or vice versa. To circumvent this, we design a new fine-tuning schedule which separates the optimizers of the encoder and the decoder.\nWe use two Adam optimizers with $\\beta _1=0.9$ and $\\beta _2=0.999$ for the encoder and the decoder, respectively, each with different warmup-steps and learning rates:\nwhere $\\tilde{lr}_{\\mathcal {E}}=2e^{-3}$, and $\\operatorname{\\operatorname{warmup}}_{\\mathcal {E}}=20,000$ for the encoder and $\\tilde{lr}_{\\mathcal {D}}=0.1$, and $\\operatorname{\\operatorname{warmup}}_{\\mathcal {D}}=10,000$ for the decoder. This is based on the assumption that the pretrained encoder should be fine-tuned with a smaller learning rate ",
              "metadata": {
                "chunk_index": 6,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.9276232632338632
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__33",
              "document": " beyond the scope of this work, and we will experiment with them in the future.\nIn other words, we treat each pretrained encoder enc as a black-box function to encode a single word or a multi-word expression $x$ in each language into a $d$-dimensional contextualized representation $\\mathbf {x}_{\\textsc {enc}} \\in \\mathbb {R}^d = \\textsc {enc}(x)$ (e.g., $d=768$ with bert). As multilingual pretrained encoders, we experiment with the multilingual bert model (m-bert) BIBREF29 and xlm BIBREF30. m-bert is pretrained on monolingual Wikipedia corpora of 102 languages (comprising all Multi-SimLex languages) with a 12-layer Transformer network, and yields 768-dimensional representations. Since the concept pairs in Multi-SimLex are lowercased, we use the uncased version of m-bert. m-bert comprises all Multi-SimLex languages, and its evident ability to perform cross-lingual transfer BIBREF12, BIBREF125, BIBREF126 also makes it a convenient baseline model for cross-lingual experiments later in §SECREF8. The second multilingual model we consider, xlm-100, is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a $1,280$-dimensional representation. In contrast to m-bert, xlm-100 drops the next-sentence prediction objective and adds a cross-lingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the last $H=4$ hidden layers in all experiments, as suggested by Wu:2019arxiv.\nBesides m-bert and xlm, covering multiple languages, we also analyze the performance of “language-specific” bert and xlm models for the languages where they are available: Finnish, Spanish, English, Mandarin Chinese, and French. The main goal of this comparison is to study the differences in performance between multilingual “one-size-fits-all” encoders and language-specific encoders. For all experiments, we rely on the pretrained models released in the Transformers repository BIBREF127.\nUnsupervised post-processing steps devised for static word embeddings (i.e., ",
              "metadata": {
                "chunk_index": 33,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8307801326906206
            },
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__4",
              "document": " the source document $\\mathbf {x} = [x_1, ..., x_n]$ to a sequence of continuous representations $\\mathbf {z} = [z_1, ..., z_n]$, and a decoder then generates the target summary $\\mathbf {y} = [y_1, ..., y_m]$ token-by-token, in an auto-regressive manner, hence modeling the conditional probability: $p(y_1, ..., y_m|x_1, ..., x_n)$.\nBIBREF20 and BIBREF21 were among the first to apply the neural encoder-decoder architecture to text summarization. BIBREF6 enhance this model with a pointer-generator network (PTgen) which allows it to copy words from the source text, and a coverage mechanism (Cov) which keeps track of words that have been summarized. BIBREF11 propose an abstractive system where multiple agents (encoders) represent the document together with a hierarchical attention mechanism (over the agents) for decoding. Their Deep Communicating Agents (DCA) model is trained end-to-end with reinforcement learning. BIBREF9 also present a deep reinforced model (DRM) for abstractive summarization which handles the coverage problem with an intra-attention mechanism where the decoder attends over previously generated words. BIBREF4 follow a bottom-up approach (BottomUp); a content selector first determines which phrases in the source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. BIBREF22 propose an abstractive model which is particularly suited to extreme summarization (i.e., single sentence summaries), based on convolutional neural networks and additionally conditioned on topic distributions (TConvS2S).\nFine-tuning Bert for Summarization ::: Summarization Encoder\nAlthough Bert has been used to fine-tune various NLP tasks, its application to summarization is not as straightforward. Since Bert is trained as a masked-language model, the output vectors are grounded to tokens instead of sentences, while in extractive summarization, most models manipulate sentence-level representations. Although segmentation embeddings represent different sentences in Bert",
              "metadata": {
                "chunk_index": 4,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.81076083737121
            }
          ],
          "reranked": [
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__33",
              "document": " beyond the scope of this work, and we will experiment with them in the future.\nIn other words, we treat each pretrained encoder enc as a black-box function to encode a single word or a multi-word expression $x$ in each language into a $d$-dimensional contextualized representation $\\mathbf {x}_{\\textsc {enc}} \\in \\mathbb {R}^d = \\textsc {enc}(x)$ (e.g., $d=768$ with bert). As multilingual pretrained encoders, we experiment with the multilingual bert model (m-bert) BIBREF29 and xlm BIBREF30. m-bert is pretrained on monolingual Wikipedia corpora of 102 languages (comprising all Multi-SimLex languages) with a 12-layer Transformer network, and yields 768-dimensional representations. Since the concept pairs in Multi-SimLex are lowercased, we use the uncased version of m-bert. m-bert comprises all Multi-SimLex languages, and its evident ability to perform cross-lingual transfer BIBREF12, BIBREF125, BIBREF126 also makes it a convenient baseline model for cross-lingual experiments later in §SECREF8. The second multilingual model we consider, xlm-100, is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a $1,280$-dimensional representation. In contrast to m-bert, xlm-100 drops the next-sentence prediction objective and adds a cross-lingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the last $H=4$ hidden layers in all experiments, as suggested by Wu:2019arxiv.\nBesides m-bert and xlm, covering multiple languages, we also analyze the performance of “language-specific” bert and xlm models for the languages where they are available: Finnish, Spanish, English, Mandarin Chinese, and French. The main goal of this comparison is to study the differences in performance between multilingual “one-size-fits-all” encoders and language-specific encoders. For all experiments, we rely on the pretrained models released in the Transformers repository BIBREF127.\nUnsupervised post-processing steps devised for static word embeddings (i.e., ",
              "metadata": {
                "chunk_index": 33,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8307801326906206
            },
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__4",
              "document": " the source document $\\mathbf {x} = [x_1, ..., x_n]$ to a sequence of continuous representations $\\mathbf {z} = [z_1, ..., z_n]$, and a decoder then generates the target summary $\\mathbf {y} = [y_1, ..., y_m]$ token-by-token, in an auto-regressive manner, hence modeling the conditional probability: $p(y_1, ..., y_m|x_1, ..., x_n)$.\nBIBREF20 and BIBREF21 were among the first to apply the neural encoder-decoder architecture to text summarization. BIBREF6 enhance this model with a pointer-generator network (PTgen) which allows it to copy words from the source text, and a coverage mechanism (Cov) which keeps track of words that have been summarized. BIBREF11 propose an abstractive system where multiple agents (encoders) represent the document together with a hierarchical attention mechanism (over the agents) for decoding. Their Deep Communicating Agents (DCA) model is trained end-to-end with reinforcement learning. BIBREF9 also present a deep reinforced model (DRM) for abstractive summarization which handles the coverage problem with an intra-attention mechanism where the decoder attends over previously generated words. BIBREF4 follow a bottom-up approach (BottomUp); a content selector first determines which phrases in the source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. BIBREF22 propose an abstractive model which is particularly suited to extreme summarization (i.e., single sentence summaries), based on convolutional neural networks and additionally conditioned on topic distributions (TConvS2S).\nFine-tuning Bert for Summarization ::: Summarization Encoder\nAlthough Bert has been used to fine-tune various NLP tasks, its application to summarization is not as straightforward. Since Bert is trained as a masked-language model, the output vectors are grounded to tokens instead of sentences, while in extractive summarization, most models manipulate sentence-level representations. Although segmentation embeddings represent different sentences in Bert",
              "metadata": {
                "chunk_index": 4,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.81076083737121
            },
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__6",
              "document": ":\nwhere $h^0=\\mathrm {PosEmb}(T)$; $T$ denotes the sentence vectors output by BertSum, and function $\\mathrm {PosEmb}$ adds sinusoid positional embeddings BIBREF3 to $T$, indicating the position of each sentence.\nThe final output layer is a sigmoid classifier:\nwhere $h^L_i$ is the vector for $sent_i$ from the top layer (the $L$-th layer ) of the Transformer. In experiments, we implemented Transformers with $L=1, 2, 3$ and found that a Transformer with $L=2$ performed best. We name this model BertSumExt.\nThe loss of the model is the binary classification entropy of prediction $\\hat{y}_i$ against gold label $y_i$. Inter-sentence Transformer layers are jointly fine-tuned with BertSum. We use the Adam optimizer with $\\beta _1=0.9$, and $\\beta _2=0.999$). Our learning rate schedule follows BIBREF3 with warming-up ($ \\operatorname{\\operatorname{warmup}}=10,000$):\nFine-tuning Bert for Summarization ::: Abstractive Summarization\nWe use a standard encoder-decoder framework for abstractive summarization BIBREF6. The encoder is the pretrained BertSum and the decoder is a 6-layered Transformer initialized randomly. It is conceivable that there is a mismatch between the encoder and the decoder, since the former is pretrained while the latter must be trained from scratch. This can make fine-tuning unstable; for example, the encoder might overfit the data while the decoder underfits, or vice versa. To circumvent this, we design a new fine-tuning schedule which separates the optimizers of the encoder and the decoder.\nWe use two Adam optimizers with $\\beta _1=0.9$ and $\\beta _2=0.999$ for the encoder and the decoder, respectively, each with different warmup-steps and learning rates:\nwhere $\\tilde{lr}_{\\mathcal {E}}=2e^{-3}$, and $\\operatorname{\\operatorname{warmup}}_{\\mathcal {E}}=20,000$ for the encoder and $\\tilde{lr}_{\\mathcal {D}}=0.1$, and $\\operatorname{\\operatorname{warmup}}_{\\mathcal {D}}=10,000$ for the decoder. This is based on the assumption that the pretrained encoder should be fine-tuned with a smaller learning rate ",
              "metadata": {
                "chunk_index": 6,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.9276232632338632
            }
          ],
          "pruned_text": "In other words, we treat each pretrained encoder enc as a black-box function to encode a single word or a multi-word expression $x$ in each language into a $d$-dimensional contextualized representation $\\mathbf {x}_{\\textsc {enc}} \\in \\mathbb {R}^d = \\textsc {enc}(x)$ (e.g., $d=768$ with bert). As multilingual pretrained encoders, we experiment with the multilingual bert model (m-bert) and xlm. m-bert is pretrained on monolingual Wikipedia corpora of 102 languages with a 12-layer Transformer network, and yields 768-dimensional representations. The second multilingual model we consider, xlm-100, is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a $1,280$-dimensional representation. For both encoders, the representations of each concept are computed as averages over the last $H=4$ hidden layers in all experiments. Besides m-bert and xlm, we also analyze the performance of “language-specific” bert and xlm models for the languages where they are available. The main goal of this comparison is to study the differences in performance between multilingual “one-size-fits-all” encoders and language-specific encoders. For all experiments, we rely on the pretrained models released in the Transformers repository. \n\nWe use a standard encoder-decoder framework for abstractive summarization. The encoder is the pretrained BertSum and the decoder is a 6-layered Transformer initialized randomly. To circumvent potential mismatches between the encoder and the decoder, we design a new fine-tuning schedule which separates the optimizers of the encoder and the decoder. We use two Adam optimizers for the encoder and the decoder, respectively, each with different warmup-steps and learning rates.",
          "answer": "The encoder has a 12-layer Transformer network architecture."
        },
        {
          "query": "what was the baseline?",
          "rewritten": "what was the baseline?",
          "retrieval": [
            {
              "id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a__1",
              "document": " variance in results. Multi-task learning, paired with multilingual training and subsequent monolingual finetuning, scored highest for five out of seven languages, improving accuracy by another 9.86% on average.\nSystem Description\nOur system is a modification of the provided CoNLL–SIGMORPHON 2018 baseline system, so we begin this section with a reiteration of the baseline system architecture, followed by a description of the three augmentations we introduce.\nBaseline\nThe CoNLL–SIGMORPHON 2018 baseline is described as follows:\nThe system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.\nTo that we add a few details regarding model size and training schedule:\nthe number of LSTM layers is one;\nembedding size, LSTM layer size and attention layer size is 100;\nmodels are trained for 20 epochs;\non every epoch, training data is subsampled at a rate of 0.3;\nLSTM dropout is applied at a rate 0.3;\ncontext word forms are randomly dropped at a rate of 0.1;\nthe Adam optimiser is used, with a default learning rate of 0.001; and\ntrained models are evaluated on the development data (the data for the shared task comes already split in train and dev sets).\nOur system\nHere we compare and contrast our system to the baseline system. A diagram of our system is shown in Figure FIGREF4 .\n",
              "metadata": {
                "chunk_index": 1,
                "source_id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a"
              },
              "score": 0.750600424805249
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__7",
              "document": "ce pair was judged by 3 independent workers, and we used the majority vote as the label for that pair. Overall, only INLINEFORM0 of the references were labeled as entailed by the table. Fleiss' INLINEFORM1 was INLINEFORM2 , which indicates a fair agreement. We found the workers sometimes disagreed on what information can be reasonably entailed by the table.\nFigure FIGREF40 shows the correlations as we vary the percent of entailed examples in the evaluation set of WikiBio. Each point is obtained by fixing the desired proportion of entailed examples, and sampling subsets from the full set which satisfy this proportion. PARENT and RG-F remain stable and show a high correlation across the entire range, whereas BLEU and BLEU-T vary a lot. In the hyperparams category, the latter two have the worst correlation when the evaluation set contains only entailed examples, which may seem surprising. However, on closer examination we found that this subset tends to omit a lot of information from the tables. Systems which produce more information than these references are penalized by BLEU, but not in the human evaluation. PARENT overcomes this issue by measuring recall against the table in addition to the reference.\nWe check how different components in the computation of PARENT contribute to its correlation to human judgments. Specifically, we remove the probability INLINEFORM0 of an n-gram INLINEFORM1 being entailed by the table from Eqs. EQREF19 and EQREF23 . The average correlation for PARENT-W drops to INLINEFORM5 in this case. We also try a variant of PARENT with INLINEFORM6 , which removes the contribution of Table Recall (Eq. EQREF22 ). The average correlation is INLINEFORM7 in this case. With these components, the correlation is INLINEFORM8 , showing that they are crucial to the performance of PARENT.\nBIBREF28 point out that hill-climbing on an automatic metric is meaningless if that metric has a low instance-level correlation to human judgments. In Table TABREF46 we show the average accuracy of the metrics in making th",
              "metadata": {
                "chunk_index": 7,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.7207426307174041
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__25",
              "document": "del building. Applying this circular, iterative process to 450 18th-century novels written in three languages, Piper was able to uncover a new form of “conversional novel” that was not previously captured in “literary history's received critical categories” BIBREF48 .\nAlong similar lines, we can subject both the machine-generated output and the human annotations to another round of content validation. That is, take a stratified random sample, selecting observations from the full range of scores, and ask: Do these make sense in light of the systematized concept? If not, what seems to be missing? Or is something extraneous being captured? This is primarily a qualitative process that requires returning to theory and interrogating the systematized concept, indicators, and scores together. This type of validation is rarely done in NLP, but it is especially important when it is difficult to assess what drives a given machine learning model. If there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted.\nOther types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth.\nBesides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power ",
              "metadata": {
                "chunk_index": 25,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7163132690491348
            }
          ],
          "reranked": [
            {
              "id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a__1",
              "document": " variance in results. Multi-task learning, paired with multilingual training and subsequent monolingual finetuning, scored highest for five out of seven languages, improving accuracy by another 9.86% on average.\nSystem Description\nOur system is a modification of the provided CoNLL–SIGMORPHON 2018 baseline system, so we begin this section with a reiteration of the baseline system architecture, followed by a description of the three augmentations we introduce.\nBaseline\nThe CoNLL–SIGMORPHON 2018 baseline is described as follows:\nThe system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma [...] The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. [...] The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism.\nTo that we add a few details regarding model size and training schedule:\nthe number of LSTM layers is one;\nembedding size, LSTM layer size and attention layer size is 100;\nmodels are trained for 20 epochs;\non every epoch, training data is subsampled at a rate of 0.3;\nLSTM dropout is applied at a rate 0.3;\ncontext word forms are randomly dropped at a rate of 0.1;\nthe Adam optimiser is used, with a default learning rate of 0.001; and\ntrained models are evaluated on the development data (the data for the shared task comes already split in train and dev sets).\nOur system\nHere we compare and contrast our system to the baseline system. A diagram of our system is shown in Figure FIGREF4 .\n",
              "metadata": {
                "chunk_index": 1,
                "source_id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a"
              },
              "score": 0.750600424805249
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__7",
              "document": "ce pair was judged by 3 independent workers, and we used the majority vote as the label for that pair. Overall, only INLINEFORM0 of the references were labeled as entailed by the table. Fleiss' INLINEFORM1 was INLINEFORM2 , which indicates a fair agreement. We found the workers sometimes disagreed on what information can be reasonably entailed by the table.\nFigure FIGREF40 shows the correlations as we vary the percent of entailed examples in the evaluation set of WikiBio. Each point is obtained by fixing the desired proportion of entailed examples, and sampling subsets from the full set which satisfy this proportion. PARENT and RG-F remain stable and show a high correlation across the entire range, whereas BLEU and BLEU-T vary a lot. In the hyperparams category, the latter two have the worst correlation when the evaluation set contains only entailed examples, which may seem surprising. However, on closer examination we found that this subset tends to omit a lot of information from the tables. Systems which produce more information than these references are penalized by BLEU, but not in the human evaluation. PARENT overcomes this issue by measuring recall against the table in addition to the reference.\nWe check how different components in the computation of PARENT contribute to its correlation to human judgments. Specifically, we remove the probability INLINEFORM0 of an n-gram INLINEFORM1 being entailed by the table from Eqs. EQREF19 and EQREF23 . The average correlation for PARENT-W drops to INLINEFORM5 in this case. We also try a variant of PARENT with INLINEFORM6 , which removes the contribution of Table Recall (Eq. EQREF22 ). The average correlation is INLINEFORM7 in this case. With these components, the correlation is INLINEFORM8 , showing that they are crucial to the performance of PARENT.\nBIBREF28 point out that hill-climbing on an automatic metric is meaningless if that metric has a low instance-level correlation to human judgments. In Table TABREF46 we show the average accuracy of the metrics in making th",
              "metadata": {
                "chunk_index": 7,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.7207426307174041
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__25",
              "document": "del building. Applying this circular, iterative process to 450 18th-century novels written in three languages, Piper was able to uncover a new form of “conversional novel” that was not previously captured in “literary history's received critical categories” BIBREF48 .\nAlong similar lines, we can subject both the machine-generated output and the human annotations to another round of content validation. That is, take a stratified random sample, selecting observations from the full range of scores, and ask: Do these make sense in light of the systematized concept? If not, what seems to be missing? Or is something extraneous being captured? This is primarily a qualitative process that requires returning to theory and interrogating the systematized concept, indicators, and scores together. This type of validation is rarely done in NLP, but it is especially important when it is difficult to assess what drives a given machine learning model. If there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted.\nOther types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth.\nBesides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power ",
              "metadata": {
                "chunk_index": 25,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7163132690491348
            }
          ],
          "pruned_text": "The CoNLL–SIGMORPHON 2018 baseline is described as follows: The system is an encoder-decoder on character sequences. It takes a lemma as input and generates a word form. The process is conditioned on the context of the lemma. The baseline treats the lemma, word form and MSD of the previous and following word as context in track 1. In track 2, the baseline only considers the word forms of the previous and next word. The baseline system concatenates embeddings for context word forms, lemmas and MSDs into a context vector. The baseline then computes character embeddings for each character in the input lemma. Each of these is concatenated with a copy of the context vector. The resulting sequence of vectors is encoded using an LSTM encoder. Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism. The number of LSTM layers is one; embedding size, LSTM layer size and attention layer size is 100; models are trained for 20 epochs; on every epoch, training data is subsampled at a rate of 0.3; LSTM dropout is applied at a rate 0.3; context word forms are randomly dropped at a rate of 0.1; the Adam optimiser is used, with a default learning rate of 0.001; and trained models are evaluated on the development data.",
          "answer": "The baseline is an encoder-decoder system on character sequences that takes a lemma as input and generates a word form, conditioned on the context of the lemma. It uses LSTM for encoding and decoding, with specific configurations for layers, sizes, training epochs, and dropout rates."
        },
        {
          "query": "What is the model performance on target language reading comprehension?",
          "rewritten": "What is the model performance on target language reading comprehension?",
          "retrieval": [
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__4",
              "document": " triples, based on the different criteria, like i) jaccard similarity of answer bearing sentence in introduction and the passage ii) the tf-idf scores of answer candidates and iii) the length of answer candidates. However, we empirically find that we were better off using the entire set rather than these subsets.\nPre-training: We make use of the generated cloze dataset to pre-train an expressive neural network designed for the task of reading comprehension. We work with two publicly available neural network models – the GA Reader BIBREF2 (to enable comparison with prior work) and BiDAF + Self-Attention (SA) model from BIBREF1 (which is among the best performing models on SQuAD and TriviaQA). After pretraining, the performance of BiDAF+SA on a dev set of the (Wikipedia) cloze questions is 0.58 F1 score and 0.55 Exact Match (EM) score. This implies that the cloze corpus is neither too easy, nor too difficult to answer.\nFine Tuning: We fine tune the pre-trained model, from the previous step, over a small set of labelled question-answer pairs. As we shall later see, this step is crucial, and it only requires a handful of labelled questions to achieve a significant proportion of the performance typically attained by training on tens of thousands of questions.\nDatasets\nWe apply our system to three datasets from different domains. SQuAD BIBREF3 consists of questions whose answers are free form spans of text from passages in Wikipedia articles. We follow the same setting as in BIBREF0 , and split $10\\%$ of training questions as the test set, and report performance when training on subsets of the remaining data ranging from $1\\%$ to $90\\%$ of the full set. We also report the performance on the dev set when trained on the full training set ( $1^\\ast $ in Table 2 ). We use the same hyperparameter settings as in prior work. We compare and study four different settings: 1) the Supervised Learning (SL) setting, which is only trained on the supervised data, 2) the best performing GDAN model from BIBREF0 , 3) pretraining on a La",
              "metadata": {
                "chunk_index": 4,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.863925604879083
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__10",
              "document": "he focus on nouns and highly frequent concepts. Finally, prior work mostly focused on languages that are widely spoken and do not account for the variety of the world's languages. Our long-term goal is devising a standardized methodology to extend the coverage also to languages that are resource-lean and/or typologically diverse (e.g., Welsh, Kiswahili as in this work).\nMultilingual Datasets for Natural Language Understanding. The Multi-SimLex initiative and corresponding datasets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual bert BIBREF29 or xlm BIBREF30 are typically probed on XNLI test data BIBREF84 for cross-lingual natural language inference. XNLI was created by translating examples from the English MultiNLI dataset, and projecting its sentence labels BIBREF85. Other recent multilingual datasets target the task of question answering based on reading comprehension: i) MLQA BIBREF86 includes 7 languages ii) XQuAD BIBREF87 10 languages; iii) TyDiQA BIBREF88 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English dataset, TyDiQA was built independently in each language. Another multilingual dataset, PAWS-X BIBREF89, focused on the paraphrase identification task and was created translating the original English PAWS BIBREF90 into 6 languages. We believe that Multi-SimLex can substantially contribute to this endeavor by offering a comprehensive multilingual benchmark for the fundamental lexical level relation of semantic similarity. In future work, Multi-SimLex also offers an opportunity to investigate the correlations between word-level semantic similarity and performance in downstream tasks such as QA and NLI across different languages.\nThe Base for Multi-SimLex: Extending English SimLex-999\nIn this section, we discuss the design principles behin",
              "metadata": {
                "chunk_index": 10,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8497984970278759
            },
            {
              "id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc__0",
              "document": "Introduction\nReading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries. In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data. Although some efforts have been made to create RC datasets for Chinese BIBREF13, BIBREF14 and Korean BIBREF15, it is not feasible to collect RC datasets for every language since annotation efforts to collect a new RC dataset are often far from trivial. Therefore, the setup of transfer learning, especially zero-shot learning, is of extraordinary importance.\nExisting methods BIBREF16 of cross-lingual transfer learning on RC datasets often count on machine translation (MT) to translate data from source language into target language, or vice versa. These methods may not require a well-annotated RC dataset for the target language, whereas a high-quality MT model is needed as a trade-off, which might not be available when it comes to low-resource languages.\nIn this paper, we leverage pre-trained multilingual language representation, for example, BERT learned from multilingual un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC. We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-lingual ability of multi-BERT. Surprisingly, we find that the models have the ability to transfer between low lexical similarity language pair, such as English and Chinese. Recent studies BIBREF17, BIBREF12, BIBREF18 show that cross-lingual language models have the ability to enab",
              "metadata": {
                "chunk_index": 0,
                "source_id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc"
              },
              "score": 0.7995232162199319
            }
          ],
          "reranked": [
            {
              "id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc__0",
              "document": "Introduction\nReading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries. In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data. Although some efforts have been made to create RC datasets for Chinese BIBREF13, BIBREF14 and Korean BIBREF15, it is not feasible to collect RC datasets for every language since annotation efforts to collect a new RC dataset are often far from trivial. Therefore, the setup of transfer learning, especially zero-shot learning, is of extraordinary importance.\nExisting methods BIBREF16 of cross-lingual transfer learning on RC datasets often count on machine translation (MT) to translate data from source language into target language, or vice versa. These methods may not require a well-annotated RC dataset for the target language, whereas a high-quality MT model is needed as a trade-off, which might not be available when it comes to low-resource languages.\nIn this paper, we leverage pre-trained multilingual language representation, for example, BERT learned from multilingual un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC. We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-lingual ability of multi-BERT. Surprisingly, we find that the models have the ability to transfer between low lexical similarity language pair, such as English and Chinese. Recent studies BIBREF17, BIBREF12, BIBREF18 show that cross-lingual language models have the ability to enab",
              "metadata": {
                "chunk_index": 0,
                "source_id": "7fc2e4d78ed71fd9cb767560f9f8f69124754bf212e2e3dc"
              },
              "score": 0.7995232162199319
            },
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__4",
              "document": " triples, based on the different criteria, like i) jaccard similarity of answer bearing sentence in introduction and the passage ii) the tf-idf scores of answer candidates and iii) the length of answer candidates. However, we empirically find that we were better off using the entire set rather than these subsets.\nPre-training: We make use of the generated cloze dataset to pre-train an expressive neural network designed for the task of reading comprehension. We work with two publicly available neural network models – the GA Reader BIBREF2 (to enable comparison with prior work) and BiDAF + Self-Attention (SA) model from BIBREF1 (which is among the best performing models on SQuAD and TriviaQA). After pretraining, the performance of BiDAF+SA on a dev set of the (Wikipedia) cloze questions is 0.58 F1 score and 0.55 Exact Match (EM) score. This implies that the cloze corpus is neither too easy, nor too difficult to answer.\nFine Tuning: We fine tune the pre-trained model, from the previous step, over a small set of labelled question-answer pairs. As we shall later see, this step is crucial, and it only requires a handful of labelled questions to achieve a significant proportion of the performance typically attained by training on tens of thousands of questions.\nDatasets\nWe apply our system to three datasets from different domains. SQuAD BIBREF3 consists of questions whose answers are free form spans of text from passages in Wikipedia articles. We follow the same setting as in BIBREF0 , and split $10\\%$ of training questions as the test set, and report performance when training on subsets of the remaining data ranging from $1\\%$ to $90\\%$ of the full set. We also report the performance on the dev set when trained on the full training set ( $1^\\ast $ in Table 2 ). We use the same hyperparameter settings as in prior work. We compare and study four different settings: 1) the Supervised Learning (SL) setting, which is only trained on the supervised data, 2) the best performing GDAN model from BIBREF0 , 3) pretraining on a La",
              "metadata": {
                "chunk_index": 4,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.863925604879083
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__10",
              "document": "he focus on nouns and highly frequent concepts. Finally, prior work mostly focused on languages that are widely spoken and do not account for the variety of the world's languages. Our long-term goal is devising a standardized methodology to extend the coverage also to languages that are resource-lean and/or typologically diverse (e.g., Welsh, Kiswahili as in this work).\nMultilingual Datasets for Natural Language Understanding. The Multi-SimLex initiative and corresponding datasets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual bert BIBREF29 or xlm BIBREF30 are typically probed on XNLI test data BIBREF84 for cross-lingual natural language inference. XNLI was created by translating examples from the English MultiNLI dataset, and projecting its sentence labels BIBREF85. Other recent multilingual datasets target the task of question answering based on reading comprehension: i) MLQA BIBREF86 includes 7 languages ii) XQuAD BIBREF87 10 languages; iii) TyDiQA BIBREF88 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English dataset, TyDiQA was built independently in each language. Another multilingual dataset, PAWS-X BIBREF89, focused on the paraphrase identification task and was created translating the original English PAWS BIBREF90 into 6 languages. We believe that Multi-SimLex can substantially contribute to this endeavor by offering a comprehensive multilingual benchmark for the fundamental lexical level relation of semantic similarity. In future work, Multi-SimLex also offers an opportunity to investigate the correlations between word-level semantic similarity and performance in downstream tasks such as QA and NLI across different languages.\nThe Base for Multi-SimLex: Extending English SimLex-999\nIn this section, we discuss the design principles behin",
              "metadata": {
                "chunk_index": 10,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8497984970278759
            }
          ],
          "pruned_text": "Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries. In recent years, many large-scale RC datasets in English have nourished the development of numerous powerful and diverse RC models. The state-of-the-art model on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data. Although some efforts have been made to create RC datasets for Chinese and Korean, it is not feasible to collect RC datasets for every language since annotation efforts to collect a new RC dataset are often far from trivial. Therefore, the setup of transfer learning, especially zero-shot learning, is of extraordinary importance.\n\nExisting methods of cross-lingual transfer learning on RC datasets often count on machine translation (MT) to translate data from source language into target language, or vice versa. These methods may not require a well-annotated RC dataset for the target language, whereas a high-quality MT model is needed as a trade-off, which might not be available when it comes to low-resource languages.\n\nIn this paper, we leverage pre-trained multilingual language representation, for example, BERT learned from multilingual un-annotated sentences (multi-BERT), in cross-lingual zero-shot RC. We fine-tune multi-BERT on the training set in source language, then test the model in target language, with a number of combinations of source-target language pair to explore the cross-lingual ability of multi-BERT. \n\nWe make use of the generated cloze dataset to pre-train an expressive neural network designed for the task of reading comprehension. We work with two publicly available neural network models – the GA Reader and BiDAF + Self-Attention (SA) model. After pretraining, the performance of BiDAF+SA on a dev set of the (Wikipedia) cloze questions is 0.58 F1 score and 0.55 Exact Match (EM) score.\n\nWe apply our system to three datasets from different domains. SQuAD consists of questions whose answers are free form spans of text from passages in Wikipedia articles. We follow the same setting as in prior work, and report performance when training on subsets of the remaining data ranging from 1% to 90% of the full set. We also report the performance on the dev set when trained on the full training set. We compare and study four different settings: 1) the Supervised Learning (SL) setting, which is only trained on the supervised data, 2) the best performing GDAN model, 3) pretraining on a language model.\n\nThe Multi-SimLex initiative and corresponding datasets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual BERT or XLM are typically probed on XNLI test data for cross-lingual natural language inference. Other recent multilingual datasets target the task of question answering based on reading comprehension: i) MLQA includes 7 languages ii) XQuAD 10 languages; iii) TyDiQA 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English dataset, TyDiQA was built independently in each language. Another multilingual dataset, PAWS-X, focused on the paraphrase identification task and was created translating the original English PAWS into 6 languages.",
          "answer": "The model performance on target language reading comprehension, specifically for the BiDAF + Self-Attention (SA) model after pretraining, is reported as 0.58 F1 score and 0.55 Exact Match (EM) score on a dev set of cloze questions. However, the context does not provide specific performance metrics for the model on the target language reading comprehension tasks from the datasets mentioned (MLQA, XQuAD, TyDiQA). Therefore, I do not know the exact performance on those specific target language reading comprehension tasks."
        },
        {
          "query": "How does this approach compare to other WSD approaches employing word embeddings?",
          "rewritten": "How does this approach compare to other WSD approaches employing word embeddings?",
          "retrieval": [
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__29",
              "document": "d on language modeling such as multilingual BERT BIBREF29. These experiments provide strong baseline scores on the new Multi-SimLex datasets and offer a first large-scale analysis of pretrained encoders on word-level semantic similarity across diverse languages. In addition, the experiments now enabled by Multi-SimLex aim to answer several important questions. (Q1) Is it viable to extract high-quality word-level representations from pretrained encoders receiving subword-level tokens as input? Are such representations competitive with standard static word-level embeddings? (Q2) What are the implications of monolingual pretraining versus (massively) multilingual pretraining for performance? (Q3) Do lightweight unsupervised post-processing techniques improve word representations consistently across different languages? (Q4) Can we effectively transfer available external lexical knowledge from resource-rich languages to resource-lean languages in order to learn word representations that distinguish between true similarity and conceptual relatedness (see the discussion in §SECREF6)?\nMonolingual Evaluation of Representation Learning Models ::: Models in Comparison\nStatic Word Embeddings in Different Languages. First, we evaluate a standard method for inducing non-contextualized (i.e., static) word embeddings across a plethora of different languages: fastText (ft) vectors BIBREF31 are currently the most popular and robust choice given 1) the availability of pretrained vectors in a large number of languages BIBREF108 trained on large Common Crawl (CC) plus Wikipedia (Wiki) data, and 2) their superior performance across a range of NLP tasks BIBREF109. In fact, fastText is an extension of the standard word-level CBOW and skip-gram word2vec models BIBREF32 that takes into account subword-level information, i.e. the contituent character n-grams of each word BIBREF110. For this reason, fastText is also more suited for modeling rare words and morphologically rich languages.\nWe rely on 300-dimensional ft word vectors trained on",
              "metadata": {
                "chunk_index": 29,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8630659925778769
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__49",
              "document": "ned.\n2) Similarity rankings obtained from word embeddings for nouns are better aligned with human judgments than all the other part-of-speech classes considered here (verbs, adjectives, and, for the first time, adverbs). This confirms previous generalizations based on experiments on English.\n3) The factor having the greatest impact on the quality of word representations is the availability of raw texts to train them in the first place, rather than language properties (such as family, geographical area, typological features).\n4) Massively multilingual pretrained encoders such as m-bert BIBREF29 and xlm-100 BIBREF30 fare quite poorly on our benchmark, whereas pretrained encoders dedicated to a single language are more competitive with static word embeddings such as fastText BIBREF31. Moreover, for language-specific encoders, parameter reduction techniques reduce performance only marginally.\n5) Techniques to inject clean lexical semantic knowledge from external resources into distributional word representations were proven to be effective in emphasizing the relation of semantic similarity. In particular, methods capable of transferring such knowledge from resource-rich to resource-lean languages BIBREF59 increased the correlation with human judgments for most languages, except for those with limited unlabelled data.\nFuture work can expand our preliminary, yet large-scale study on the ability of pretrained encoders to reason over word-level semantic similarity in different languages. For instance, we have highlighted how sharing the same encoder parameters across multiple languages may harm performance. However, it remains unclear if, and to what extent, the input language embeddings present in xlm-100 but absent in m-bert help mitigate this issue. In addition, pretrained language embeddings can be obtained both from typological databases BIBREF99 and from neural architectures BIBREF100. Plugging these embeddings into the encoders in lieu of embeddings trained end-to-end as suggested by prior work BIBREF162, BIBREF16",
              "metadata": {
                "chunk_index": 49,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8434779435854635
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__32",
              "document": " oriented) knowledge into the vector space. Note that different perturbations can also be stacked: e.g., we can apply uncovec and then use abtt on top the output uncovec vectors. When using uncovec and abtt we always length-normalize and mean-center the data first (i.e., we apply the simple mc normalization). Finally, we tune the two hyper-parameters $d_A$ (for abtt) and $\\alpha $ (uncovec) on the English Multi-SimLex and use the same values on the datasets of all other languages; we report results with $dd_A = 3$ or $dd_A = 10$, and $\\alpha =-0.3$.\nContextualized Word Embeddings. We also evaluate the capacity of unsupervised pretraining architectures based on language modeling objectives to reason over lexical semantic similarity. To the best of our knowledge, our article is the first study performing such analyses. State-of-the-art models such as bert BIBREF29, xlm BIBREF30, or roberta BIBREF118 are typically very deep neural networks based on the Transformer architecture BIBREF119. They receive subword-level tokens as inputs (such as WordPieces BIBREF120) to tackle data sparsity. In output, they return contextualized embeddings, dynamic representations for words in context.\nTo represent words or multi-word expressions through a pretrained model, we follow prior work BIBREF121 and compute an input item's representation by 1) feeding it to a pretrained model in isolation; then 2) averaging the $H$ last hidden representations for each of the item’s constituent subwords; and then finally 3) averaging the resulting subword representations to produce the final $d$-dimensional representation, where $d$ is the embedding and hidden-layer dimensionality (e.g., $d=768$ with bert). We opt for this approach due to its proven viability and simplicity BIBREF121, as it does not require any additional corpora to condition the induction of contextualized embeddings. Other ways to extract the representations from pretrained models BIBREF122, BIBREF123, BIBREF124 are beyond the scope of this work, and we will experiment with them",
              "metadata": {
                "chunk_index": 32,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.823307547183107
            }
          ],
          "reranked": [
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__49",
              "document": "ned.\n2) Similarity rankings obtained from word embeddings for nouns are better aligned with human judgments than all the other part-of-speech classes considered here (verbs, adjectives, and, for the first time, adverbs). This confirms previous generalizations based on experiments on English.\n3) The factor having the greatest impact on the quality of word representations is the availability of raw texts to train them in the first place, rather than language properties (such as family, geographical area, typological features).\n4) Massively multilingual pretrained encoders such as m-bert BIBREF29 and xlm-100 BIBREF30 fare quite poorly on our benchmark, whereas pretrained encoders dedicated to a single language are more competitive with static word embeddings such as fastText BIBREF31. Moreover, for language-specific encoders, parameter reduction techniques reduce performance only marginally.\n5) Techniques to inject clean lexical semantic knowledge from external resources into distributional word representations were proven to be effective in emphasizing the relation of semantic similarity. In particular, methods capable of transferring such knowledge from resource-rich to resource-lean languages BIBREF59 increased the correlation with human judgments for most languages, except for those with limited unlabelled data.\nFuture work can expand our preliminary, yet large-scale study on the ability of pretrained encoders to reason over word-level semantic similarity in different languages. For instance, we have highlighted how sharing the same encoder parameters across multiple languages may harm performance. However, it remains unclear if, and to what extent, the input language embeddings present in xlm-100 but absent in m-bert help mitigate this issue. In addition, pretrained language embeddings can be obtained both from typological databases BIBREF99 and from neural architectures BIBREF100. Plugging these embeddings into the encoders in lieu of embeddings trained end-to-end as suggested by prior work BIBREF162, BIBREF16",
              "metadata": {
                "chunk_index": 49,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8434779435854635
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__32",
              "document": " oriented) knowledge into the vector space. Note that different perturbations can also be stacked: e.g., we can apply uncovec and then use abtt on top the output uncovec vectors. When using uncovec and abtt we always length-normalize and mean-center the data first (i.e., we apply the simple mc normalization). Finally, we tune the two hyper-parameters $d_A$ (for abtt) and $\\alpha $ (uncovec) on the English Multi-SimLex and use the same values on the datasets of all other languages; we report results with $dd_A = 3$ or $dd_A = 10$, and $\\alpha =-0.3$.\nContextualized Word Embeddings. We also evaluate the capacity of unsupervised pretraining architectures based on language modeling objectives to reason over lexical semantic similarity. To the best of our knowledge, our article is the first study performing such analyses. State-of-the-art models such as bert BIBREF29, xlm BIBREF30, or roberta BIBREF118 are typically very deep neural networks based on the Transformer architecture BIBREF119. They receive subword-level tokens as inputs (such as WordPieces BIBREF120) to tackle data sparsity. In output, they return contextualized embeddings, dynamic representations for words in context.\nTo represent words or multi-word expressions through a pretrained model, we follow prior work BIBREF121 and compute an input item's representation by 1) feeding it to a pretrained model in isolation; then 2) averaging the $H$ last hidden representations for each of the item’s constituent subwords; and then finally 3) averaging the resulting subword representations to produce the final $d$-dimensional representation, where $d$ is the embedding and hidden-layer dimensionality (e.g., $d=768$ with bert). We opt for this approach due to its proven viability and simplicity BIBREF121, as it does not require any additional corpora to condition the induction of contextualized embeddings. Other ways to extract the representations from pretrained models BIBREF122, BIBREF123, BIBREF124 are beyond the scope of this work, and we will experiment with them",
              "metadata": {
                "chunk_index": 32,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.823307547183107
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__29",
              "document": "d on language modeling such as multilingual BERT BIBREF29. These experiments provide strong baseline scores on the new Multi-SimLex datasets and offer a first large-scale analysis of pretrained encoders on word-level semantic similarity across diverse languages. In addition, the experiments now enabled by Multi-SimLex aim to answer several important questions. (Q1) Is it viable to extract high-quality word-level representations from pretrained encoders receiving subword-level tokens as input? Are such representations competitive with standard static word-level embeddings? (Q2) What are the implications of monolingual pretraining versus (massively) multilingual pretraining for performance? (Q3) Do lightweight unsupervised post-processing techniques improve word representations consistently across different languages? (Q4) Can we effectively transfer available external lexical knowledge from resource-rich languages to resource-lean languages in order to learn word representations that distinguish between true similarity and conceptual relatedness (see the discussion in §SECREF6)?\nMonolingual Evaluation of Representation Learning Models ::: Models in Comparison\nStatic Word Embeddings in Different Languages. First, we evaluate a standard method for inducing non-contextualized (i.e., static) word embeddings across a plethora of different languages: fastText (ft) vectors BIBREF31 are currently the most popular and robust choice given 1) the availability of pretrained vectors in a large number of languages BIBREF108 trained on large Common Crawl (CC) plus Wikipedia (Wiki) data, and 2) their superior performance across a range of NLP tasks BIBREF109. In fact, fastText is an extension of the standard word-level CBOW and skip-gram word2vec models BIBREF32 that takes into account subword-level information, i.e. the contituent character n-grams of each word BIBREF110. For this reason, fastText is also more suited for modeling rare words and morphologically rich languages.\nWe rely on 300-dimensional ft word vectors trained on",
              "metadata": {
                "chunk_index": 29,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8630659925778769
            }
          ],
          "pruned_text": "2) Similarity rankings obtained from word embeddings for nouns are better aligned with human judgments than all the other part-of-speech classes considered here (verbs, adjectives, and, for the first time, adverbs). \n\n4) Massively multilingual pretrained encoders such as m-bert and xlm-100 fare quite poorly on our benchmark, whereas pretrained encoders dedicated to a single language are more competitive with static word embeddings such as fastText. \n\n5) Techniques to inject clean lexical semantic knowledge from external resources into distributional word representations were proven to be effective in emphasizing the relation of semantic similarity. \n\nContextualized Word Embeddings. We also evaluate the capacity of unsupervised pretraining architectures based on language modeling objectives to reason over lexical semantic similarity. \n\nTo represent words or multi-word expressions through a pretrained model, we follow prior work and compute an input item's representation by 1) feeding it to a pretrained model in isolation; then 2) averaging the last hidden representations for each of the item’s constituent subwords; and then finally 3) averaging the resulting subword representations to produce the final representation.\n\nThese experiments provide strong baseline scores on the new Multi-SimLex datasets and offer a first large-scale analysis of pretrained encoders on word-level semantic similarity across diverse languages. \n\nStatic Word Embeddings in Different Languages. First, we evaluate a standard method for inducing non-contextualized (i.e., static) word embeddings across a plethora of different languages: fastText vectors are currently the most popular and robust choice given the availability of pretrained vectors in a large number of languages trained on large Common Crawl plus Wikipedia data, and their superior performance across a range of NLP tasks.",
          "answer": "The approach described in the context compares favorably to other WSD (Word Sense Disambiguation) approaches employing word embeddings in several ways. Firstly, similarity rankings obtained from word embeddings for nouns are better aligned with human judgments compared to other part-of-speech classes like verbs, adjectives, and adverbs. Additionally, while massively multilingual pretrained encoders like m-bert and xlm-100 perform poorly on benchmarks, pretrained encoders dedicated to a single language show competitive performance with static word embeddings such as fastText. Furthermore, techniques that inject clean lexical semantic knowledge into distributional word representations enhance the relation of semantic similarity, indicating that the approach may leverage external resources effectively. Overall, the context suggests that the evaluated approach provides strong baseline scores and a large-scale analysis of pretrained encoders on word-level semantic similarity, indicating its robustness compared to other methods."
        },
        {
          "query": "What NLP tasks do they consider?",
          "rewritten": "What NLP tasks do they consider?",
          "retrieval": [
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__19",
              "document": "e processing steps. In unsupervised settings, it is more challenging to understand the effects of different steps. Inferences drawn from unsupervised settings can be sensitive to pre-processing choices BIBREF33 . Stemming has been found to provide little measurable benefits for topic modeling and can sometimes even be harmful BIBREF34 . All in all, this again highlights the need to document these steps.\nFinally, we can also mark up the data, e.g., by identifying entities (people, places, organizations, etc.) or parts of speech. Although many NLP tools are available for such tasks, they are often challenged by linguistic variation, such as orthographic variation in historical texts BIBREF35 and social media BIBREF32 . Moreover, the performance of NLP tools often drops when applying them outside the training domain, such as applying tools developed on newswire texts to texts written by younger authors BIBREF36 . Problems (e.g., disambiguation in named entity recognition) are sometimes resolved using considerable manual intervention. This combination of the automated and the manual, however, becomes more difficult as the scale of the data increases, and the `certainty' brought by the latter may have to be abandoned.\nDictionary-based approaches\nDictionaries are frequently used to code texts in content analyses BIBREF37 . Dictionaries consist of one or more categories (i.e. word lists). Sometimes the output is simply the number of category occurrences (e.g., positive sentiment), thus weighting words within a category equally. In some other cases, words are assigned continuous scores. The high transparency of dictionaries makes them sometimes more suitable than supervised machine learning models. However, dictionaries should only be used if the scores assigned to words match how the words are used in the data (see BIBREF38 for a detailed discussion on limitations). There are many off-the-shelf dictionaries available (e.g., LIWC BIBREF39 ). These are often well-validated, but applying them on a new domain may not be app",
              "metadata": {
                "chunk_index": 19,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.8750318343473231
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__3",
              "document": "one thing, or does it comprise multiple forms of expression? Is there a clear boundary between hate speech and other types of speech, and what features make it more or less ambiguous? In these cases, it is critical to communicate high-level patterns in terms that are recognizable.\nThis contrasts with much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently. These tasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment. Many tasks are motivated by applications, for example to automatically block online trolls. Success, then, is often measured by performance, and communicating why a certain prediction was made—for example, why a document was labeled as positive sentiment, or why a word was classified as a noun—is less important than the accuracy of the prediction itself. The approaches we use and what we mean by `success' are thus guided by our research questions.\nDomain experts and fellow researchers can provide feedback on questions and help with dynamically revising them. For example, they may say “we already think we know that”, “that's too naïve”, “that doesn't reflect social reality” (negative); “two major camps in the field would give different answers to that question” (neutral); “we tried to look at that back in the 1960s, but we didn't have the technology” (positive); and “that sounds like something that people who made that archive would love”, “that's a really fundamental question” (very positive).\nSometimes we also hope to connect to multiple disciplines. For example, while focusing on the humanistic concerns of an archive, we could also ask social questions such as “is this archive more about collaborative processes, culture-building or norm creation?” or “how well does this archive reflect the society in which it is embedded?\" BIBREF3 used quantitative methods to tell a story about Darwin's in",
              "metadata": {
                "chunk_index": 3,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7845898120633631
            },
            {
              "id": "4c9552eec5c238657f8ed5237bf66067d3fdda2409a903b6__6",
              "document": "d not rely on human made rules allowing, as much as possible, its independence from a specific language. In this way, its potential application to another language would be easier, simply by changing the modules or the models of specific modules. In fact, we have explored the use of already existing modules and adopted and integrated several of these tools into our pipeline.\nIt is important to point out that, as far as we know, there is no integrated architecture supporting the full processing pipeline for the Portuguese language. We evaluated several systems like Rembrandt BIBREF22 or LinguaKit: the former only has the initial steps of our proposal (until NER) and the later performed worse than our system.\nThis framework, developed within the context of the Agatha project (described in Section SECREF1) has the full processing pipeline for Portuguese texts: it receives sentences as input and outputs ontological information: a) first performs all NLP typical tasks until semantic role labelling; b) then, it extracts subject-verb-object triples; c) and, then, it performs ontology matching procedures. As a final result, the obtained output is inserted into a specialized ontology.\nWe are aware that each of the architecture modules can, and should, be improved but our main goal was the creation of a full working text processing pipeline for the Portuguese language.\nConclusions and Future Work\nBesides the end–to–end NLP pipeline for the Portuguese language, the other main contributions of this work can be summarize as follows:\nDevelopment of an ontology for the criminal law domain;\nAlignment of the Eurovoc thesaurus and IATE terminology with the ontology created;\nRepresentation of the extracted events from texts in the linked knowledge base defined.\nThe obtained results support our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language. Being composed by several modules, each of them with a high level of complexity, it is certain that our approach can be improved",
              "metadata": {
                "chunk_index": 6,
                "source_id": "4c9552eec5c238657f8ed5237bf66067d3fdda2409a903b6"
              },
              "score": 0.7286283385155758
            }
          ],
          "reranked": [
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__19",
              "document": "e processing steps. In unsupervised settings, it is more challenging to understand the effects of different steps. Inferences drawn from unsupervised settings can be sensitive to pre-processing choices BIBREF33 . Stemming has been found to provide little measurable benefits for topic modeling and can sometimes even be harmful BIBREF34 . All in all, this again highlights the need to document these steps.\nFinally, we can also mark up the data, e.g., by identifying entities (people, places, organizations, etc.) or parts of speech. Although many NLP tools are available for such tasks, they are often challenged by linguistic variation, such as orthographic variation in historical texts BIBREF35 and social media BIBREF32 . Moreover, the performance of NLP tools often drops when applying them outside the training domain, such as applying tools developed on newswire texts to texts written by younger authors BIBREF36 . Problems (e.g., disambiguation in named entity recognition) are sometimes resolved using considerable manual intervention. This combination of the automated and the manual, however, becomes more difficult as the scale of the data increases, and the `certainty' brought by the latter may have to be abandoned.\nDictionary-based approaches\nDictionaries are frequently used to code texts in content analyses BIBREF37 . Dictionaries consist of one or more categories (i.e. word lists). Sometimes the output is simply the number of category occurrences (e.g., positive sentiment), thus weighting words within a category equally. In some other cases, words are assigned continuous scores. The high transparency of dictionaries makes them sometimes more suitable than supervised machine learning models. However, dictionaries should only be used if the scores assigned to words match how the words are used in the data (see BIBREF38 for a detailed discussion on limitations). There are many off-the-shelf dictionaries available (e.g., LIWC BIBREF39 ). These are often well-validated, but applying them on a new domain may not be app",
              "metadata": {
                "chunk_index": 19,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.8750318343473231
            },
            {
              "id": "4c9552eec5c238657f8ed5237bf66067d3fdda2409a903b6__6",
              "document": "d not rely on human made rules allowing, as much as possible, its independence from a specific language. In this way, its potential application to another language would be easier, simply by changing the modules or the models of specific modules. In fact, we have explored the use of already existing modules and adopted and integrated several of these tools into our pipeline.\nIt is important to point out that, as far as we know, there is no integrated architecture supporting the full processing pipeline for the Portuguese language. We evaluated several systems like Rembrandt BIBREF22 or LinguaKit: the former only has the initial steps of our proposal (until NER) and the later performed worse than our system.\nThis framework, developed within the context of the Agatha project (described in Section SECREF1) has the full processing pipeline for Portuguese texts: it receives sentences as input and outputs ontological information: a) first performs all NLP typical tasks until semantic role labelling; b) then, it extracts subject-verb-object triples; c) and, then, it performs ontology matching procedures. As a final result, the obtained output is inserted into a specialized ontology.\nWe are aware that each of the architecture modules can, and should, be improved but our main goal was the creation of a full working text processing pipeline for the Portuguese language.\nConclusions and Future Work\nBesides the end–to–end NLP pipeline for the Portuguese language, the other main contributions of this work can be summarize as follows:\nDevelopment of an ontology for the criminal law domain;\nAlignment of the Eurovoc thesaurus and IATE terminology with the ontology created;\nRepresentation of the extracted events from texts in the linked knowledge base defined.\nThe obtained results support our claim that the proposed system can be used as a base tool for information extraction for the Portuguese language. Being composed by several modules, each of them with a high level of complexity, it is certain that our approach can be improved",
              "metadata": {
                "chunk_index": 6,
                "source_id": "4c9552eec5c238657f8ed5237bf66067d3fdda2409a903b6"
              },
              "score": 0.7286283385155758
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__3",
              "document": "one thing, or does it comprise multiple forms of expression? Is there a clear boundary between hate speech and other types of speech, and what features make it more or less ambiguous? In these cases, it is critical to communicate high-level patterns in terms that are recognizable.\nThis contrasts with much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently. These tasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment. Many tasks are motivated by applications, for example to automatically block online trolls. Success, then, is often measured by performance, and communicating why a certain prediction was made—for example, why a document was labeled as positive sentiment, or why a word was classified as a noun—is less important than the accuracy of the prediction itself. The approaches we use and what we mean by `success' are thus guided by our research questions.\nDomain experts and fellow researchers can provide feedback on questions and help with dynamically revising them. For example, they may say “we already think we know that”, “that's too naïve”, “that doesn't reflect social reality” (negative); “two major camps in the field would give different answers to that question” (neutral); “we tried to look at that back in the 1960s, but we didn't have the technology” (positive); and “that sounds like something that people who made that archive would love”, “that's a really fundamental question” (very positive).\nSometimes we also hope to connect to multiple disciplines. For example, while focusing on the humanistic concerns of an archive, we could also ask social questions such as “is this archive more about collaborative processes, culture-building or norm creation?” or “how well does this archive reflect the society in which it is embedded?\" BIBREF3 used quantitative methods to tell a story about Darwin's in",
              "metadata": {
                "chunk_index": 3,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7845898120633631
            }
          ],
          "pruned_text": "Inferences drawn from unsupervised settings can be sensitive to pre-processing choices. Stemming has been found to provide little measurable benefits for topic modeling and can sometimes even be harmful. We can also mark up the data, e.g., by identifying entities (people, places, organizations, etc.) or parts of speech. Many NLP tools are available for such tasks but are often challenged by linguistic variation. The performance of NLP tools often drops when applying them outside the training domain. Problems in named entity recognition are sometimes resolved using considerable manual intervention. \n\nDictionaries are frequently used to code texts in content analyses. The high transparency of dictionaries makes them sometimes more suitable than supervised machine learning models. There are many off-the-shelf dictionaries available, but applying them on a new domain may not be appropriate. \n\nWe evaluated several systems like Rembrandt or LinguaKit; the former only has the initial steps of our proposal and the latter performed worse than our system. This framework has the full processing pipeline for Portuguese texts: it performs all NLP typical tasks until semantic role labelling, extracts subject-verb-object triples, and performs ontology matching procedures. \n\nThe other main contributions of this work can be summarized as follows: Development of an ontology for the criminal law domain; Alignment of the Eurovoc thesaurus and IATE terminology with the ontology created; Representation of the extracted events from texts in the linked knowledge base defined. \n\nTasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment. Many tasks are motivated by applications, for example, to automatically block online trolls. Success is often measured by performance, and communicating why a certain prediction was made is less important than the accuracy of the prediction itself.",
          "answer": "The NLP tasks considered include part-of-speech tagging, parsing, topic modeling, named entity recognition, semantic role labeling, extracting subject-verb-object triples, filtering spam, and detecting sentiment."
        },
        {
          "query": "Is it possible to convert a cloze-style questions to a naturally-looking questions?",
          "rewritten": "Is it possible to convert a cloze-style questions to a naturally-looking questions?",
          "retrieval": [
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__8",
              "document": "r receive a large boost. Also, \"why\" questions show the least improvement, which is in line with our expectation, since these usually require reasoning or world knowledge which cloze questions rarely require.\nConclusion\nIn this paper, we show that pre-training QA models with automatically constructed cloze questions improves the performance of the models significantly, especially when there are few labeled examples. The performance of the model trained only on the cloze questions is poor, validating the need for fine-tuning. Through regression analysis, we find that pretraining helps with questions which ask for factual information located in a specific part of the context. For future work, we plan to explore the active learning setup for this task – specifically, which passages and / or types of questions can we select to annotate, such that there is a maximum performance gain from fine-tuning. We also want to explore how to adapt cloze style pre-training to NLP tasks other than QA.\nAcknowledgments\nBhuwan Dhingra is supported by NSF under grants CCF-1414030 and IIS-1250956 and by grants from Google. Danish Pruthi and Dheeraj Rajagopal are supported by the DARPA Big Mechanism program under ARO contract W911NF-14-1-0436.",
              "metadata": {
                "chunk_index": 8,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.867220044894243
            },
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__1",
              "document": "erely using 1% of the training data. Our system outperforms the approaches for semi-supervised QA presented in BIBREF0 , and a baseline which uses the same unlabeled data but with a language modeling objective for pretraining. In the BioASQ challenge, we outperform the best performing system from previous year's challenge, improving over a baseline which does transfer learning from the SQuAD dataset. Our analysis reveals that questions which ask for factual information and match to specific parts of the context documents benefit the most from pretraining on automatically constructed clozes.\nRelated Work\nSemi-supervised learning augments the labeled dataset $L$ with a potentially larger unlabeled dataset $U$ . BIBREF0 presented a model, GDAN, which trained an auxiliary neural network to generate questions from passages by reinforcement learning, and augment the labeled dataset with the generated questions to train the QA model. Here we use a much simpler heuristic to generate the auxiliary questions, which also turns out to be more effective as we show superior performance compared to GDAN. Several approaches have been suggested for generating natural questions BIBREF6 , BIBREF7 , BIBREF8 , however none of them show a significant improvement of using the generated questions in a semi-supervised setting. Recent papers also use unlabeled data for QA by training large language models and extracting contextual word vectors from them to input to the QA model BIBREF9 , BIBREF10 , BIBREF11 . The applicability of this method in the low-resource setting is unclear as the extra inputs increase the number of parameters in the QA model, however, our pretraining can be easily applied to these models as well.\nDomain adaptation (and Transfer learning) leverage existing large scale datasets from a source domain (or task) to improve performance on a target domain (or task). For deep learning and QA, a common approach is to pretrain on the source dataset and then fine-tune on the target dataset BIBREF12 , BIBREF13 . BIBREF14 used S",
              "metadata": {
                "chunk_index": 1,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.705872419600198
            },
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__0",
              "document": "Introduction\nDeep learning systems have shown a lot of promise for extractive Question Answering (QA), with performance comparable to humans when large scale data is available. However, practitioners looking to build QA systems for specific applications may not have the resources to collect tens of thousands of questions on corpora of their choice. At the same time, state-of-the-art machine reading systems do not lend well to low-resource QA settings where the number of labeled question-answer pairs are limited (c.f. Table 2 ). Semi-supervised QA methods like BIBREF0 aim to improve this performance by leveraging unlabeled data which is easier to collect.\nIn this work, we present a semi-supervised QA system which requires the end user to specify a set of base documents and only a small set of question-answer pairs over a subset of these documents. Our proposed system consists of three stages. First, we construct cloze-style questions (predicting missing spans of text) from the unlabeled corpus; next, we use the generated clozes to pre-train a powerful neural network model for extractive QA BIBREF1 , BIBREF2 ; and finally, we fine-tune the model on the small set of provided QA pairs.\nOur cloze construction process builds on a typical writing phenomenon and document structure: an introduction precedes and summarizes the main body of the article. Many large corpora follow such a structure, including Wikipedia, academic papers, and news articles. We hypothesize that we can benefit from the un-annotated corpora to better answer various questions – at least ones that are lexically similar to the content in base documents and directly require factual information.\nWe apply the proposed system on three datasets from different domains – SQuAD BIBREF3 , TriviaQA-Web BIBREF4 and the BioASQ challenge BIBREF5 . We observe significant improvements in a low-resource setting across all three datasets. For SQuAD and TriviaQA, we attain an F1 score of more than 50% by merely using 1% of the training data. Our system outperforms the ",
              "metadata": {
                "chunk_index": 0,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.679867158048809
            }
          ],
          "reranked": [
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__8",
              "document": "r receive a large boost. Also, \"why\" questions show the least improvement, which is in line with our expectation, since these usually require reasoning or world knowledge which cloze questions rarely require.\nConclusion\nIn this paper, we show that pre-training QA models with automatically constructed cloze questions improves the performance of the models significantly, especially when there are few labeled examples. The performance of the model trained only on the cloze questions is poor, validating the need for fine-tuning. Through regression analysis, we find that pretraining helps with questions which ask for factual information located in a specific part of the context. For future work, we plan to explore the active learning setup for this task – specifically, which passages and / or types of questions can we select to annotate, such that there is a maximum performance gain from fine-tuning. We also want to explore how to adapt cloze style pre-training to NLP tasks other than QA.\nAcknowledgments\nBhuwan Dhingra is supported by NSF under grants CCF-1414030 and IIS-1250956 and by grants from Google. Danish Pruthi and Dheeraj Rajagopal are supported by the DARPA Big Mechanism program under ARO contract W911NF-14-1-0436.",
              "metadata": {
                "chunk_index": 8,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.867220044894243
            },
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__1",
              "document": "erely using 1% of the training data. Our system outperforms the approaches for semi-supervised QA presented in BIBREF0 , and a baseline which uses the same unlabeled data but with a language modeling objective for pretraining. In the BioASQ challenge, we outperform the best performing system from previous year's challenge, improving over a baseline which does transfer learning from the SQuAD dataset. Our analysis reveals that questions which ask for factual information and match to specific parts of the context documents benefit the most from pretraining on automatically constructed clozes.\nRelated Work\nSemi-supervised learning augments the labeled dataset $L$ with a potentially larger unlabeled dataset $U$ . BIBREF0 presented a model, GDAN, which trained an auxiliary neural network to generate questions from passages by reinforcement learning, and augment the labeled dataset with the generated questions to train the QA model. Here we use a much simpler heuristic to generate the auxiliary questions, which also turns out to be more effective as we show superior performance compared to GDAN. Several approaches have been suggested for generating natural questions BIBREF6 , BIBREF7 , BIBREF8 , however none of them show a significant improvement of using the generated questions in a semi-supervised setting. Recent papers also use unlabeled data for QA by training large language models and extracting contextual word vectors from them to input to the QA model BIBREF9 , BIBREF10 , BIBREF11 . The applicability of this method in the low-resource setting is unclear as the extra inputs increase the number of parameters in the QA model, however, our pretraining can be easily applied to these models as well.\nDomain adaptation (and Transfer learning) leverage existing large scale datasets from a source domain (or task) to improve performance on a target domain (or task). For deep learning and QA, a common approach is to pretrain on the source dataset and then fine-tune on the target dataset BIBREF12 , BIBREF13 . BIBREF14 used S",
              "metadata": {
                "chunk_index": 1,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.705872419600198
            },
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__0",
              "document": "Introduction\nDeep learning systems have shown a lot of promise for extractive Question Answering (QA), with performance comparable to humans when large scale data is available. However, practitioners looking to build QA systems for specific applications may not have the resources to collect tens of thousands of questions on corpora of their choice. At the same time, state-of-the-art machine reading systems do not lend well to low-resource QA settings where the number of labeled question-answer pairs are limited (c.f. Table 2 ). Semi-supervised QA methods like BIBREF0 aim to improve this performance by leveraging unlabeled data which is easier to collect.\nIn this work, we present a semi-supervised QA system which requires the end user to specify a set of base documents and only a small set of question-answer pairs over a subset of these documents. Our proposed system consists of three stages. First, we construct cloze-style questions (predicting missing spans of text) from the unlabeled corpus; next, we use the generated clozes to pre-train a powerful neural network model for extractive QA BIBREF1 , BIBREF2 ; and finally, we fine-tune the model on the small set of provided QA pairs.\nOur cloze construction process builds on a typical writing phenomenon and document structure: an introduction precedes and summarizes the main body of the article. Many large corpora follow such a structure, including Wikipedia, academic papers, and news articles. We hypothesize that we can benefit from the un-annotated corpora to better answer various questions – at least ones that are lexically similar to the content in base documents and directly require factual information.\nWe apply the proposed system on three datasets from different domains – SQuAD BIBREF3 , TriviaQA-Web BIBREF4 and the BioASQ challenge BIBREF5 . We observe significant improvements in a low-resource setting across all three datasets. For SQuAD and TriviaQA, we attain an F1 score of more than 50% by merely using 1% of the training data. Our system outperforms the ",
              "metadata": {
                "chunk_index": 0,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.679867158048809
            }
          ],
          "pruned_text": "In this paper, we show that pre-training QA models with automatically constructed cloze questions improves the performance of the models significantly, especially when there are few labeled examples. The performance of the model trained only on the cloze questions is poor, validating the need for fine-tuning. Through regression analysis, we find that pretraining helps with questions which ask for factual information located in a specific part of the context. We also want to explore how to adapt cloze style pre-training to NLP tasks other than QA.\n\nSemi-supervised learning augments the labeled dataset with a potentially larger unlabeled dataset. Several approaches have been suggested for generating natural questions; however, none of them show a significant improvement of using the generated questions in a semi-supervised setting. Recent papers also use unlabeled data for QA by training large language models and extracting contextual word vectors from them to input to the QA model.\n\nDeep learning systems have shown a lot of promise for extractive Question Answering (QA), with performance comparable to humans when large scale data is available. However, practitioners looking to build QA systems for specific applications may not have the resources to collect tens of thousands of questions on corpora of their choice. Semi-supervised QA methods aim to improve this performance by leveraging unlabeled data which is easier to collect.\n\nIn this work, we present a semi-supervised QA system which requires the end user to specify a set of base documents and only a small set of question-answer pairs over a subset of these documents. Our proposed system consists of three stages. First, we construct cloze-style questions from the unlabeled corpus; next, we use the generated clozes to pre-train a powerful neural network model for extractive QA; and finally, we fine-tune the model on the small set of provided QA pairs.\n\nWe apply the proposed system on three datasets from different domains – SQuAD, TriviaQA-Web, and the BioASQ challenge. We observe significant improvements in a low-resource setting across all three datasets. For SQuAD and TriviaQA, we attain an F1 score of more than 50% by merely using 1% of the training data.",
          "answer": "Yes, it is possible to convert cloze-style questions to naturally-looking questions, as the context mentions that several approaches have been suggested for generating natural questions. However, it also notes that none of these approaches have shown significant improvement in a semi-supervised setting."
        },
        {
          "query": "How are weights dynamically adjusted?",
          "rewritten": "How are weights dynamically adjusted?",
          "retrieval": [
            {
              "id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a__2",
              "document": " the NER task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.\nThe rest of this paper is organized as follows: related work is presented in Section 2. We describe different training objectives in Section 3. Experimental results are presented in Section 4. We perform ablation studies in Section 5, followed by a brief conclusion in Section 6.\nRelated Work ::: Data Resample\nThe idea of weighting training examples has a long history. Importance sampling BIBREF17 assigns weights to different samples and changes the data distribution. Boosting algorithms such as AdaBoost BIBREF18 select harder examples to train subsequent classifiers. Similarly, hard example mining BIBREF19 downsamples the majority class and exploits the most difficult examples. Oversampling BIBREF20, BIBREF21 is used to balance the data distribution. Another line of data resampling is to dynamically control the weights of examples as training proceeds. For example, focal loss BIBREF16 used a soft weighting scheme that emphasizes harder examples during training. In self-paced learning BIBREF22, example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first. At each training step, self-paced learning algorithm optimizes model parameters and example weights jointly. Other works BIBREF23, BIBREF24 adjusted the weights of different training examples based on training loss. Besides, recent work BIBREF25, BIBREF26 proposed to learn a separate network to predict sample weights.\nRelated Work ::: Data Imbalance Issue in Object Detection\nThe background-object label imbalance issue is severe and thus well studied in the field of object detection BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31. The idea of hard negative mining (HNM) BIBREF30 has gained much attention recently. shrivastava2016ohem proposed the online hard example mining (OHEM) algorithm in an iterative manner that makes training progressively more difficult, and pushes the model ",
              "metadata": {
                "chunk_index": 2,
                "source_id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a"
              },
              "score": 0.7676061694666886
            },
            {
              "id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238__6",
              "document": " layer normalization, and the transformation is defined as: DISPLAYFORM0\nwhere INLINEFORM0 represents the weight matrix of hidden layer, and INLINEFORM1 denotes the bias term, INLINEFORM2 is the parameter of LeakyReLU activation and is set to 0.1, INLINEFORM3 and INLINEFORM4 denote the normalized hidden states and the outputs of the hidden layer, and INLINEFORM5 represents the layer normalization.\nThen, to project INLINEFORM0 into four document-level event related word distributions ( INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 shown in Figure FIGREF4 ), four subnets (each contains a linear layer, a batch normalization layer and a softmax layer) are employed in INLINEFORM5 . And the exact transformation is based on the formulas below: DISPLAYFORM0\nwhere INLINEFORM0 means softmax layer, INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 denote the weight matrices of the linear layers in subnets, INLINEFORM5 , INLINEFORM6 , INLINEFORM7 and INLINEFORM8 represent the corresponding bias terms, INLINEFORM9 , INLINEFORM10 , INLINEFORM11 and INLINEFORM12 are state vectors. INLINEFORM13 , INLINEFORM14 , INLINEFORM15 and INLINEFORM16 denote the generated entity distribution, location distribution, keyword distribution and date distribution, respectively, that correspond to the given event distribution INLINEFORM17 . And each dimension represents the relevance between corresponding entity/location/keyword/date term and the input event distribution.\nFinally, four generated distributions are concatenated to represent the generated document INLINEFORM0 corresponding to the input INLINEFORM1 : DISPLAYFORM0\nThe discriminator network INLINEFORM0 is designed as a fully-connected network which contains an input layer, a discriminative feature layer (discriminative features are employed for event visualization) and an output layer. In AEM, INLINEFORM1 uses fake document INLINEFORM2 and real document INLINEFORM3 as input and outputs the signal INLINEFORM4 to indicate the source of the input data (lower value denotes ",
              "metadata": {
                "chunk_index": 6,
                "source_id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238"
              },
              "score": 0.7547472112804318
            },
            {
              "id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238__7",
              "document": "4 to indicate the source of the input data (lower value denotes that INLINEFORM5 is prone to predict the input data as a fake document and vice versa).\nAs have previously been discussed in BIBREF7 , BIBREF8 , lipschitz continuity of INLINEFORM0 network is crucial to the training of the GAN-based approaches. To ensure the lipschitz continuity of INLINEFORM1 , we employ the spectral normalization technique BIBREF9 . More concretely, for each linear layer INLINEFORM2 (bias term is omitted for simplicity) in INLINEFORM3 , the weight matrix INLINEFORM4 is normalized by INLINEFORM5 . Here, INLINEFORM6 is the spectral norm of the weight matrix INLINEFORM7 with the definition below: DISPLAYFORM0\nwhich is equivalent to the largest singular value of INLINEFORM0 . The weight matrix INLINEFORM1 is then normalized using: DISPLAYFORM0\nObviously, the normalized weight matrix INLINEFORM0 satisfies that INLINEFORM1 and further ensures the lipschitz continuity of the INLINEFORM2 network BIBREF9 . To reduce the high cost of computing spectral norm INLINEFORM3 using singular value decomposition at each iteration, we follow BIBREF10 and employ the power iteration method to estimate INLINEFORM4 instead. With this substitution, the spectral norm can be estimated with very small additional computational time.\nObjective and Training Procedure\nThe real document INLINEFORM0 and fake document INLINEFORM1 shown in Figure FIGREF4 could be viewed as random samples from two distributions INLINEFORM2 and INLINEFORM3 , and each of them is a joint distribution constituted by four Dirichlet distributions (corresponding to entity distribution, location distribution, keyword distribution and date distribution). The training objective of AEM is to let the distribution INLINEFORM4 (produced by INLINEFORM5 network) to approximate the real data distribution INLINEFORM6 as much as possible.\nTo compare the different GAN losses, Kurach kurach2018gan takes a sober view of the current state of GAN and suggests that the Jansen-Shannon divergence used in BIBREF",
              "metadata": {
                "chunk_index": 7,
                "source_id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238"
              },
              "score": 0.5581003742404244
            }
          ],
          "reranked": [
            {
              "id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a__2",
              "document": " the NER task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.\nThe rest of this paper is organized as follows: related work is presented in Section 2. We describe different training objectives in Section 3. Experimental results are presented in Section 4. We perform ablation studies in Section 5, followed by a brief conclusion in Section 6.\nRelated Work ::: Data Resample\nThe idea of weighting training examples has a long history. Importance sampling BIBREF17 assigns weights to different samples and changes the data distribution. Boosting algorithms such as AdaBoost BIBREF18 select harder examples to train subsequent classifiers. Similarly, hard example mining BIBREF19 downsamples the majority class and exploits the most difficult examples. Oversampling BIBREF20, BIBREF21 is used to balance the data distribution. Another line of data resampling is to dynamically control the weights of examples as training proceeds. For example, focal loss BIBREF16 used a soft weighting scheme that emphasizes harder examples during training. In self-paced learning BIBREF22, example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first. At each training step, self-paced learning algorithm optimizes model parameters and example weights jointly. Other works BIBREF23, BIBREF24 adjusted the weights of different training examples based on training loss. Besides, recent work BIBREF25, BIBREF26 proposed to learn a separate network to predict sample weights.\nRelated Work ::: Data Imbalance Issue in Object Detection\nThe background-object label imbalance issue is severe and thus well studied in the field of object detection BIBREF27, BIBREF28, BIBREF29, BIBREF30, BIBREF31. The idea of hard negative mining (HNM) BIBREF30 has gained much attention recently. shrivastava2016ohem proposed the online hard example mining (OHEM) algorithm in an iterative manner that makes training progressively more difficult, and pushes the model ",
              "metadata": {
                "chunk_index": 2,
                "source_id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a"
              },
              "score": 0.7676061694666886
            },
            {
              "id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238__6",
              "document": " layer normalization, and the transformation is defined as: DISPLAYFORM0\nwhere INLINEFORM0 represents the weight matrix of hidden layer, and INLINEFORM1 denotes the bias term, INLINEFORM2 is the parameter of LeakyReLU activation and is set to 0.1, INLINEFORM3 and INLINEFORM4 denote the normalized hidden states and the outputs of the hidden layer, and INLINEFORM5 represents the layer normalization.\nThen, to project INLINEFORM0 into four document-level event related word distributions ( INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 shown in Figure FIGREF4 ), four subnets (each contains a linear layer, a batch normalization layer and a softmax layer) are employed in INLINEFORM5 . And the exact transformation is based on the formulas below: DISPLAYFORM0\nwhere INLINEFORM0 means softmax layer, INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 denote the weight matrices of the linear layers in subnets, INLINEFORM5 , INLINEFORM6 , INLINEFORM7 and INLINEFORM8 represent the corresponding bias terms, INLINEFORM9 , INLINEFORM10 , INLINEFORM11 and INLINEFORM12 are state vectors. INLINEFORM13 , INLINEFORM14 , INLINEFORM15 and INLINEFORM16 denote the generated entity distribution, location distribution, keyword distribution and date distribution, respectively, that correspond to the given event distribution INLINEFORM17 . And each dimension represents the relevance between corresponding entity/location/keyword/date term and the input event distribution.\nFinally, four generated distributions are concatenated to represent the generated document INLINEFORM0 corresponding to the input INLINEFORM1 : DISPLAYFORM0\nThe discriminator network INLINEFORM0 is designed as a fully-connected network which contains an input layer, a discriminative feature layer (discriminative features are employed for event visualization) and an output layer. In AEM, INLINEFORM1 uses fake document INLINEFORM2 and real document INLINEFORM3 as input and outputs the signal INLINEFORM4 to indicate the source of the input data (lower value denotes ",
              "metadata": {
                "chunk_index": 6,
                "source_id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238"
              },
              "score": 0.7547472112804318
            },
            {
              "id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238__7",
              "document": "4 to indicate the source of the input data (lower value denotes that INLINEFORM5 is prone to predict the input data as a fake document and vice versa).\nAs have previously been discussed in BIBREF7 , BIBREF8 , lipschitz continuity of INLINEFORM0 network is crucial to the training of the GAN-based approaches. To ensure the lipschitz continuity of INLINEFORM1 , we employ the spectral normalization technique BIBREF9 . More concretely, for each linear layer INLINEFORM2 (bias term is omitted for simplicity) in INLINEFORM3 , the weight matrix INLINEFORM4 is normalized by INLINEFORM5 . Here, INLINEFORM6 is the spectral norm of the weight matrix INLINEFORM7 with the definition below: DISPLAYFORM0\nwhich is equivalent to the largest singular value of INLINEFORM0 . The weight matrix INLINEFORM1 is then normalized using: DISPLAYFORM0\nObviously, the normalized weight matrix INLINEFORM0 satisfies that INLINEFORM1 and further ensures the lipschitz continuity of the INLINEFORM2 network BIBREF9 . To reduce the high cost of computing spectral norm INLINEFORM3 using singular value decomposition at each iteration, we follow BIBREF10 and employ the power iteration method to estimate INLINEFORM4 instead. With this substitution, the spectral norm can be estimated with very small additional computational time.\nObjective and Training Procedure\nThe real document INLINEFORM0 and fake document INLINEFORM1 shown in Figure FIGREF4 could be viewed as random samples from two distributions INLINEFORM2 and INLINEFORM3 , and each of them is a joint distribution constituted by four Dirichlet distributions (corresponding to entity distribution, location distribution, keyword distribution and date distribution). The training objective of AEM is to let the distribution INLINEFORM4 (produced by INLINEFORM5 network) to approximate the real data distribution INLINEFORM6 as much as possible.\nTo compare the different GAN losses, Kurach kurach2018gan takes a sober view of the current state of GAN and suggests that the Jansen-Shannon divergence used in BIBREF",
              "metadata": {
                "chunk_index": 7,
                "source_id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238"
              },
              "score": 0.5581003742404244
            }
          ],
          "pruned_text": "The idea of weighting training examples has a long history. Importance sampling assigns weights to different samples and changes the data distribution. Boosting algorithms such as AdaBoost select harder examples to train subsequent classifiers. Similarly, hard example mining downsamples the majority class and exploits the most difficult examples. Oversampling is used to balance the data distribution. Another line of data resampling is to dynamically control the weights of examples as training proceeds. For example, focal loss used a soft weighting scheme that emphasizes harder examples during training. In self-paced learning, example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first. At each training step, self-paced learning algorithm optimizes model parameters and example weights jointly. Other works adjusted the weights of different training examples based on training loss. Besides, recent work proposed to learn a separate network to predict sample weights.",
          "answer": "Weights are dynamically adjusted through various methods such as focal loss, which uses a soft weighting scheme to emphasize harder examples, and self-paced learning, where example weights are optimized alongside model parameters. Additionally, weights can be adjusted based on training loss, and some approaches involve learning a separate network to predict sample weights."
        },
        {
          "query": "Which existing benchmarks did they compare to?",
          "rewritten": "Which existing benchmarks did they compare to?",
          "retrieval": [
            {
              "id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238__12",
              "document": "e observed that documents describing the same event have been grouped into the same cluster.\nTo further evaluate if a variation of the parameters INLINEFORM0 (the number of discriminator iterations per generator iteration), INLINEFORM1 (the number of units in hidden layer) and the structure of generator INLINEFORM2 will impact the extraction performance, additional experiments have been conducted on the Google dataset, with INLINEFORM3 set to 5, 7 and 10, INLINEFORM4 set to 100, 150 and 200, and three INLINEFORM5 structures (3, 4 and 5 layers). The comparison results on precision, recall and F-measure are shown in Figure FIGREF20 . From the results, it could be observed that AEM with the 5-layer generator performs the best and achieves 96.7% in F-measure, and the worst F-measure obtained by AEM is 85.7%. Overall, the AEM outperforms all compared approaches acorss various parameter settings, showing relatively stable performance.\nFinally, we compare in Figure FIGREF37 the training time required for each model, excluding the constant time required by each model to load the data. We could observe that K-means runs fastest among all four approaches. Both LEM and DPEMM need to sample the event allocation for each document and update the relevant counts during Gibbs sampling which are time consuming. AEM only requires a fraction of the training time compared to LEM and DPEMM. Moreover, on a larger dataset such as the Google dataset, AEM appears to be far more efficient compared to LEM and DPEMM.\nConclusions and Future Work\nIn this paper, we have proposed a novel approach based on adversarial training to extract the structured representation of events from online text. The experimental comparison with the state-of-the-art methods shows that AEM achieves improved extraction performance, especially on long text corpora with an improvement of 15% observed in F-measure. AEM only requires a fraction of training time compared to existing Bayesian graphical modeling approaches. In future work, we will explore incorporating ext",
              "metadata": {
                "chunk_index": 12,
                "source_id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238"
              },
              "score": 0.8790265731535734
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__4",
              "document": "tegory produce high quality fluent texts, and differ primarily on the quantity and accuracy of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance. This is an important use-case as metrics are often used to tune hyperparameters of a model.\nHuman Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24 . However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0 , for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22 , which assigns a score to each model based on how many times it was preferred over an alternative.\nThe data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.\nCompared Metrics\nText only: We compare BLEU BIBREF8 , ROUGE BIBREF9 , METEOR BIBREF18 , CIDEr and CIDEr-D BIBREF25 using their publicly available implementations.\nInformation Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4 .\nT",
              "metadata": {
                "chunk_index": 4,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8026583982177496
            },
            {
              "id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a__11",
              "document": "acy. The same as SST-5, we observe a slight performance drop with DL and DSC, which means that the dice loss actually works well for F1 but not for accuracy.\nAblation Studies ::: The Effect of Hyperparameters in Tversky index\nAs mentioned in Section SECREF10, Tversky index (TI) offers the flexibility in controlling the tradeoff between false-negatives and false-positives. In this subsection, we explore the effect of hyperparameters (i.e., $\\alpha $ and $\\beta $) in TI to test how they manipulate the tradeoff. We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset to examine the influence of tradeoff between precision and recall. Experiment results are shown in Table . The highest F1 for Chinese OntoNotes4.0 is 84.67 when $\\alpha $ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha $ is set to 0.4. In addition, we can observe that the performance varies a lot as $\\alpha $ changes in distinct datasets, which shows that the hyperparameters $\\alpha ,\\beta $ play an important role in the proposed method.\nConclusion\nIn this paper, we alleviate the severe data imbalance issue in NLP tasks. We propose to use dice loss in replacement of the standard cross-entropy loss, which performs as a soft version of F1 score. Using dice loss can help narrow the gap between training objectives and evaluation metrics. Empirically, we show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks.",
              "metadata": {
                "chunk_index": 11,
                "source_id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a"
              },
              "score": 0.7648460392813754
            }
          ],
          "reranked": [
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__4",
              "document": "tegory produce high quality fluent texts, and differ primarily on the quantity and accuracy of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance. This is an important use-case as metrics are often used to tune hyperparameters of a model.\nHuman Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24 . However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0 , for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22 , which assigns a score to each model based on how many times it was preferred over an alternative.\nThe data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.\nCompared Metrics\nText only: We compare BLEU BIBREF8 , ROUGE BIBREF9 , METEOR BIBREF18 , CIDEr and CIDEr-D BIBREF25 using their publicly available implementations.\nInformation Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4 .\nT",
              "metadata": {
                "chunk_index": 4,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8026583982177496
            },
            {
              "id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238__12",
              "document": "e observed that documents describing the same event have been grouped into the same cluster.\nTo further evaluate if a variation of the parameters INLINEFORM0 (the number of discriminator iterations per generator iteration), INLINEFORM1 (the number of units in hidden layer) and the structure of generator INLINEFORM2 will impact the extraction performance, additional experiments have been conducted on the Google dataset, with INLINEFORM3 set to 5, 7 and 10, INLINEFORM4 set to 100, 150 and 200, and three INLINEFORM5 structures (3, 4 and 5 layers). The comparison results on precision, recall and F-measure are shown in Figure FIGREF20 . From the results, it could be observed that AEM with the 5-layer generator performs the best and achieves 96.7% in F-measure, and the worst F-measure obtained by AEM is 85.7%. Overall, the AEM outperforms all compared approaches acorss various parameter settings, showing relatively stable performance.\nFinally, we compare in Figure FIGREF37 the training time required for each model, excluding the constant time required by each model to load the data. We could observe that K-means runs fastest among all four approaches. Both LEM and DPEMM need to sample the event allocation for each document and update the relevant counts during Gibbs sampling which are time consuming. AEM only requires a fraction of the training time compared to LEM and DPEMM. Moreover, on a larger dataset such as the Google dataset, AEM appears to be far more efficient compared to LEM and DPEMM.\nConclusions and Future Work\nIn this paper, we have proposed a novel approach based on adversarial training to extract the structured representation of events from online text. The experimental comparison with the state-of-the-art methods shows that AEM achieves improved extraction performance, especially on long text corpora with an improvement of 15% observed in F-measure. AEM only requires a fraction of training time compared to existing Bayesian graphical modeling approaches. In future work, we will explore incorporating ext",
              "metadata": {
                "chunk_index": 12,
                "source_id": "db4afd55783aaf6d069c5228152492cf0804e9cf310cb238"
              },
              "score": 0.8790265731535734
            },
            {
              "id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a__11",
              "document": "acy. The same as SST-5, we observe a slight performance drop with DL and DSC, which means that the dice loss actually works well for F1 but not for accuracy.\nAblation Studies ::: The Effect of Hyperparameters in Tversky index\nAs mentioned in Section SECREF10, Tversky index (TI) offers the flexibility in controlling the tradeoff between false-negatives and false-positives. In this subsection, we explore the effect of hyperparameters (i.e., $\\alpha $ and $\\beta $) in TI to test how they manipulate the tradeoff. We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset to examine the influence of tradeoff between precision and recall. Experiment results are shown in Table . The highest F1 for Chinese OntoNotes4.0 is 84.67 when $\\alpha $ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha $ is set to 0.4. In addition, we can observe that the performance varies a lot as $\\alpha $ changes in distinct datasets, which shows that the hyperparameters $\\alpha ,\\beta $ play an important role in the proposed method.\nConclusion\nIn this paper, we alleviate the severe data imbalance issue in NLP tasks. We propose to use dice loss in replacement of the standard cross-entropy loss, which performs as a soft version of F1 score. Using dice loss can help narrow the gap between training objectives and evaluation metrics. Empirically, we show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks.",
              "metadata": {
                "chunk_index": 11,
                "source_id": "677dad703988e8f26b322211803fad38ffe9e58d1725f51a"
              },
              "score": 0.7648460392813754
            }
          ],
          "pruned_text": "Compared Metrics  \nText only: We compare BLEU, ROUGE, METEOR, CIDEr and CIDEr-D using their publicly available implementations.  \nInformation Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4.  \n\nConclusions and Future Work  \nIn this paper, we have proposed a novel approach based on adversarial training to extract the structured representation of events from online text. The experimental comparison with the state-of-the-art methods shows that AEM achieves improved extraction performance, especially on long text corpora with an improvement of 15% observed in F-measure. AEM only requires a fraction of training time compared to existing Bayesian graphical modeling approaches.  \n\nAblation Studies ::: The Effect of Hyperparameters in Tversky index  \nAs mentioned in Section SECREF10, Tversky index (TI) offers the flexibility in controlling the tradeoff between false-negatives and false-positives. We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset to examine the influence of tradeoff between precision and recall. Experiment results are shown in Table. The highest F1 for Chinese OntoNotes4.0 is 84.67 when $\\alpha$ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha$ is set to 0.4.  \n\nConclusion  \nIn this paper, we alleviate the severe data imbalance issue in NLP tasks. We propose to use dice loss in replacement of the standard cross-entropy loss, which performs as a soft version of F1 score. Using dice loss can help narrow the gap between training objectives and evaluation metrics. Empirically, we show that the proposed training objective leads to significant performance boost for part-of-speech, named entity recognition, machine reading comprehension and paraphrase identification tasks.",
          "answer": "They compared to BLEU, ROUGE, METEOR, CIDEr, CIDEr-D, and the CS, RG, and RG-F metrics."
        },
        {
          "query": "What metrics are used for evaluation?",
          "rewritten": "What metrics are used for evaluation?",
          "retrieval": [
            {
              "id": "10f50fd914b227d12f503b3ef4ac5fe772e70f44557c17b5__5",
              "document": "top INLINEFORM1 keywords for the report. The INLINEFORM2 is empirically set to 50 within this research. The extracted keywords are further grouped into different topics by using latent Dirichlet allocation (LDA) BIBREF11 . The keywords in a report are highlighted using the color theme based on their topics.\nEvaluation metrics\nEach model is evaluated using two standard NLP metrics—micro and macro averaged F-scores, the harmonic mean of related metrics precision and recall. For each diagnostic category INLINEFORM0 from a set of 37 different classes INLINEFORM1 , the number of true positives INLINEFORM2 , false positives INLINEFORM3 , and false negatives INLINEFORM4 , the micro F-score is defined as DISPLAYFORM0\nwhereas macro F-score is given by DISPLAYFORM0\nIn summary, micro-averaged metrics have class representation roughly proportional to their test set representation (same as accuracy for classification problem with a single label per data point), whereas macro-averaged metrics are averaged by class without weighting by class prevalence BIBREF12 .\nExperimental setting\nIn this study, we performed two different series of experiments: i) evaluating the performance of TF-IDF features and various machine learning classifiers on the task of predicting primary diagnosis from the text content of a given report, and ii) using TF-IDF and LDA techniques to highlight the important keywords within a report. For the first experiment series, training reports are pre-processed, then their TF-IDF features are extracted. The TF-IDF features and the training labels are used to train different classification models. These different classification models and their hyper-parameters are reported in tab:classifier. The performance of classifiers is measured quantitatively on the test dataset using the evaluation metrics discussed in the previous section. For the second experiment series, a random report is selected and its top 50 keywords are extracted using TF-IDF weights. These 50 keywords are highlighted using different colors based",
              "metadata": {
                "chunk_index": 5,
                "source_id": "10f50fd914b227d12f503b3ef4ac5fe772e70f44557c17b5"
              },
              "score": 0.9122595314764906
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__4",
              "document": "tegory produce high quality fluent texts, and differ primarily on the quantity and accuracy of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance. This is an important use-case as metrics are often used to tune hyperparameters of a model.\nHuman Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24 . However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0 , for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22 , which assigns a score to each model based on how many times it was preferred over an alternative.\nThe data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.\nCompared Metrics\nText only: We compare BLEU BIBREF8 , ROUGE BIBREF9 , METEOR BIBREF18 , CIDEr and CIDEr-D BIBREF25 using their publicly available implementations.\nInformation Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4 .\nT",
              "metadata": {
                "chunk_index": 4,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8462239982270474
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__24",
              "document": "e (systemized) concept. Reliability aims to capture repeatability, i.e. the extent to which a given tool provides consistent results.\nValidity assesses the extent to which a given measurement tool measures what it is supposed to measure. In NLP and machine learning, most models are primarily evaluated by comparing the machine-generated labels against an annotated sample. This approach presumes that the human output is the “gold standard\" against which performance should be tested. In contrast, when the reliability is measured based on the output of different annotators, no coder is taken as the standard and the likelihood of coders reaching agreement by chance (rather than because they are “correct\") is factored into the resulting statistic. Comparing against a “gold standard” suggests that the threshold for human inter- and intra-coder reliability should be particularly high.\nAccuracy, as well as other measures such as precision, recall and F-score, are sometimes presented as a measure of validity, but if we do not have a genuinely objective determination of what something is supposed measure—as is often the case in text analysis—then accuracy is perhaps a better indication of reliability than of validity. In that case, validity needs to be assessed based on other techniques like those we discuss later in this section. It is also worth asking what level of accuracy is sufficient for our analysis and to what extent there may be an upper bound, especially when the labels are native to the data or when the notion of a “gold standard” is not appropriate.\nFor some in the humanities, validation takes the form of close reading, not designed to confirm whether the model output is correct, but to present what BIBREF48 refers to as a form of “further discovery in two directions”. Model outputs tell us something about the texts, while a close reading of the texts alongside those outputs tells us something about the models that can be used for more effective model building. Applying this circular, iterative process to 450 1",
              "metadata": {
                "chunk_index": 24,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7897543700907991
            }
          ],
          "reranked": [
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__4",
              "document": "tegory produce high quality fluent texts, and differ primarily on the quantity and accuracy of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance. This is an important use-case as metrics are often used to tune hyperparameters of a model.\nHuman Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform, proficient in English, were shown a table with pairs of generated texts, or a generated text and the reference, and asked to select the one they prefer. Figure FIGREF34 shows the instructions they were given. Paired comparisons have been shown to be superior to rating scales for comparing generated texts BIBREF24 . However, for measuring correlation the comparisons need to be aggregated into real-valued scores, INLINEFORM0 , for each of the INLINEFORM1 models. For this, we use Thurstone's method BIBREF22 , which assigns a score to each model based on how many times it was preferred over an alternative.\nThe data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. 1100 tables were sampled from the development set, and for each table we got 8 different sentence pairs annotated across the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one worker only which means there may be noise at the instance-level, but the aggregated system-level scores had low variance (cf. Table TABREF32 ). In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio.\nCompared Metrics\nText only: We compare BLEU BIBREF8 , ROUGE BIBREF9 , METEOR BIBREF18 , CIDEr and CIDEr-D BIBREF25 using their publicly available implementations.\nInformation Extraction based: We compare the CS, RG and RG-F metrics discussed in § SECREF4 .\nT",
              "metadata": {
                "chunk_index": 4,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.8462239982270474
            },
            {
              "id": "10f50fd914b227d12f503b3ef4ac5fe772e70f44557c17b5__5",
              "document": "top INLINEFORM1 keywords for the report. The INLINEFORM2 is empirically set to 50 within this research. The extracted keywords are further grouped into different topics by using latent Dirichlet allocation (LDA) BIBREF11 . The keywords in a report are highlighted using the color theme based on their topics.\nEvaluation metrics\nEach model is evaluated using two standard NLP metrics—micro and macro averaged F-scores, the harmonic mean of related metrics precision and recall. For each diagnostic category INLINEFORM0 from a set of 37 different classes INLINEFORM1 , the number of true positives INLINEFORM2 , false positives INLINEFORM3 , and false negatives INLINEFORM4 , the micro F-score is defined as DISPLAYFORM0\nwhereas macro F-score is given by DISPLAYFORM0\nIn summary, micro-averaged metrics have class representation roughly proportional to their test set representation (same as accuracy for classification problem with a single label per data point), whereas macro-averaged metrics are averaged by class without weighting by class prevalence BIBREF12 .\nExperimental setting\nIn this study, we performed two different series of experiments: i) evaluating the performance of TF-IDF features and various machine learning classifiers on the task of predicting primary diagnosis from the text content of a given report, and ii) using TF-IDF and LDA techniques to highlight the important keywords within a report. For the first experiment series, training reports are pre-processed, then their TF-IDF features are extracted. The TF-IDF features and the training labels are used to train different classification models. These different classification models and their hyper-parameters are reported in tab:classifier. The performance of classifiers is measured quantitatively on the test dataset using the evaluation metrics discussed in the previous section. For the second experiment series, a random report is selected and its top 50 keywords are extracted using TF-IDF weights. These 50 keywords are highlighted using different colors based",
              "metadata": {
                "chunk_index": 5,
                "source_id": "10f50fd914b227d12f503b3ef4ac5fe772e70f44557c17b5"
              },
              "score": 0.9122595314764906
            },
            {
              "id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20__24",
              "document": "e (systemized) concept. Reliability aims to capture repeatability, i.e. the extent to which a given tool provides consistent results.\nValidity assesses the extent to which a given measurement tool measures what it is supposed to measure. In NLP and machine learning, most models are primarily evaluated by comparing the machine-generated labels against an annotated sample. This approach presumes that the human output is the “gold standard\" against which performance should be tested. In contrast, when the reliability is measured based on the output of different annotators, no coder is taken as the standard and the likelihood of coders reaching agreement by chance (rather than because they are “correct\") is factored into the resulting statistic. Comparing against a “gold standard” suggests that the threshold for human inter- and intra-coder reliability should be particularly high.\nAccuracy, as well as other measures such as precision, recall and F-score, are sometimes presented as a measure of validity, but if we do not have a genuinely objective determination of what something is supposed measure—as is often the case in text analysis—then accuracy is perhaps a better indication of reliability than of validity. In that case, validity needs to be assessed based on other techniques like those we discuss later in this section. It is also worth asking what level of accuracy is sufficient for our analysis and to what extent there may be an upper bound, especially when the labels are native to the data or when the notion of a “gold standard” is not appropriate.\nFor some in the humanities, validation takes the form of close reading, not designed to confirm whether the model output is correct, but to present what BIBREF48 refers to as a form of “further discovery in two directions”. Model outputs tell us something about the texts, while a close reading of the texts alongside those outputs tells us something about the models that can be used for more effective model building. Applying this circular, iterative process to 450 1",
              "metadata": {
                "chunk_index": 24,
                "source_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20"
              },
              "score": 0.7897543700907991
            }
          ],
          "pruned_text": "Human Evaluation\nWe collected human judgments on the quality of the 16 models trained for WikiBio. Workers on a crowd-sourcing platform were shown pairs of generated texts and asked to select the one they prefer. Paired comparisons have been shown to be superior to rating scales for comparing generated texts. For measuring correlation, the comparisons need to be aggregated into real-valued scores for each model. We use Thurstone's method, which assigns a score to each model based on how many times it was preferred over an alternative. The data collection was performed separately for models in the WikiBio-Systems and WikiBio-Hyperparams categories. In total around 500 different workers were involved in the annotation. References were also included in the evaluation, and they received a lower score than PG-Net.\n\nCompared Metrics\nText only: We compare BLEU, ROUGE, METEOR, CIDEr, and CIDEr-D using their publicly available implementations. Information Extraction based: We compare the CS, RG, and RG-F metrics.\n\nEvaluation metrics\nEach model is evaluated using two standard NLP metrics—micro and macro averaged F-scores, the harmonic mean of related metrics precision and recall. For each diagnostic category, the micro F-score is defined as the number of true positives, false positives, and false negatives, whereas macro F-score is averaged by class without weighting by class prevalence.\n\nExperimental setting\nWe performed two different series of experiments: i) evaluating the performance of TF-IDF features and various machine learning classifiers on the task of predicting primary diagnosis from the text content of a given report, and ii) using TF-IDF and LDA techniques to highlight the important keywords within a report. The performance of classifiers is measured quantitatively on the test dataset using the evaluation metrics discussed.\n\nReliability aims to capture repeatability, i.e., the extent to which a given tool provides consistent results. Validity assesses the extent to which a given measurement tool measures what it is supposed to measure. In NLP and machine learning, most models are primarily evaluated by comparing the machine-generated labels against an annotated sample. Accuracy, as well as other measures such as precision, recall, and F-score, are sometimes presented as a measure of validity. Validity needs to be assessed based on other techniques.",
          "answer": "The metrics used for evaluation include:\n\n1. Human Evaluation using Thurstone's method for paired comparisons.\n2. NLP metrics: BLEU, ROUGE, METEOR, CIDEr, and CIDEr-D.\n3. Information Extraction based metrics: CS, RG, and RG-F.\n4. Micro and macro averaged F-scores, which are the harmonic mean of precision and recall.\n5. Accuracy, precision, recall, and F-score for assessing validity."
        },
        {
          "query": "Ngrams of which length are aligned using PARENT?",
          "rewritten": "Ngrams of which length are aligned using PARENT?",
          "retrieval": [
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__3",
              "document": "the length of $q_i$ . We consider a $\\text{match} (q_i, p_j)$ , if there is an exact string match of a sequence of words $\\lbrace w_k, w_{k+1}, .. w_{l_{q_i}}\\rbrace $ between the sentence $q_i$ and passage $p_j$ . If this sequence is either a noun phrase, verb phrase, adjective phrase or a named entity in $\\lbrace p_1, p_2, .. p_m\\rbrace $0 , as recognized by CoreNLP or BANNER, we select it as an answer span $\\lbrace p_1, p_2, .. p_m\\rbrace $1 . Additionally, we use $\\lbrace p_1, p_2, .. p_m\\rbrace $2 as the passage $\\lbrace p_1, p_2, .. p_m\\rbrace $3 and form a cloze question $\\lbrace p_1, p_2, .. p_m\\rbrace $4 from the answer bearing sentence $\\lbrace p_1, p_2, .. p_m\\rbrace $5 by replacing $\\lbrace p_1, p_2, .. p_m\\rbrace $6 with a placeholder. As a result, we obtain passage-question-answer ( $\\lbrace p_1, p_2, .. p_m\\rbrace $7 ) triples (Table 1 shows an example). As a post-processing step, we prune out $\\lbrace p_1, p_2, .. p_m\\rbrace $8 triples where the word overlap between the question (Q) and passage (P) is less than 2 words (after excluding the stop words).\nThe process relies on the fact that answer candidates from the introduction are likely to be discussed in detail in the remainder of the article. In effect, the cloze question from the introduction and the matching paragraph in the body forms a question and context passage pair. We create two cloze datasets, one each from Wikipedia corpus (for SQuAD and TriviaQA) and PUBMed academic papers (for the BioASQ challenge), consisting of 2.2M and 1M clozes respectively. From analyzing the cloze data manually, we were able to answer 76% times for the Wikipedia set and 80% times for the PUBMed set using the information in the passage. In most cases the cloze paraphrased the information in the passage, which we hypothesized to be a useful signal for the downstream QA task.\nWe also investigate the utility of forming subsets of the large cloze corpus, where we select the top passage-question-answer triples, based on the different criteria, like i) jaccard simil",
              "metadata": {
                "chunk_index": 3,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.8623104221866078
            },
            {
              "id": "3b069a546d5417be546f57d02981cf5ac68b4d3d6c55624a__6",
              "document": " language might have more than one treebank in Universal Dependencies, we use the following treebanks: en_ewt (English), fr_gsd (French), ru_syntagrus (Russian) ar_padt (Arabic), vi_vtb (Vietnamese), hi_hdtb (Hindi), and zh_gsd (Chinese).\nZero-shot Experiments ::: Data ::: Remark on BPE\nBIBREF22 show that sharing subwords between languages improves alignments between embedding spaces. BIBREF2 observe a strong correlation between the percentage of overlapping subwords and mBERT's performances for cross-lingual zero-shot transfer. However, in our current approach, subwords between source and target are not shared. A subword that is in both English and foreign vocabulary has two different embeddings.\nZero-shot Experiments ::: Estimating translation probabilities\nSince pre-trained models operate on subword level, we need to estimate subword translation probabilities. Therefore, we subsample 2M sentence pairs from each parallel corpus and tokenize the data into subwords before running fast-align BIBREF13.\nEstimating subword translation probabilities from aligned word vectors requires an additional processing step since the provided vectors from fastText are not at subword level. We use the following approximation to obtain subword vectors: the vector $_s$ of subword $s$ is the weighted average of all the aligned word vectors $_{w_i}$ that have $s$ as an subword\nwhere $p(w_j)$ is the unigram probability of word $w_j$ and $n_s = \\sum _{w_j:\\, s\\in w_j} p(w_j)$. We take the top 50,000 words in each aligned word-vectors to compute subword vectors.\nIn both cases, not all the words in the foreign vocabulary can be initialized from the English word-embeddings. Those words are initialized randomly from a Gaussian $\\mathcal {N}(0, {1}{d^2})$.\nZero-shot Experiments ::: Hyper-parameters\nIn all the experiments, we tune RAMEN$_{\\textsc {base}}$ for 175,000 updates and RAMEN$_{\\textsc {large}}$ for 275,000 updates where the first 25,000 updates are for language specific parameters. The sequence length is set to 256. The mini-batch ",
              "metadata": {
                "chunk_index": 6,
                "source_id": "3b069a546d5417be546f57d02981cf5ac68b4d3d6c55624a"
              },
              "score": 0.8395080118697105
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__12",
              "document": "0 tokens. Various beam sizes and length normalization penalties were applied for the table-to-text system, which are listed in the main paper. For the information extraction system, we found a beam size of 8 and no length penalty to produce the highest F-score on the dev set.\nSample Outputs\nTable TABREF55 shows some sample references and the corresponding predictions from the best performing model, PG-Net for WikiBio.",
              "metadata": {
                "chunk_index": 12,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.7772302597344181
            }
          ],
          "reranked": [
            {
              "id": "3b069a546d5417be546f57d02981cf5ac68b4d3d6c55624a__6",
              "document": " language might have more than one treebank in Universal Dependencies, we use the following treebanks: en_ewt (English), fr_gsd (French), ru_syntagrus (Russian) ar_padt (Arabic), vi_vtb (Vietnamese), hi_hdtb (Hindi), and zh_gsd (Chinese).\nZero-shot Experiments ::: Data ::: Remark on BPE\nBIBREF22 show that sharing subwords between languages improves alignments between embedding spaces. BIBREF2 observe a strong correlation between the percentage of overlapping subwords and mBERT's performances for cross-lingual zero-shot transfer. However, in our current approach, subwords between source and target are not shared. A subword that is in both English and foreign vocabulary has two different embeddings.\nZero-shot Experiments ::: Estimating translation probabilities\nSince pre-trained models operate on subword level, we need to estimate subword translation probabilities. Therefore, we subsample 2M sentence pairs from each parallel corpus and tokenize the data into subwords before running fast-align BIBREF13.\nEstimating subword translation probabilities from aligned word vectors requires an additional processing step since the provided vectors from fastText are not at subword level. We use the following approximation to obtain subword vectors: the vector $_s$ of subword $s$ is the weighted average of all the aligned word vectors $_{w_i}$ that have $s$ as an subword\nwhere $p(w_j)$ is the unigram probability of word $w_j$ and $n_s = \\sum _{w_j:\\, s\\in w_j} p(w_j)$. We take the top 50,000 words in each aligned word-vectors to compute subword vectors.\nIn both cases, not all the words in the foreign vocabulary can be initialized from the English word-embeddings. Those words are initialized randomly from a Gaussian $\\mathcal {N}(0, {1}{d^2})$.\nZero-shot Experiments ::: Hyper-parameters\nIn all the experiments, we tune RAMEN$_{\\textsc {base}}$ for 175,000 updates and RAMEN$_{\\textsc {large}}$ for 275,000 updates where the first 25,000 updates are for language specific parameters. The sequence length is set to 256. The mini-batch ",
              "metadata": {
                "chunk_index": 6,
                "source_id": "3b069a546d5417be546f57d02981cf5ac68b4d3d6c55624a"
              },
              "score": 0.8395080118697105
            },
            {
              "id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1__3",
              "document": "the length of $q_i$ . We consider a $\\text{match} (q_i, p_j)$ , if there is an exact string match of a sequence of words $\\lbrace w_k, w_{k+1}, .. w_{l_{q_i}}\\rbrace $ between the sentence $q_i$ and passage $p_j$ . If this sequence is either a noun phrase, verb phrase, adjective phrase or a named entity in $\\lbrace p_1, p_2, .. p_m\\rbrace $0 , as recognized by CoreNLP or BANNER, we select it as an answer span $\\lbrace p_1, p_2, .. p_m\\rbrace $1 . Additionally, we use $\\lbrace p_1, p_2, .. p_m\\rbrace $2 as the passage $\\lbrace p_1, p_2, .. p_m\\rbrace $3 and form a cloze question $\\lbrace p_1, p_2, .. p_m\\rbrace $4 from the answer bearing sentence $\\lbrace p_1, p_2, .. p_m\\rbrace $5 by replacing $\\lbrace p_1, p_2, .. p_m\\rbrace $6 with a placeholder. As a result, we obtain passage-question-answer ( $\\lbrace p_1, p_2, .. p_m\\rbrace $7 ) triples (Table 1 shows an example). As a post-processing step, we prune out $\\lbrace p_1, p_2, .. p_m\\rbrace $8 triples where the word overlap between the question (Q) and passage (P) is less than 2 words (after excluding the stop words).\nThe process relies on the fact that answer candidates from the introduction are likely to be discussed in detail in the remainder of the article. In effect, the cloze question from the introduction and the matching paragraph in the body forms a question and context passage pair. We create two cloze datasets, one each from Wikipedia corpus (for SQuAD and TriviaQA) and PUBMed academic papers (for the BioASQ challenge), consisting of 2.2M and 1M clozes respectively. From analyzing the cloze data manually, we were able to answer 76% times for the Wikipedia set and 80% times for the PUBMed set using the information in the passage. In most cases the cloze paraphrased the information in the passage, which we hypothesized to be a useful signal for the downstream QA task.\nWe also investigate the utility of forming subsets of the large cloze corpus, where we select the top passage-question-answer triples, based on the different criteria, like i) jaccard simil",
              "metadata": {
                "chunk_index": 3,
                "source_id": "ca91b274c44baec976e7acbd926d7d823fd5a81e34dfcce1"
              },
              "score": 0.8623104221866078
            },
            {
              "id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7__12",
              "document": "0 tokens. Various beam sizes and length normalization penalties were applied for the table-to-text system, which are listed in the main paper. For the information extraction system, we found a beam size of 8 and no length penalty to produce the highest F-score on the dev set.\nSample Outputs\nTable TABREF55 shows some sample references and the corresponding predictions from the best performing model, PG-Net for WikiBio.",
              "metadata": {
                "chunk_index": 12,
                "source_id": "7e339da53449dbd58a1e815d55e440e712af6a1950f342b7"
              },
              "score": 0.7772302597344181
            }
          ],
          "pruned_text": "language might have more than one treebank in Universal Dependencies, we use the following treebanks: en_ewt (English), fr_gsd (French), ru_syntagrus (Russian), ar_padt (Arabic), vi_vtb (Vietnamese), hi_hdtb (Hindi), and zh_gsd (Chinese).\n\nZero-shot Experiments ::: Estimating translation probabilities\nSince pre-trained models operate on subword level, we need to estimate subword translation probabilities. Therefore, we subsample 2M sentence pairs from each parallel corpus and tokenize the data into subwords before running fast-align BIBREF13. Estimating subword translation probabilities from aligned word vectors requires an additional processing step since the provided vectors from fastText are not at subword level. We use the following approximation to obtain subword vectors: the vector $_s$ of subword $s$ is the weighted average of all the aligned word vectors $_{w_i}$ that have $s$ as an subword where $p(w_j)$ is the unigram probability of word $w_j$ and $n_s = \\sum _{w_j:\\, s\\in w_j} p(w_j)$. We take the top 50,000 words in each aligned word-vectors to compute subword vectors. In both cases, not all the words in the foreign vocabulary can be initialized from the English word-embeddings. Those words are initialized randomly from a Gaussian $\\mathcal {N}(0, {1}{d^2})$.\n\nZero-shot Experiments ::: Hyper-parameters\nIn all the experiments, we tune RAMEN$_{\\textsc {base}}$ for 175,000 updates and RAMEN$_{\\textsc {large}}$ for 275,000 updates where the first 25,000 updates are for language specific parameters. The sequence length is set to 256. The mini-batch the length of $q_i$. We consider a $\\text{match} (q_i, p_j)$, if there is an exact string match of a sequence of words $\\lbrace w_k, w_{k+1}, .. w_{l_{q_i}}\\rbrace $ between the sentence $q_i$ and passage $p_j$. If this sequence is either a noun phrase, verb phrase, adjective phrase or a named entity in $\\lbrace p_1, p_2, .. p_m\\rbrace $0, as recognized by CoreNLP or BANNER, we select it as an answer span $\\lbrace p_1, p_2, .. p_m\\rbrace $1. Additionally, we use $\\lbrace p_1, p_2, .. p_m\\rbrace $2 as the passage $\\lbrace p_1, p_2, .. p_m\\rbrace $3 and form a cloze question $\\lbrace p_1, p_2, .. p_m\\rbrace $4 from the answer bearing sentence $\\lbrace p_1, p_2, .. p_m\\rbrace $5 by replacing $\\lbrace p_1, p_2, .. p_m\\rbrace $6 with a placeholder. As a result, we obtain passage-question-answer ($\\lbrace p_1, p_2, .. p_m\\rbrace $7) triples. As a post-processing step, we prune out $\\lbrace p_1, p_2, .. p_m\\rbrace $8 triples where the word overlap between the question (Q) and passage (P) is less than 2 words (after excluding the stop words).\n\nThe process relies on the fact that answer candidates from the introduction are likely to be discussed in detail in the remainder of the article. In effect, the cloze question from the introduction and the matching paragraph in the body forms a question and context passage pair. We create two cloze datasets, one each from Wikipedia corpus (for SQuAD and TriviaQA) and PUBMed academic papers (for the BioASQ challenge), consisting of 2.2M and 1M clozes respectively. From analyzing the cloze data manually, we were able to answer 76% times for the Wikipedia set and 80% times for the PUBMed set using the information in the passage. In most cases the cloze paraphrased the information in the passage, which we hypothesized to be a useful signal for the downstream QA task.",
          "answer": "I do not know."
        },
        {
          "query": "What are the 12 languages covered?",
          "rewritten": "What are the 12 languages covered?",
          "retrieval": [
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__15",
              "document": "he language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family). Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex. We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project.\nThe work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.\nMulti-SimLex: Translation and Annotation ::: Word Pair Translation\nTranslators for each target language were instructed to find direct or approximate translations for the 1,888 word pairs that satisfy the following rules. (1) All pairs in the translated set must be unique (i.e., no duplicate pairs); (2) Translating two words from the same English pair into the same word in the target language is not allowed (e.g., it is not allowed to translate car and automobile to the same Spanish word coche). (3) The translated pairs must preserve the semantic relations between the two words when possible. This means that, when multiple translations are possible, the translation that best conveys the semantic relation between the two words found in the original English pair is selected. (4) If it is not possible to use a single-word translation in the target language, then a multi-word expression (MWE) can be us",
              "metadata": {
                "chunk_index": 15,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8508167338108972
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__48",
              "document": "ments on the semantic similarity of word pairs for 12 monolingual and 66 cross-lingual datasets. The languages covered are typologically diverse and include also under-resourced ones, such as Welsh and Kiswahili. The resource covers an unprecedented amount of 1,888 word pairs, carefully balanced according to their similarity score, frequency, concreteness, part-of-speech class, and lexical field. In addition to Multi-Simlex, we release the detailed protocol we followed to create this resource. We hope that our consistent guidelines will encourage researchers to translate and annotate Multi-Simlex -style datasets for additional languages. This can help and create a hugely valuable, large-scale semantic resource for multilingual NLP research.\nThe core Multi-SimLex we release with this paper already enables researchers to carry out novel linguistic analysis as well as establishes a benchmark for evaluating representation learning models. Based on our preliminary analyses, we found that speakers of closely related languages tend to express equivalent similarity judgments. In particular, geographical proximity seems to play a greater role than family membership in determining the similarity of judgments across languages. Moreover, we tested several state-of-the-art word embedding models, both static and contextualized representations, as well as several (supervised and unsupervised) post-processing techniques, on the newly released Multi-SimLex. This enables future endeavors to improve multilingual representation learning with challenging baselines. In addition, our results provide several important insights for research on both monolingual and cross-lingual word representations:\n1) Unsupervised post-processing techniques (mean centering, elimination of top principal components, adjusting similarity orders) are always beneficial independently of the language, although the combination leading to the best scores is language-specific and hence needs to be tuned.\n2) Similarity rankings obtained from word embeddings for no",
              "metadata": {
                "chunk_index": 48,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.7125174751126317
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__2",
              "document": "oncept pairs for typologically diverse languages. We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese). We demonstrate that our proposed dataset creation procedure yields data with high inter-annotator agreement rates (e.g., the average mean inter-annotator agreement for Welsh is 0.742).\nThe unified construction protocol and alignment between concept pairs enables a series of quantitative analyses. Preliminary studies on the influence that polysemy and cross-lingual variation in lexical categories (see §SECREF6) have on similarity judgments are provided in §SECREF5. Data created according to Multi-SimLex protocol also allow for probing into whether similarity judgments are universal across languages, or rather depend on linguistic affinity (in terms of linguistic features, phylogeny, and geographical location). We investigate this question in §SECREF25. Naturally, Multi-SimLex datasets can be used as an intrinsic evaluation benchmark to assess the quality of lexical representations based on monolingual, joint multilingual, and transfer learning paradigms. We conduct a systematic evaluation of several state-of-the-art representation models in §SECREF7, showing that there are large gaps between human and system performance in all languages. The proposed construction paradigm also supports the automatic creation of 66 cross-lingual Multi-SimLex datasets by interleaving the monolingual ones. We outline the construction of the cross-lingual datasets in §SECREF6, and then present a quantitative evaluation of a series of cutting-edge cross-lingual representation models on this benchmark in §SECREF8.\nContributions. We now summarize the main contributions of this work:\n1) Building on lessons learned from prior work, we create a more comprehensive lexical semantic similarity dataset for the English language spanning a total of 1,888 concept pairs balanced with respect to si",
              "metadata": {
                "chunk_index": 2,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.6887144249030206
            }
          ],
          "reranked": [
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__48",
              "document": "ments on the semantic similarity of word pairs for 12 monolingual and 66 cross-lingual datasets. The languages covered are typologically diverse and include also under-resourced ones, such as Welsh and Kiswahili. The resource covers an unprecedented amount of 1,888 word pairs, carefully balanced according to their similarity score, frequency, concreteness, part-of-speech class, and lexical field. In addition to Multi-Simlex, we release the detailed protocol we followed to create this resource. We hope that our consistent guidelines will encourage researchers to translate and annotate Multi-Simlex -style datasets for additional languages. This can help and create a hugely valuable, large-scale semantic resource for multilingual NLP research.\nThe core Multi-SimLex we release with this paper already enables researchers to carry out novel linguistic analysis as well as establishes a benchmark for evaluating representation learning models. Based on our preliminary analyses, we found that speakers of closely related languages tend to express equivalent similarity judgments. In particular, geographical proximity seems to play a greater role than family membership in determining the similarity of judgments across languages. Moreover, we tested several state-of-the-art word embedding models, both static and contextualized representations, as well as several (supervised and unsupervised) post-processing techniques, on the newly released Multi-SimLex. This enables future endeavors to improve multilingual representation learning with challenging baselines. In addition, our results provide several important insights for research on both monolingual and cross-lingual word representations:\n1) Unsupervised post-processing techniques (mean centering, elimination of top principal components, adjusting similarity orders) are always beneficial independently of the language, although the combination leading to the best scores is language-specific and hence needs to be tuned.\n2) Similarity rankings obtained from word embeddings for no",
              "metadata": {
                "chunk_index": 48,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.7125174751126317
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__2",
              "document": "oncept pairs for typologically diverse languages. We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese). We demonstrate that our proposed dataset creation procedure yields data with high inter-annotator agreement rates (e.g., the average mean inter-annotator agreement for Welsh is 0.742).\nThe unified construction protocol and alignment between concept pairs enables a series of quantitative analyses. Preliminary studies on the influence that polysemy and cross-lingual variation in lexical categories (see §SECREF6) have on similarity judgments are provided in §SECREF5. Data created according to Multi-SimLex protocol also allow for probing into whether similarity judgments are universal across languages, or rather depend on linguistic affinity (in terms of linguistic features, phylogeny, and geographical location). We investigate this question in §SECREF25. Naturally, Multi-SimLex datasets can be used as an intrinsic evaluation benchmark to assess the quality of lexical representations based on monolingual, joint multilingual, and transfer learning paradigms. We conduct a systematic evaluation of several state-of-the-art representation models in §SECREF7, showing that there are large gaps between human and system performance in all languages. The proposed construction paradigm also supports the automatic creation of 66 cross-lingual Multi-SimLex datasets by interleaving the monolingual ones. We outline the construction of the cross-lingual datasets in §SECREF6, and then present a quantitative evaluation of a series of cutting-edge cross-lingual representation models on this benchmark in §SECREF8.\nContributions. We now summarize the main contributions of this work:\n1) Building on lessons learned from prior work, we create a more comprehensive lexical semantic similarity dataset for the English language spanning a total of 1,888 concept pairs balanced with respect to si",
              "metadata": {
                "chunk_index": 2,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.6887144249030206
            },
            {
              "id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff__15",
              "document": "he language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family). Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex. We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project.\nThe work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.\nMulti-SimLex: Translation and Annotation ::: Word Pair Translation\nTranslators for each target language were instructed to find direct or approximate translations for the 1,888 word pairs that satisfy the following rules. (1) All pairs in the translated set must be unique (i.e., no duplicate pairs); (2) Translating two words from the same English pair into the same word in the target language is not allowed (e.g., it is not allowed to translate car and automobile to the same Spanish word coche). (3) The translated pairs must preserve the semantic relations between the two words when possible. This means that, when multiple translations are possible, the translation that best conveys the semantic relation between the two words found in the original English pair is selected. (4) If it is not possible to use a single-word translation in the target language, then a multi-word expression (MWE) can be us",
              "metadata": {
                "chunk_index": 15,
                "source_id": "388c857ed1b4175e114a0ce9f3489766797a32cd183f10ff"
              },
              "score": 0.8508167338108972
            }
          ],
          "pruned_text": "The languages covered are typologically diverse and include under-resourced ones, such as Welsh and Kiswahili. We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese). We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili.",
          "answer": "The 12 languages covered are Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, and several others that are not specified in the context."
        },
        {
          "query": "Were any of the pipeline components based on deep learning models?",
          "rewritten": "Were any of the pipeline components based on deep learning models?",
          "retrieval": [
            {
              "id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48__3",
              "document": "l. BIBREF19 . In addition to masking layer norm, we additionally applied masking prior to the convolution operation, and masking the mean and variance calculation in batch norm. These results are shown in Table TABREF16 . Interestingly, we found that while masking before convolution gives a lower WER, using masks for both convolutions and batch norm results in worse performance.\nAs a final note, we found that training with weight norm was very unstable leading to exploding activations.\nResidual Connections\nFor models deeper than Jasper 5x3, we observe consistently that residual connections are necessary for training to converge. In addition to the simple residual and dense residual model described above, we investigated DenseNet BIBREF15 and DenseRNet BIBREF16 variants of Jasper. Both connect the outputs of each sub-block to the inputs of following sub-blocks within a block. DenseRNet, similar to Dense Residual, connects the output of each output of each block to the input of all following blocks. DenseNet and DenseRNet combine residual connections using concatenation whereas Residual and Dense Residual use addition. We found that Dense Residual and DenseRNet perform similarly with each performing better on specific subsets of LibriSpeech. We decided to use Dense Residual for subsequent experiments. The main reason is that due to concatenation, the growth factor for DenseNet and DenseRNet requires tuning for deeper models whereas Dense Residual simply just repeats a sub-blocks.\nLanguage Model\nA language model (LM) is a probability distribution over arbitrary symbol sequences INLINEFORM0 such that more likely sequences are assigned high probabilities. LMs are frequently used to condition beam search. During decoding, candidates are evaluated using both acoustic scores and LM scores. Traditional N-gram LMs have been augmented with neural LMs in recent work BIBREF20 , BIBREF21 , BIBREF22 .\nWe experiment with statistical N-gram language models BIBREF23 and neural Transformer-XL BIBREF11 models. Our best results use a",
              "metadata": {
                "chunk_index": 3,
                "source_id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48"
              },
              "score": 0.7534122808914934
            },
            {
              "id": "6ab7a0f094ebc681c85b0bdbf251970c5fc6bc638da5e3b5__5",
              "document": "g by using random subsets of training subtrees to parse new sentences. BIBREF46 utilize an incremental algorithm to unsupervised parsing which makes local decisions to create constituents based on a complex set of heuristics. BIBREF47 induce parse trees through cascaded applications of finite state models.\nMore recently, neural network-based approaches to grammar induction have shown promising results on inducing parse trees directly from words. BIBREF14 , BIBREF15 learn tree structures through soft gating layers within neural language models, while BIBREF16 combine recursive autoencoders with the inside-outside algorithm. BIBREF17 train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees, and BIBREF78 utilize image captions to identify and ground constituents.\nOur work is also related to latent variable PCFGs BIBREF79 , BIBREF80 , BIBREF81 , which extend PCFGs to the latent variable setting by splitting nonterminal symbols into latent subsymbols. In particular, latent vector grammars BIBREF82 and compositional vector grammars BIBREF83 also employ continuous vectors within their grammars. However these approaches have been employed for learning supervised parsers on annotated treebanks, in contrast to the unsupervised setting of the current work.\nConclusion\nThis work explores grammar induction with compound PCFGs, which modulate rule probabilities with per-sentence continuous latent vectors. The latent vector induces marginal dependencies beyond the traditional first-order context-free assumptions within a tree-based generative process, leading to improved performance. The collapsed amortized variational inference approach is general and can be used for generative models which admit tractable inference through partial conditioning. Learning deep generative models which exhibit such conditional Markov properties is an interesting direction for future work.\nAcknowledgments\nWe thank Phil Blunsom for initial discussions which seeded many of the core ideas in the p",
              "metadata": {
                "chunk_index": 5,
                "source_id": "6ab7a0f094ebc681c85b0bdbf251970c5fc6bc638da5e3b5"
              },
              "score": 0.7492099224045365
            },
            {
              "id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48__4",
              "document": "nd neural Transformer-XL BIBREF11 models. Our best results use acoustic and word-level N-gram language models to generate a candidate list using beam search with a width of 2048. Next, an external Transformer-XL LM rescores the final list. All LMs were trained on datasets independently from acoustic models. We show results with the neural LM in our Results section. We observed a strong correlation between the quality of the neural LM (measured by perplexity) and WER as shown in Figure FIGREF20 .\nNovoGrad\nFor training, we use either Stochastic Gradient Descent (SGD) with momentum or our own NovoGrad, an optimizer similar to Adam BIBREF14 , except that its second moments are computed per layer instead of per weight. Compared to Adam, it reduces memory consumption and we find it to be more numerically stable.\nAt each step INLINEFORM0 , NovoGrad computes the stochastic gradient INLINEFORM1 following the regular forward-backward pass. Then the second-order moment INLINEFORM2 is computed for each layer INLINEFORM3 similar to ND-Adam BIBREF27 : DISPLAYFORM0\nThe second-order moment INLINEFORM0 is used to re-scale gradients INLINEFORM1 before calculating the first-order moment INLINEFORM2 : DISPLAYFORM0\nIf L2-regularization is used, a weight decay INLINEFORM0 is added to the re-scaled gradient (as in AdamW BIBREF28 ): DISPLAYFORM0\nFinally, new weights are computed using the learning rate INLINEFORM0 : DISPLAYFORM0\nUsing NovoGrad instead of SGD with momentum, we decreased the WER on dev-clean LibriSpeech from 4.00% to 3.64%, a relative improvement of 9% for Jasper DR 10x5. We will further analyze NovoGrad in forthcoming work.\nResults\nWe evaluate Jasper across a number of datasets in various domains. In all experiments, we use dropout and weight decay as regularization. At training time, we use speed perturbation with fixed +/-10% BIBREF29 for LibriSpeech. For WSJ and Hub5'00, we use a random speed perturbation factor between [-10%, 10%] as each utterance is fed into the model. All models have been trained on NVIDIA DGX-1 i",
              "metadata": {
                "chunk_index": 4,
                "source_id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48"
              },
              "score": 0.7310350810556501
            }
          ],
          "reranked": [
            {
              "id": "6ab7a0f094ebc681c85b0bdbf251970c5fc6bc638da5e3b5__5",
              "document": "g by using random subsets of training subtrees to parse new sentences. BIBREF46 utilize an incremental algorithm to unsupervised parsing which makes local decisions to create constituents based on a complex set of heuristics. BIBREF47 induce parse trees through cascaded applications of finite state models.\nMore recently, neural network-based approaches to grammar induction have shown promising results on inducing parse trees directly from words. BIBREF14 , BIBREF15 learn tree structures through soft gating layers within neural language models, while BIBREF16 combine recursive autoencoders with the inside-outside algorithm. BIBREF17 train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees, and BIBREF78 utilize image captions to identify and ground constituents.\nOur work is also related to latent variable PCFGs BIBREF79 , BIBREF80 , BIBREF81 , which extend PCFGs to the latent variable setting by splitting nonterminal symbols into latent subsymbols. In particular, latent vector grammars BIBREF82 and compositional vector grammars BIBREF83 also employ continuous vectors within their grammars. However these approaches have been employed for learning supervised parsers on annotated treebanks, in contrast to the unsupervised setting of the current work.\nConclusion\nThis work explores grammar induction with compound PCFGs, which modulate rule probabilities with per-sentence continuous latent vectors. The latent vector induces marginal dependencies beyond the traditional first-order context-free assumptions within a tree-based generative process, leading to improved performance. The collapsed amortized variational inference approach is general and can be used for generative models which admit tractable inference through partial conditioning. Learning deep generative models which exhibit such conditional Markov properties is an interesting direction for future work.\nAcknowledgments\nWe thank Phil Blunsom for initial discussions which seeded many of the core ideas in the p",
              "metadata": {
                "chunk_index": 5,
                "source_id": "6ab7a0f094ebc681c85b0bdbf251970c5fc6bc638da5e3b5"
              },
              "score": 0.7492099224045365
            },
            {
              "id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48__4",
              "document": "nd neural Transformer-XL BIBREF11 models. Our best results use acoustic and word-level N-gram language models to generate a candidate list using beam search with a width of 2048. Next, an external Transformer-XL LM rescores the final list. All LMs were trained on datasets independently from acoustic models. We show results with the neural LM in our Results section. We observed a strong correlation between the quality of the neural LM (measured by perplexity) and WER as shown in Figure FIGREF20 .\nNovoGrad\nFor training, we use either Stochastic Gradient Descent (SGD) with momentum or our own NovoGrad, an optimizer similar to Adam BIBREF14 , except that its second moments are computed per layer instead of per weight. Compared to Adam, it reduces memory consumption and we find it to be more numerically stable.\nAt each step INLINEFORM0 , NovoGrad computes the stochastic gradient INLINEFORM1 following the regular forward-backward pass. Then the second-order moment INLINEFORM2 is computed for each layer INLINEFORM3 similar to ND-Adam BIBREF27 : DISPLAYFORM0\nThe second-order moment INLINEFORM0 is used to re-scale gradients INLINEFORM1 before calculating the first-order moment INLINEFORM2 : DISPLAYFORM0\nIf L2-regularization is used, a weight decay INLINEFORM0 is added to the re-scaled gradient (as in AdamW BIBREF28 ): DISPLAYFORM0\nFinally, new weights are computed using the learning rate INLINEFORM0 : DISPLAYFORM0\nUsing NovoGrad instead of SGD with momentum, we decreased the WER on dev-clean LibriSpeech from 4.00% to 3.64%, a relative improvement of 9% for Jasper DR 10x5. We will further analyze NovoGrad in forthcoming work.\nResults\nWe evaluate Jasper across a number of datasets in various domains. In all experiments, we use dropout and weight decay as regularization. At training time, we use speed perturbation with fixed +/-10% BIBREF29 for LibriSpeech. For WSJ and Hub5'00, we use a random speed perturbation factor between [-10%, 10%] as each utterance is fed into the model. All models have been trained on NVIDIA DGX-1 i",
              "metadata": {
                "chunk_index": 4,
                "source_id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48"
              },
              "score": 0.7310350810556501
            },
            {
              "id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48__3",
              "document": "l. BIBREF19 . In addition to masking layer norm, we additionally applied masking prior to the convolution operation, and masking the mean and variance calculation in batch norm. These results are shown in Table TABREF16 . Interestingly, we found that while masking before convolution gives a lower WER, using masks for both convolutions and batch norm results in worse performance.\nAs a final note, we found that training with weight norm was very unstable leading to exploding activations.\nResidual Connections\nFor models deeper than Jasper 5x3, we observe consistently that residual connections are necessary for training to converge. In addition to the simple residual and dense residual model described above, we investigated DenseNet BIBREF15 and DenseRNet BIBREF16 variants of Jasper. Both connect the outputs of each sub-block to the inputs of following sub-blocks within a block. DenseRNet, similar to Dense Residual, connects the output of each output of each block to the input of all following blocks. DenseNet and DenseRNet combine residual connections using concatenation whereas Residual and Dense Residual use addition. We found that Dense Residual and DenseRNet perform similarly with each performing better on specific subsets of LibriSpeech. We decided to use Dense Residual for subsequent experiments. The main reason is that due to concatenation, the growth factor for DenseNet and DenseRNet requires tuning for deeper models whereas Dense Residual simply just repeats a sub-blocks.\nLanguage Model\nA language model (LM) is a probability distribution over arbitrary symbol sequences INLINEFORM0 such that more likely sequences are assigned high probabilities. LMs are frequently used to condition beam search. During decoding, candidates are evaluated using both acoustic scores and LM scores. Traditional N-gram LMs have been augmented with neural LMs in recent work BIBREF20 , BIBREF21 , BIBREF22 .\nWe experiment with statistical N-gram language models BIBREF23 and neural Transformer-XL BIBREF11 models. Our best results use a",
              "metadata": {
                "chunk_index": 3,
                "source_id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48"
              },
              "score": 0.7534122808914934
            }
          ],
          "pruned_text": "More recently, neural network-based approaches to grammar induction have shown promising results on inducing parse trees directly from words. BIBREF14, BIBREF15 learn tree structures through soft gating layers within neural language models, while BIBREF16 combine recursive autoencoders with the inside-outside algorithm. BIBREF17 train unsupervised recurrent neural network grammars with a structured inference network to induce latent trees. Our work explores grammar induction with compound PCFGs, which modulate rule probabilities with per-sentence continuous latent vectors. The collapsed amortized variational inference approach is general and can be used for generative models which admit tractable inference through partial conditioning. Learning deep generative models which exhibit such conditional Markov properties is an interesting direction for future work. We thank Phil Blunsom for initial discussions which seeded many of the core ideas in the p and neural Transformer-XL BIBREF11 models. Our best results use acoustic and word-level N-gram language models to generate a candidate list using beam search with a width of 2048. We show results with the neural LM in our Results section. We observed a strong correlation between the quality of the neural LM (measured by perplexity) and WER as shown in Figure FIGREF20. We evaluate Jasper across a number of datasets in various domains. In all experiments, we use dropout and weight decay as regularization. We found that training with weight norm was very unstable leading to exploding activations. For models deeper than Jasper 5x3, we observe consistently that residual connections are necessary for training to converge. We found that Dense Residual and DenseRNet perform similarly with each performing better on specific subsets of LibriSpeech. A language model (LM) is a probability distribution over arbitrary symbol sequences INLINEFORM0 such that more likely sequences are assigned high probabilities. LMs are frequently used to condition beam search. During decoding, candidates are evaluated using both acoustic scores and LM scores. Traditional N-gram LMs have been augmented with neural LMs in recent work BIBREF20, BIBREF21, BIBREF22.",
          "answer": "Yes, some of the pipeline components are based on deep learning models, such as neural network-based approaches to grammar induction and the use of neural language models (LMs)."
        },
        {
          "query": "What are the datasets used for evaluation?",
          "rewritten": "What are the datasets used for evaluation?",
          "retrieval": [
            {
              "id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12__3",
              "document": "s non-zero scores for other emotions. At this stage, we didn't perform any other entropy-based selection of posts, to be investigated in future work.\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation.\nAffective Text dataset\nTask 14 at SemEval 2007 BIBREF7 was concerned with the classification of emotions and valence in news headlines. The headlines where collected from several news websites including Google news, The New York Times, BBC News and CNN. The used emotion labels were Anger, Disgust, Fear, Joy, Sadness, Surprise, in line with the six basic emotions of Ekman's standard model BIBREF8 . Valence was to be determined as positive or negative. Classification of emotion and valence were treated as separate tasks. Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision, recall, and f-sco",
              "metadata": {
                "chunk_index": 3,
                "source_id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12"
              },
              "score": 0.8744680023145965
            },
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__14",
              "document": "tent may produce disfluent or ungrammatical output. Specifically, we followed the Best-Worst Scaling BIBREF34 method where participants were presented with the output of two systems (and the original document) and asked to decide which one was better according to the criteria of Informativeness, Fluency, and Succinctness.\nBoth types of evaluation were conducted on the Amazon Mechanical Turk platform. For the CNN/DailyMail and NYT datasets we used the same documents (20 in total) and questions from previous work BIBREF8, BIBREF18. For XSum, we randomly selected 20 documents (and their questions) from the release of BIBREF22. We elicited 3 responses per HIT. With regard to QA evaluation, we adopted the scoring mechanism from BIBREF33; correct answers were marked with a score of one, partially correct answers with 0.5, and zero otherwise. For quality-based evaluation, the rating of each system was computed as the percentage of times it was chosen as better minus the times it was selected as worse. Ratings thus range from -1 (worst) to 1 (best).\nResults for extractive and abstractive systems are shown in Tables TABREF37 and TABREF38, respectively. We compared the best performing BertSum model in each setting (extractive or abstractive) against various state-of-the-art systems (whose output is publicly available), the Lead baseline, and the Gold standard as an upper bound. As shown in both tables participants overwhelmingly prefer the output of our model against comparison systems across datasets and evaluation paradigms. All differences between BertSum and comparison models are statistically significant ($p<0.05$), with the exception of TConvS2S (see Table TABREF38; XSum) in the QA evaluation setting.\nConclusions\nIn this paper, we showcased how pretrained Bert can be usefully applied in text summarization. We introduced a novel document-level encoder and proposed a general framework for both abstractive and extractive summarization. Experimental results across three datasets show that our model achieves state-of-the-",
              "metadata": {
                "chunk_index": 14,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.8427990726897691
            },
            {
              "id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1__5",
              "document": "e in the number of labeled features or in class distribution. Then, to test the influence of $\\lambda $ , we conduct some experiments with the method which incorporates the KL divergence of class distribution. Last, we evaluate our approaches in 9 commonly used text classification datasets. We set $\\lambda = 5|K|$ by default in all experiments unless there is explicit declaration. The baseline we choose here is GE-FL BIBREF0 , a method based on generalization expectation criteria.\nData Preparation\nWe evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. We use bag-of-words feature and remove stopwords in the preprocess stage. Though we have labels of all documents, we do not use them during the learning process, instead, we use the label of features.\nThe movie dataset, in which the task is to classify the movie reviews as positive or negtive, is used for testing the proposed approaches with unbalanced labeled features, unbalanced datasets or different $\\lambda $ parameters. All unbalanced datasets are constructed based on the movie dataset by randomly removing documents of the positive class. For each experiment, we conduct 10-fold cross validation.\nAs described in BIBREF0 , there are two ways to obtain labeled features. The first way is to use information gain. We first calculate the mutual information of all features according to the labels of the documents and select the top 20 as labeled features for each class as a feature pool. Note that using information gain requires the document label, but this is only to simulate how we human provide prior knowledge to the model. The second way is to use LDA BIBREF9 to select features. We use the same selection process as BIBREF0 , where they first train a LDA on the dataset, and then select the most probable features of each topic (sorted by $P(w_i|t_j)$ , the probability of word $w_i$ given topic $t_j$ ).\nSimilar to BIBREF10 , BIBREF0 , we estimate the reference distribution of the labele",
              "metadata": {
                "chunk_index": 5,
                "source_id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1"
              },
              "score": 0.7789209025971406
            }
          ],
          "reranked": [
            {
              "id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12__3",
              "document": "s non-zero scores for other emotions. At this stage, we didn't perform any other entropy-based selection of posts, to be investigated in future work.\nEmotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation.\nAffective Text dataset\nTask 14 at SemEval 2007 BIBREF7 was concerned with the classification of emotions and valence in news headlines. The headlines where collected from several news websites including Google news, The New York Times, BBC News and CNN. The used emotion labels were Anger, Disgust, Fear, Joy, Sadness, Surprise, in line with the six basic emotions of Ekman's standard model BIBREF8 . Valence was to be determined as positive or negative. Classification of emotion and valence were treated as separate tasks. Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/developing data amounted to 250 annotated headlines (Affective development), while systems were evaluated on another 1000 (Affective test). Evaluation was done using two different methods: a fine-grained evaluation using Pearson's r to measure the correlation between the system scores and the gold standard; and a coarse-grained method where each emotion score was converted to a binary label, and precision, recall, and f-sco",
              "metadata": {
                "chunk_index": 3,
                "source_id": "88d89e5b02c860bd1fdac17796e2b6048a6d2b86950c4c12"
              },
              "score": 0.8744680023145965
            },
            {
              "id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58__14",
              "document": "tent may produce disfluent or ungrammatical output. Specifically, we followed the Best-Worst Scaling BIBREF34 method where participants were presented with the output of two systems (and the original document) and asked to decide which one was better according to the criteria of Informativeness, Fluency, and Succinctness.\nBoth types of evaluation were conducted on the Amazon Mechanical Turk platform. For the CNN/DailyMail and NYT datasets we used the same documents (20 in total) and questions from previous work BIBREF8, BIBREF18. For XSum, we randomly selected 20 documents (and their questions) from the release of BIBREF22. We elicited 3 responses per HIT. With regard to QA evaluation, we adopted the scoring mechanism from BIBREF33; correct answers were marked with a score of one, partially correct answers with 0.5, and zero otherwise. For quality-based evaluation, the rating of each system was computed as the percentage of times it was chosen as better minus the times it was selected as worse. Ratings thus range from -1 (worst) to 1 (best).\nResults for extractive and abstractive systems are shown in Tables TABREF37 and TABREF38, respectively. We compared the best performing BertSum model in each setting (extractive or abstractive) against various state-of-the-art systems (whose output is publicly available), the Lead baseline, and the Gold standard as an upper bound. As shown in both tables participants overwhelmingly prefer the output of our model against comparison systems across datasets and evaluation paradigms. All differences between BertSum and comparison models are statistically significant ($p<0.05$), with the exception of TConvS2S (see Table TABREF38; XSum) in the QA evaluation setting.\nConclusions\nIn this paper, we showcased how pretrained Bert can be usefully applied in text summarization. We introduced a novel document-level encoder and proposed a general framework for both abstractive and extractive summarization. Experimental results across three datasets show that our model achieves state-of-the-",
              "metadata": {
                "chunk_index": 14,
                "source_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58"
              },
              "score": 0.8427990726897691
            },
            {
              "id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1__5",
              "document": "e in the number of labeled features or in class distribution. Then, to test the influence of $\\lambda $ , we conduct some experiments with the method which incorporates the KL divergence of class distribution. Last, we evaluate our approaches in 9 commonly used text classification datasets. We set $\\lambda = 5|K|$ by default in all experiments unless there is explicit declaration. The baseline we choose here is GE-FL BIBREF0 , a method based on generalization expectation criteria.\nData Preparation\nWe evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. We use bag-of-words feature and remove stopwords in the preprocess stage. Though we have labels of all documents, we do not use them during the learning process, instead, we use the label of features.\nThe movie dataset, in which the task is to classify the movie reviews as positive or negtive, is used for testing the proposed approaches with unbalanced labeled features, unbalanced datasets or different $\\lambda $ parameters. All unbalanced datasets are constructed based on the movie dataset by randomly removing documents of the positive class. For each experiment, we conduct 10-fold cross validation.\nAs described in BIBREF0 , there are two ways to obtain labeled features. The first way is to use information gain. We first calculate the mutual information of all features according to the labels of the documents and select the top 20 as labeled features for each class as a feature pool. Note that using information gain requires the document label, but this is only to simulate how we human provide prior knowledge to the model. The second way is to use LDA BIBREF9 to select features. We use the same selection process as BIBREF0 , where they first train a LDA on the dataset, and then select the most probable features of each topic (sorted by $P(w_i|t_j)$ , the probability of word $w_i$ given topic $t_j$ ).\nSimilar to BIBREF10 , BIBREF0 , we estimate the reference distribution of the labele",
              "metadata": {
                "chunk_index": 5,
                "source_id": "eb64a6b64d9837a23cd1824be1986861b0b23d98ff1c52b1"
              },
              "score": 0.7789209025971406
            }
          ],
          "pruned_text": "Emotion datasets\nThree datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems: the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. Affective Text dataset: Task 14 at SemEval 2007 was concerned with the classification of emotions and valence in news headlines. The used emotion labels were Anger, Disgust, Fear, Joy, Sadness, Surprise. Training/developing data amounted to 250 annotated headlines, while systems were evaluated on another 1000. Evaluation was done using fine-grained and coarse-grained methods.",
          "answer": ""
        }
      ],
      "error": null,
      "errors": null
    }
  ]
}