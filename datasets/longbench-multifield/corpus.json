[
  {
    "id": "8fbf0a6531d9250e6bcda0c7ba456441f6d4073bf08de826",
    "content": "Time to clean house in Paso Robles Home\nFront Page » Time to clean house in Paso Robles\nSeptember 5, 2010 Opinion By JIM REED\nI’d like to give you an update on the issue of our civil servants cramming hundreds of millions of dollars in spending down our throats after the people of Paso Robles voted down the water rate increase last November. The rate increase is being hung up in the courts by the City Attorney. What was supposed to be a quick issue to get in front of a judge, has been drug out as long as possible by the City Attorney.\nEven if the courts throw out the current rate increase, I expect that our civil servants will just change a couple of words in the rate increase notice and force the same old plan on us again.\nThere is a real problem with the people we have hired to work for us in Paso Robles. It seems that decisions are made based on some agenda, even if it is contrary to citizens’ wishes.\nCity Councilmen Ed Steinbeck, Nick Gilman and Mayor Duane Picanco, on August 19th, voted unanimously to hire the same law firm employed by the City of Bell. You may have heard the recent news story about the City of Bell’s corrupt city representatives.\nThis law firm allowed the elected officials and City employees to pillage the General Fund for their own benefit, contrary to the rights and interests of the citizens. We are already paying several City employees $12,000 per month with equally ridiculous benefits and pensions. What does this say about our elected representatives?\nI believe most residents are like me. We elect people we believe have our best interest in mind. Over the last few years I have seen that nothing is farther from the truth. The people we have elected have lost track of the fact that “the City” exists to protect and deliver services to the citizens. To them it is some all-important ideal they strive to cultivate and improve according to their agenda. They have forgotten that they are elected to represent the citizens.\nWe have an election coming up in November. We have the opportunity to elect some responsible, principled people to represent us. If we elect more people from within this system, we will get more of the same type of government. We need to look at where the new candidates stand. Will they lawfully represent the citizens of the city? Or, are they happy with the way things are being run?\nWe have stood together in the past and have made real significant changes in important matters that are going to affect our lives for years to come. There are several thousand citizens that made their voice heard on the water issue, more than enough votes to make a change in our city government.\nPlease come out and vote for a democratic representative governing body for Paso Robles instead of the tyrannical leadership that exists now.\nJim Reed is a longtime resident of Paso Robles.\nSubjects: Opinion Paso Robles Paso Robles City Council Vote\tRelated:\n<- Previous Next ->\tEndless Summer Nights at Edna Valley, event photos Trial postponed for Paso Robles woman accused of forgery The comments below represent the opinion of the writer and do not represent the views or policies of CalCoastNews.com. (moderator@calcoastnews.com Comment Guidelines )\n2 whatisup says:\t09/13/2010 at 9:27 pm\npasoobserver – Here is something to observe and get you going in the right direction:\nCalifornia Government Code Section 65584\n(a) (1) For the fourth and subsequent revisions of the\nhousing element pursuant to Section 65588, the department shall\ndetermine the existing and projected need for housing for each region\npursuant to this article. For purposes of subdivision (a) of Section\n65583, the share of a city or county of the regional housing need\nshall include that share of the housing need of persons at all income\nlevels within the area significantly affected by the general plan of\n(2) While it is the intent of the Legislature that cities,\ncounties, and cities and counties should undertake all necessary\nactions to encourage, promote, and facilitate the development of\nhousing to accommodate the entire regional housing need, it is\nrecognized, however, that future housing production may not equal the\nregional housing need established for planning purposes.\n(b) The department, in consultation with each council of\ngovernments, shall determine each region’s existing and projected\nhousing need pursuant to Section 65584.01 at least two years prior to\nthe scheduled revision required pursuant to Section 65588. The\nappropriate council of governments, or for cities and counties\nwithout a council of governments, the department, shall adopt a final\nregional housing need plan that allocates a share of the regional\nhousing need to each city, county, or city and county at least one\nyear prior to the scheduled revision for the region required by\nSection 65588. The allocation plan prepared by a council of\ngovernments shall be prepared pursuant to Sections 65584.04 and\n65584.05 with the advice of the department.\n(c) Notwithstanding any other provision of law, the due dates for\nthe determinations of the department or for the council of\ngovernments, respectively, regarding the regional housing need may be\nextended by the department by not more than 60 days if the extension\nwill enable access to more recent critical population or housing\ndata from a pending or recent release of the United States Census\nBureau or the Department of Finance. If the due date for the\ndetermination of the department or the council of governments is\nextended for this reason, the department shall extend the\ncorresponding housing element revision deadline pursuant to Section\n65588 by not more than 60 days.\n(d) The regional housing needs allocation plan shall be consistent\nwith all of the following objectives:\n(1) Increasing the housing supply and the mix of housing types,\ntenure, and affordability in all cities and counties within the\nregion in an equitable manner, which shall result in each\njurisdiction receiving an allocation of units for low- and very low\n(2) Promoting infill development and socioeconomic equity, the\nprotection of environmental and agricultural resources, and the\nencouragement of efficient development patterns.\n(3) Promoting an improved intraregional relationship between jobs\n(4) Allocating a lower proportion of housing need to an income\ncategory when a jurisdiction already has a disproportionately high\nshare of households in that income category, as compared to the\ncountywide distribution of households in that category from the most\nrecent decennial United States census.\n(e) For purposes of this section, “household income levels” are as\ndetermined by the department as of the most recent decennial census\npursuant to the following code sections:\n(1) Very low incomes as defined by Section 50105 of the Health and\n(2) Lower incomes, as defined by Section 50079.5 of the Health and\n(3) Moderate incomes, as defined by Section 50093 of the Health\nand Safety Code.\n(4) Above moderate incomes are those exceeding the moderate-income\nlevel of Section 50093 of the Health and Safety Code.\n(f) Notwithstanding any other provision of law, determinations\nmade by the department, a council of governments, or a city or county\npursuant to this section or Section 65584.01, 65584.02, 65584.03,\n65584.04, 65584.05, 65584.06, 65584.07, or 65584.08 are exempt from\nthe California Environmental Quality Act (Division 13 (commencing\nwith Section 21000) of the Public Resources Code).\npasoobserver says:\t09/13/2010 at 6:52 pm\nTo whatisup —- First of all, I reviewed AB 602 Assembly Bill. Thanks. I am sorry to inform you but AB 602 is not the LAW as you so stated in your blog. I contacted the Deputy Chief Council’s office in Sacramento handling AB 602 to confirm your misstatement of facts. You know,in the English language, It shouldn’t be so difficult to answer some simple questions with a “YES” or “NO” answer. Yet, you are reluctant to do so, but you go on and on with a thesis along with some rhetoric. I never talked about a court suit over the “water issue”, I asked YOU, not about waiting for a court decision. Maybe, you did with some other people. Also, I was not ranting about the wineries usage of water. My response to you on your vague question about “there are people not paying their fair share for their use of water”. I related, are you talking about the wineries? I am well aware that most of the wineries are outside the city limits using the same aquifer. You took my question out of context., nice try! You are just being a popinjay and rhetorical. Also, you didn’t answer another question about “what is the unit cost of water” in Templeton? as compared to Paso Robles.\nwhatisup says:\t09/13/2010 at 8:54 pm\nI am on a well. I am sure you are capable of doing your own homework. I also am quite sure if you really contacted the Deputy Chief Counsel’s Office you have been set straight. What I gave you is a proposed small adjustment in the wide range of laws that make up the California Housing element. I assumed you could stumble onto the facts based on what I gave you. By the way, I believe you can review the Paso Robles Housing element plan on the City’s website or at the Library. The California Housing Element Laws that all cities and counties have to follow have been in place for almost 25 years. I realize you don’t actually have a clue how to look the laws up. Either educate yourself or keep making a fool of yourself, your choice. A simple Google search of California Housing Element Laws will get you going. Good Luck!\nTO WHATISUP — I WOULD LIKE TO KNOW WHAT LAW YOU ARE REFERRING TO THAT SAYS “WE” THE PEOPLE HAVE TO SUBSIDIZE NEW DEVELOPMENT? AGAIN, FOR THE THIRD TIME, YOU FAILED TO ANSWER MY QUESTIONS POSED TO YOU IN MY PRIOR RESPONSES TO YOU ON SEPT.10TH &11TH. IS THERE A REASON WHY YOU DON’T WANT TO ANSWER THEM? YOU DO WHAT OUR ELECTED OFFICIALS DO SO WELL, AND THAT IS “IN ONE EAR AND OUT OF THE OTHER EAR” IT SEEMS TO ME THAT YOU ARE EITHER EMPLOYED BY THE CITY OR YOU HAVE OTHER DEALING WITH THE CITY, SO BE IT. IT APPEARS TO ME THAT YOU THINK THE CITY DOES EVERYTHING RIGHT. APPARENTLY, YOU PRESENT YOURSELF AS BEING VERY BIAS ON CITY DECISIONS. IT LIKE THEY CAN’T DO ANYTHING WRONG ACCORDING TO YOUR LOGIC. THEY KNOW WHAT IS BEST FOR THE CITIZENS OF PASO,THAT IS A GOOD EXAMPLE OF ARROGANCE ALONG WITH NARCISSISM.\nWHAT PEOPLE ARE YOU TALKING ABOUT THAT DOESN’T PAY THEIR FAIR SHARE OF WATER? ARE YOU REFERRING TO THE WINERIES USING THE SAME AQUIFER?\nI BELIEVE YOU RELATED THAT YOU RESIDE IN TEMPLETON, BUT YOU OWN PROPERTY IN PASO. BY THE WAY, WHAT IS THE COST PER UNIT OF WATER USAGE IN TEMPLETON COMPARED TO PASO? OF COURSE, TEMPLETON IS IN AN UNINCORPORATED AREA (COUNTY JURISDICTION).\nWELL, I GAVE YOU SOME SUGGESTIONS ON HOW TO PAY FOR THE NACIMIENTO WATER PIPELINE AND SEWER TREATMENT PLANT. ALSO, REMEMBER IT’S THE CITIZENS’ MONEY THAT IS BEING SPENT. WHAT IS MOST IMPORTANT OF ALL, IS LET THE CITIZENS OF PASO DECIDE WITH THEIR VOTE ON HOW TO FINANCE THIS HUGE CAPITAL IMPROVEMENT PROJECT EXPENDITURE. JUST BE IN COMPLIANCE WITH STATE PROPOSITION 218 AND STOP CIRCUMVENTING THE LAW.\nWOULD YOU OBJECT TO HAVING TO FINANCE SOME NEW BONDS ON YOUR PROPERTY TAX BILL AS A ” SPECIAL TAX” OR AN ASSESSMENT TAX” TO PAY FOR THE NACIMIENTO WATER PIPELINE AND SEWER TREATMENT PLANT? A PERCENTAGE OF PASO CITIZENS FINANCE LOCAL SCHOOL BONDS ON THEIR PROPERTY TAX BILL AND DON’T HAVE ANY KIDS GOING TO SCHOOL. HOW ABOUT THAT COMPARISON FOR YOU TO THINK ABOUT? WHAT SAY YOU?\nI say less CapsLock, please.\nwhatisup says:\t09/12/2010 at 11:41 pm\nI have answered your questions. I have been quite detailed in my answers and I am sorry if you can’t deal with the detail. I guess it is your inconvenient truth. You do seem to like to deflect and go around in circles. Another example, now you are ranting about the wineries using the same aquaifier as the City. Let me be clear for you, I don’t like the amount of water the wineries are using. However, the wineries are in the County, not in the City and the City can’t do anything about it. They wineries are allowed to take the water they are taking even if it drops the City’s water levels in their wells. You need to complain to Sacramento. It sounds like you just don’t want to pay anything for the infrastructure because you really just don’t want it built.\nSeveral of your observations of my opinions are bizarre considering I have stated several times I believe the Courts need to decide if Paso Robles has, or has not followed the rules as to funding the infrastucture. Obviously, as I have stated before, if the City loses the lawsuit the infrastructure will have to be paid out of the City’s General Fund until a new method of payment is voted on by the Citizens of Paso Robles. Pretty clear.\nYour idea of charging based on a special assesment rather than the amount of water a property uses means that people who use little water, but live on a more expensive property will pay more than their share, based on their water usage. In addition, how do you deal with a rental unit where the renter is supposed to pay the water bill? Your idea is inherantly unfair, but my guess is it will favor you, so you don’t care if it is unfair and other people would pay part of your share. You also have decided that since I have alternative ideas to yours I must work for, or have business with the City of Paso Robles, another attempt to deflect from the issue. However, once again, I have never worked for the City or have ever done business with the City and don’t expect to ever do business with the City. I do own property in the City which is why I pay attention. Finally, it turns out there needs to be a fix to the housing element laws, the existance of which you are questioning. As I understand it the fix to the housing elemnt laws is because of some lawsuit. This should give you all the information you need to educate yourself on the California Housing Element laws that every city and county in California has to follow:\nBILL ANALYSIS ————————————————————\n|SENATE RULES COMMITTEE | AB 602|\n|Office of Senate Floor Analyses | |\n|1020 N Street, Suite 524 | |\n|(916) 651-1520 Fax: (916) | |\n|327-4478 | |\n———————————————————— THIRD READING\nBill No: AB 602\nAuthor: Feuer (D), et al\nAmended: 8/20/10 in Senate\nSENATE TRANSPORTATION & HOUSING COMM : 6-3, 6/29/10\nAYES: Lowenthal, DeSaulnier, Kehoe, Pavley, Simitian, Wolk\nNOES: Huff, Ashburn, Harman\nASSEMBLY FLOOR : Not relevant\nSUBJECT : Statute of limitations on housing element\nSOURCE : California Rural Legal Assistance Foundation\nHousing California DIGEST : This bill states the intent of the Legislature\nin enacting this bill to modify the courts opinion in Urban\nHabitat Program v. City of Pleasanton (2008) 164\nCal.App.4th 1561, with respect to the interpretation of\nSection 65009 of the Government Code, and revises and\nclarifies statute of limitations and remedies for specified\nhousing related challenges.\nSenate Floor Amendments of 8/20/10 revise the statute of\nlimitations and remedies for specified housing-related\nANALYSIS : The Planning and Zoning Law requires cities\nand counties to prepare and adopt a general plan, including\na housing element, to guide the future growth of a\ncommunity. Following a staggered statutory schedule,\ncities and counties located within the territory of a\nmetropolitan planning organization (MPO) must revise their\nhousing elements every eight years, and cities and counties\nin rural non-MPO regions must revise their housing elements\nevery five years. These five- and eight-year periods are\nknown as the housing element planning period.\nBefore each revision, each community is assigned its fair\nshare of housing for each income category through the\nregional housing needs assessment (RHNA) process. A\nhousing element must identify and analyze existing and\nprojected housing needs, identify adequate sites with\nappropriate zoning to meet its share of the RHNA, and\nensure that regulatory systems provide opportunities for,\nand do not unduly constrain, housing development. The\nreviews both draft and adopted housing elements to\ndetermine whether or not they are in substantial compliance\nwith the law. The Planning and Zoning Law and the Subdivision Map Act\nalso includes a number of sections governing zoning and\nentitlements specifically related to housing, including:\n? The Housing Accountability Act, which requires a city or\ncounty to make one or more specified findings in order to\ndisapprove a particular housing development.\n? A provision requiring cities and counties, when adopting\nan ordinance which limits the number of housing units\nwhich may be constructed on an annual basis, to make\nfindings as to the public health, safety, and welfare\nbenefits that justify reducing the housing opportunities\nof the region. ? Density bonus law, which requires cities and counties to\ngrant a developer a density bonus, incentives, and\nconcessions when the developer proposes to include\nspecified percentages of affordable housing within a\ndevelopment. ? The Least Cost Zoning Law, which requires cities and AB 602\ncounties to designate and zone sufficient vacant land for\nresidential use with appropriate standards to meet\nhousing needs for all income categories and to contribute\nto producing housing at the lowest possible cost.\n? A requirement that, when determining whether to approve a\ntentative subdivision map, a city or county shall apply\nonly those ordinances, policies, and standards in effect\nas of the date the developer’s application is deemed\nPrior to a recent court decision, it was understood that\ncurrent law allowed a party to challenge the adequacy of a\ncity’s or county’s housing element at any time during a\nplanning period, provided that the challenger brought the\naction “in support of or to encourage or facilitate the\ndevelopment of housing that would increase the community’s\nsupply of [affordable] housing.” The challenging party was\nrequired first to serve the city or county with a notice\nidentifying the deficiencies in the housing element. After\n60 days or the date on which the city or county took final\naction in response to the notice, whichever occurred first,\nthe challenging party had one year to file the action in\ncourt. This process and statute of limitations also\napplied to actions brought pursuant to the housing-related\nstatutes listed above. In 2006 Urban Habitat Program brought suit to challenge the\nCity of Pleasanton’s housing policies, including the city’s\nannual cap on housing permits and the city’s cap on the\naggregate number of permissible housing units, both of\nwhich Urban Habitat claimed were insufficient to allow the\ncity to meet its RHNA obligation. In 2008, the First\nDistrict California Court of Appeals issued an unpublished\ndecision in the case of Urban Habitat Program v. City of\nPleasanton allowing the case to proceed with respect to\nsome causes of action, but ruling that the challenge to the\nhousing element itself was time-barred. The court stated:\nAlthough the statute does not specify the time within\nwhich [a deficiency] notice must be given, it is our\nconclusion that the statute must be interpreted as\ncontaining a time limit within which this requirement\nmust be met? In sum, a party bringing a challenge AB 602\ngoverned by section 65009, subdivision (d), has 90\ndays from the date a legislative action is taken or\napproval is given to notify the local land use\nauthority of any claimed deficiencies in such an\naction or approval. Its claim then accrues 60 days\nafter it gives this notice.\nIn other words, instead of being able to initiate a\nchallenge to a deficient housing element at any time during\nthe planning period, housing advocates and other interested\nparties may now only initiate such a challenge by\nsubmitting a deficiency notice within 90 days of the\nhousing element’s adoption.\n1.Removes from the current list of city or county actions\nwhich may be challenged pursuant to Government Code 65009\nnotice and accrual provisions those actions related to\nthe Housing Accountability Act, the Subdivision Map Act,\nand the application of a Density Bonus ordinance to a\nparticular project, all of which are project-specific\nactions. The bill maintains the ability to use these\nnotice and accrual provisions to challenge the adequacy\nof a city’s or county’s density bonus ordinance\n2.Extends lengthening the time in which a deficiency notice\nmay be served to cover all remaining city or county\nactions described in this section of law, as opposed to\njust housing element challenges. In other words, the\namendments apply the longer timeframe to serve the\ndeficiency notice to actions relating to the Least Cost\nZoning Law, annual limits on housing permits, and the\nadequacy of a density bonus ordinance, in addition to\nhousing element law. 3.Provides that an entity challenging such an action in\nsupport of affordable housing may serve the deficiency\nnotice up to five years after the city’s or county’s\naction. After 60 days or the date on which the city or\ncounty takes final action in response to the notice,\nwhichever occurs first, the challenging party has one\nyear to file an action in court, except that the lawsuit AB 602\nmay not be filed more than five years after the city’s or\ncounty’s action. In other words, the entity must file\nthe lawsuit within one year of the expiration of the\ndeficiency notice or within five years of the city’s or\ncounty’s action, whichever occurs first.\n4.Provides that a housing element from a prior planning\nperiod may not be challenged if the city or county has\nadopted a revised housing element for the new planning\nGovernment Code 65755 . Current law requires a court, if it\nfinds any portion of a general plan, including a housing\nelement, out of compliance with the law, to include within\nits order or judgment one or more of the following remedies\nfor any or all types of developments or any or all\ngeographic segments of the city or county until the city or\ncounty has complied with the law:\n? Suspend the authority of the city or county to\nissue building permits.\ngrant zoning changes and/or variances.\ngrant subdivision map approvals.\n? Mandate the approval of building permits for\nresidential housing that meet specified criteria.\n? Mandate the approval of final subdivision maps for\nhousing projects that meet specified criteria.\n? Mandate the approval of tentative subdivision maps\nfor residential housing projects that meet specified\nThis bill clarifies that in any action or proceeding\nbrought pursuant to the notice and accrual provisions of\nGovernment Code Section 65009 described above, neither the\ncourt remedies described above nor any injunction against\nthe development of a housing project shall abrogate,\nimpair, or otherwise interfere with the full exercise of\nthe rights and protections granted to an applicant for a\ntentative map or a vesting tentative map under specified\nprovisions of the Subdivision Map Act or to a developer\nunder a specified provision relating to development AB 602\nUnder current law, HCD operates a number of grant programs\nto which cities and counties may apply. In many cases, the\nlaw requires a city or county to have an HCD-approved\nhousing element in order to be eligible for funding. This bill provides that if a third-party challenges the\nadequacy of a housing element in court and the court finds\nthat the housing element substantially complies with all of\nthe requirements of housing element law, the element shall\nbe deemed to be in compliance for purposes of state housing\nThe statutory language interpreted by the court and at\nissue in this bill was added to statute by AB 998 (Waters),\nChapter 1138, Statutes of 1983, a bill sponsored by the\nLeague of California Cities and the California Building\nIndustry Association. AB 998 created a short statute of\nlimitations period for land use decisions generally but\nprovided a specific exception to protect the ability to\nchallenge deficient housing elements. The Senate Housing\nand Land Use Committee and the Senate Third Reading\nanalysis of the bill stated that the bill:\nSpecifies that for challenges in support of low- and\nmoderate-income housing requirements, the petitioner\nshall notice local government 60 days prior to filing\naction. The [one-year] statute of limitations then\nbegins on the first day the legislative body fails to\nIn the intervening 25 years prior to the Urban Habitat\nruling, housing advocates filed and successfully settled at\nleast ten cases in which the 60-day deficiency notice was\nsent more than 90 days after adoption of the city’s or\ncounty’s housing element. In none of these cases was the\ntimeliness on the advocates’ suit contested. Likewise, six\nbills amended other portions of this statute during those\nintervening years, and there was never any controversy\nsurrounding the lack of a deadline for housing advocates to\nserve a deficiency notice nor any attempt to change the AB 602\nstatute in this regard. Current level of housing element compliance . According to\nHCD’s website as of June 7, 2010, only 46 percent of cities\nand counties have adopted an HCD-approved housing element\nfor the current planning period that began in 2005 for the\nSan Diego region, 2008 for the Southern California, Fresno,\nKern, and Sacramento regions, and the summer of 2009 for\nthe remaining areas of the state. Unlocking the private market . The purpose of housing\nelement law is to create opportunities for the private\nhousing market to function. Builders cannot build without\naccess to appropriately zoned land, and current land use\nplans in many cities and counties in California fail to\nprovide sufficient opportunities to accommodate projected\npopulation growth. The San Diego Association of\nGovernments’ Regional Comprehensive Plan describes this\ntypical California paradox in the following way:\nUnder current plans and policies, more than 90 percent\nof [the San Diego region’s] remaining vacant land\ndesignated for housing is planned for densities of\nless than one home per acre, and most is in the rural\nback country areas dependent upon scarce groundwater\nsupplies. And of the remaining vacant land planned for\nhousing in the 18 incorporated cities, only about\nseven percent is planned for multifamily housing. When\ntaken together, the current land use plans of the 19\nlocal jurisdictions do not accommodate the amount of\ngrowth anticipated in our region. SANDAG’s population\nforecast, which reflects the current adopted local\nland use plans in the region, projects that while\npopulation will increase by 37 percent by 2030,\nhousing will grow by just 30 percent. The forecast\nshows that if local plans are not changed, demand for\nhousing will continue to outpace the supply, just as\nHousing element law addresses this problem directly by\nrequiring cities and counties to zone land at appropriate\ndensities to accommodate the projected housing needs of all\nincome groups and to remove constraints that prevent such\nsites from being developed at the allowed densities. AB 602\nCities and counties, however, are not required to build\nhousing because that is the role of private developers.\nThe law holds cities and counties accountable only for that\nwhich they control: zoning and land use entitlements.\nWithout the ability to enforce housing element law, the\nmarket’s ability to meet housing demand may well remain\nlocked up.\nFISCAL EFFECT : Appropriation: No Fiscal Com.: No\nSUPPORT : (Verified 8/23/10)\nCalifornia Rural Legal Assistance Foundation (co-source)\nHousing California (co-source)\nAdvocates for Affordable Homes in Fremont\nCalifornia Coalition for Rural Housing\nCommunity Housing Improvement Program\nCommunity Housing Works\nEden Housing\nFair Housing of Marin\nGrassroots Leadership Network of Marin\nKennedy Commission\nPublic Advocates, Inc\nSan Diego Housing Federation\nSelf-Help Enterprises\nSierra Club of California\nAmerican Planning Association, California Chapter\nJA:nl 8/23/10 Senate Floor Analyses SUPPORT/OPPOSITION: SEE ABOVE\npasoobserver says:\t09/11/2010 at 11:17 pm\nTo whatisup — Thank you for your response to my comments. However, you failed to answer some of my questions that I mentioned to you. It’s almost like dealing with some City officials. They just let the public vent at their bimonthly council meetings. In my opinion, it’s difficult to deal with narcissism and arrogance. Over the years, there has been some very good input to our elected officials on how to proceed on the Nacimiento water pipeline,but it fell on deaf ears. You wanted me to answer some of your questions,but you did not answer some of my questions. Again, are you willing to subsidize new development?,Yes?or No?, are you willing to pay for a commodity that you are not receiving? Yes?or No? and another question for you. Are you willing to pay over 300% on your water bills within the five (5) year plan that the City has proposed? Also, the water rates will be subject to later increases too. By the way, I do concur with the city’s plan of “you pay for the amount of water units you use”. (748 gal=one unit). However, the higher water rates are not good for our senior citizens on fixed incomes and other struggling families in our community. My first suggestion years ago was desalination. The response was it was too expensive. Of course, now it is more expensive. I would suggest that our elected officials recall the existing bonds (The bonds can be recalled early). The City council can explain to the citizens in detail with financing of new bonds at a lower interest rate as of now for the sewer plant and Nacimiento water pipeline and present their new proposal in compliance with Proposition 218. Let the citizens of Paso VOTE on the financing bonds for their approval. Most of the citizens,that I had spoken to were not happy with the way our City Council handled the Nacimiento water pipeline project. The citizens of Paso didn’t give our City Council a “BLANK CHECK” for $176 million to spend without voter approval. I would suggest that it be a “special tax” or “an assessment” be levied on our property taxes. A percentage of those bonds can be deducted on Federal Income taxes. As it is now, a” fee” on a capital funding project is not deductible. Of course, there are homeowners would not go for this suggestion due to our poor economy. My analogy mentioned above would be, you would get something back on a “special tax” or an “assessment” verses nothing on a “fee”. What say you?\nwhatisup says:\t09/12/2010 at 9:02 am\nUnfortunately the law says we have to subsidize new development in California. I don’t like it, but it is the law. I know paying using the property taxes was bandied about. The argument against it was it would mean some would be paying for water they aren’t using and others could be big water users, but pay a small special assessment on their property taxes. I think the decision that was made to base it on usage was out of fairness. It seems to me if people are using water and not paying their share of the costs it is not fair. The Senior issue is very difficult. If someone is retired for twenty years is it realistic to think prices don’t go up during the 20 years of retirement. Think what prices were in 1990 compared to today. Should Seniors never have to pay for capital improvements? Paso Robles also had very low water rates. Rates that are no longer possible given the circumstances. Desalination will happen eventually. California is out of water. If you want to pay $1,000,000 a gallon there is no more allotable water of any consequence in California. The expense will be tremendous — still have to build a desalination plant, still have to build a pipeline. I don’t know if the plant has to be built along the ocean or if the salt water could be piped over to Paso Robles. If it has to be built along the ocean, Paso Robles doesn’t own land on the ocean and, in any case, the environmentalists will keep it in courts for years as they have done so for other proposed desalination plants in Southern California. Eventually necessity will force desalination past the environmentalists, but not yet.\npasojim says:\t09/13/2010 at 7:46 am\nWhatisup – On one of your previous post you made the comment you haven’t heard any of the legal suggestions for the water issue, But you obviously have. That is a good thing. So we can move the discussion ahead.\nOnce, again this was handled incorrectly by our city custodians from the beginning. And now here we are. The public is not supporting this very expensive, very limited benefit project. As you said, until a plan is developed that the public can support, things don’t look good.\nAll this discussion about the water issue has only reinforced my opinion the issue hasn’t been about water, only how the plan should be paid for. Or more specifically, to what extent do we allow our elected custodians and our un-elected GOD tzar decide which laws they will follow and which laws they will ignore. When the City GOD tzar tell citizens at a council meeting if we don’t agree with the City’s plan, then we should just sue him, and when the City Attorney explains to a citizen at a City Council meeting that she does have to respond to their questions because she does NOT work for them. When the project is voted down by the citizens and the council brings it right back up, it is clear that our elected representatives are not doing their job providing direction to their employees and listening to and representing the CITIZENS.\nThe subject of the original post was the need to elect different representation. I think with all the conversation made on this post, as well as the post on Cal Coast about the hiring of the new legal firm you were involved in, Supports my original opinion."
  },
  {
    "id": "b5b0eb150f44a4d7641b9adf9267ca0c2492ea46626449b3",
    "content": "For other uses, see Electricity (disambiguation).\n\"Electric\" redirects here. For other uses, see Electric (disambiguation).\nLightning is one of the most dramatic effects of electricity.\nElectricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. In early days, electricity was considered as being not related to magnetism. Later on, many experimental results and the development of Maxwell's equations indicated that both electricity and magnetism are from a single phenomenon: electromagnetism. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.\nThe presence of an electric charge, which can be either positive or negative, produces an electric field. The movement of electric charges is an electric current and produces a magnetic field.\nWhen a charge is placed in a location with a non-zero electric field, a force will act on it. The magnitude of this force is given by Coulomb's law. Thus, if that charge were to move, the electric field would be doing work on the electric charge. Thus we can speak of electric potential at a certain point in space, which is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts.\nelectronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.\nElectrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. Even then, practical applications for electricity were few, and it would not be until the late nineteenth century that electrical engineers were able to put it to industrial and residential use. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.\nLong before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the \"Thunderer of the Nile\", and described them as the \"protectors\" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. Possibly the earliest and nearest approach to the discovery of the identity of lightning, and electricity from any other source, is to be attributed to the Arabs, who before the 15th century had the Arabic word for lightning ra‘ad (رعد) applied to the electric ray.\nAncient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.\nBenjamin Franklin conducted extensive research on electricity in the 18th century, as documented by Joseph Priestley (1767) History and Present Status of Electricity, with whom Franklin carried on extended correspondence.\nElectricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete, in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word electricus (\"of amber\" or \"like amber\", from ἤλεκτρον, elektron, the Greek word for \"amber\") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words \"electric\" and \"electricity\", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.\nFurther work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges.\nIn 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his \"On Physical Lines of Force\" in 1861 and 1862.\nWhile the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life.\nIn 1887, Heinrich Hertz:843–44 discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for \"his discovery of the law of the photoelectric effect\". The photoelectric effect is also employed in photocells such as can be found in solar panels and this is frequently used to make electricity commercially.\nThe first solid-state device was the \"cat's-whisker detector\" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.\nThe solid-state device came into its own with the invention of the transistor in 1947. Common solid-state devices include transistors, microprocessor chips, and RAM. A specialized type of RAM called flash RAM is used in USB flash drives and more recently, solid-state drives to replace mechanically rotating magnetic disc hard disk drives. Solid state devices became prevalent in the 1950s and the 1960s, during the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) and the light-emitting diode (LED).\nThe presence of charge gives rise to an electrostatic force: charges exert a force on each other, an effect that was known, though not understood, in antiquity.:457 A lightweight ball suspended from a string can be charged by touching it with a glass rod that has itself been charged by rubbing with a cloth. If a similar ball is charged by the same glass rod, it is found to repel the first: the charge acts to force the two balls apart. Two balls that are charged with a rubbed amber rod also repel each other. However, if one ball is charged by the glass rod, and the other by an amber rod, the two balls are found to attract each other. These phenomena were investigated in the late eighteenth century by Charles-Augustin de Coulomb, who deduced that charge manifests itself in two opposing forms. This discovery led to the well-known axiom: like-charged objects repel and opposite-charged objects attract.\nThe force acts on the charged particles themselves, hence charge has a tendency to spread itself as evenly as possible over a conducting surface. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges and has an inverse-square relation to the distance between them.:35 The electromagnetic force is very strong, second only in strength to the strong interaction, but unlike that force it operates over all distances. In comparison with the much weaker gravitational force, the electromagnetic force pushing two electrons apart is 1042 times that of the gravitational attraction pulling them together.\nStudy has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. Electric charge gives rise to and interacts with the electromagnetic force, one of the four fundamental forces of nature. The most familiar carriers of electrical charge are the electron and proton. Experiment has shown charge to be a conserved quantity, that is, the net charge within an electrically isolated system will always remain constant regardless of any changes taking place within that system. Within the system, charge may be transferred between bodies, either by direct contact, or by passing along a conducting material, such as a wire.:2–5 The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.\nThe charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. By convention, the charge carried by electrons is deemed negative, and that by protons positive, a custom that originated with the work of Benjamin Franklin. The amount of charge is usually given the symbol Q and expressed in coulombs; each electron carries the same charge of approximately −1.6022×10−19 coulomb. The proton has a charge that is equal and opposite, and thus +1.6022×10−19 coulomb. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle.\nThe movement of electric charge is known as an electric current, the intensity of which is usually measured in amperes. Current can consist of any moving charged particles; most commonly these are electrons, but any charge in motion constitutes a current. Electric current can flow through some things, electrical conductors, but will not flow through an electrical insulator.\nBy historical convention, a positive current is defined as having the same direction of flow as any positive charge it contains, or to flow from the most positive part of a circuit to the most negative part. Current defined in this manner is called conventional current. The motion of negatively charged electrons around an electric circuit, one of the most familiar forms of current, is thus deemed positive in the opposite direction to that of the electrons. However, depending on the conditions, an electric current can consist of a flow of charged particles in either direction, or even in both directions at once. The positive-to-negative convention is widely used to simplify this situation.\nThe process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. Examples of electric currents include metallic conduction, where electrons flow through a conductor such as metal, and electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. While the particles themselves can move quite slowly, sometimes with an average drift velocity only fractions of a millimetre per second,:17 the electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires.\nCurrent causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. Their work was greatly expanded upon by Michael Faraday in 1833. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840.:23–24 One of the most important discoveries relating to current was made accidentally by Hans Christian Ørsted in 1820, when, while preparing a lecture, he witnessed the current in a wire disturbing the needle of a magnetic compass. He had discovered electromagnetism, a fundamental interaction between electricity and magnetics. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment.\nIn engineering or household applications, current is often described as being either direct current (DC) or alternating current (AC). These terms refer to how the current varies in time. Direct current, as produced by example from a battery and required by most electronic devices, is a unidirectional flow from the positive part of a circuit to the negative.:11 If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. Alternating current is any current that reverses direction repeatedly; almost always this takes the form of a sine wave.:206–07 Alternating current thus pulses back and forth within a conductor without the charge moving any net distance over time. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. Alternating current is affected by electrical properties that are not observed under steady state direct current, such as inductance and capacitance.:223–25 These properties however can become important when circuitry is subjected to transients, such as when first energised.\nThe concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the way that the gravitational field acts between two masses, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electric field can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker.\nA hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body.:88 This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects.\nThe principles of electrostatics are important when designing items of high-voltage equipment. There is a finite limit to the electric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker, perhaps 1 kV per centimetre. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises the electric field in the air to greater than it can withstand. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh.\nA pair of AA cells. The + sign indicates the polarity of the potential difference between the battery terminals.\nThe concept of electric potential is closely linked to that of the electric field. A small charge placed within an electric field experiences a force, and to have brought that charge to that point against the force requires work. The electric potential at any point is defined as the energy required to bring a unit test charge from an infinite distance slowly to that point. It is usually measured in volts, and one volt is the potential for which one joule of work must be expended to bring a charge of one coulomb from infinity.:494–98 This definition of potential, while formal, has little practical application, and a more useful concept is that of electric potential difference, and is the energy required to move a unit charge between two specified points. An electric field has the special property that it is conservative, which means that the path taken by the test charge is irrelevant: all paths between two specified points expend the same energy, and thus a unique value for potential difference may be stated.:494–98 The volt is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage.\nFor practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable.\nElectric potential is a scalar quantity, that is, it has only magnitude and not direction. It may be viewed as analogous to height: just as a released object will fall through a difference in heights caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. As relief maps show contour lines marking points of equal height, a set of lines marking points of equal potential (known as equipotentials) may be drawn around an electrostatically charged object. The equipotentials cross all lines of force at right angles. They must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface.\nØrsted's discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. Moreover, the interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The force on the compass needle did not direct it to or away from the current-carrying wire, but acted at right angles to it. Ørsted's slightly obscure words were that \"the electric conflict acts in a revolving manner.\" The force also depended on the direction of the current, for if the flow was reversed, then the force did too.\nØrsted did not fully understand his discovery, but he observed the effect was reciprocal: a current exerts a force on a magnet, and a magnetic field exerts a force on a current. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other: two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The interaction is mediated by the magnetic field each current produces and forms the basis for the international definition of the ampere.\nThis relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 1821. Faraday's homopolar motor consisted of a permanent magnet sitting in a pool of mercury. A current was allowed through a wire suspended from a pivot above the magnet and dipped into the mercury. The magnet exerted a tangential force on the wire, making it circle around the magnet for as long as the current was maintained.\nExperimentation by Faraday in 1831 revealed that a wire moving perpendicular to a magnetic field developed a potential difference between its ends. Further analysis of this process, known as electromagnetic induction, enabled him to state the principle, now known as Faraday's law of induction, that the potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Faraday's disc was inefficient and of no use as a practical generator, but it showed the possibility of generating electric power using magnetism, a possibility that would be taken up by those that followed on from his work.\nItalian physicist Alessandro Volta showing his \"battery\" to French emperor Napoleon Bonaparte in the early 19th century.\nThe ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses.\nElectrochemistry has always been an important part of electricity. From the initial invention of the Voltaic pile, electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells.\nA basic electric circuit. The voltage source V on the left drives a current I around the circuit, delivering electrical energy into the resistor R. From the resistor, the current returns to the source, completing the circuit.\nAn electric circuit is an interconnection of electric components such that electric charge is made to flow along a closed path (a circuit), usually to perform some useful task.\nElectric power is the rate at which electric energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.\nElectricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries or by other means from a wide variety of sources of energy. Electric power is generally supplied to businesses and homes by the electric power industry. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. Electric utilities measure power using electricity meters, which keep a running total of the electric energy delivered to a customer. Unlike fossil fuels, electricity is a low entropy form of energy and can be converted into motion or many other forms of energy with high efficiency.\nElectronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, optoelectronics, sensors and integrated circuits, and associated passive interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and electronics is widely used in information processing, telecommunications, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.\nToday, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.\nThus, the work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances.\nEarly 20th-century alternator made in Budapest, Hungary, in the power generating hall of a hydroelectric station (photograph by Prokudin-Gorsky, 1905–1915).\nIn the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. While this method, now known as the triboelectric effect, can lift light objects and generate sparks, it is extremely inefficient. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The voltaic pile, and its modern descendant, the electrical battery, store energy chemically and make it available on demand in the form of electrical energy. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines.\nElectrical power is usually generated by electro-mechanical generators driven by steam produced from fossil fuel combustion, or the heat released from nuclear reactions; or from other sources such as kinetic energy extracted from wind or flowing water. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on his electromagnetic principle that a conductor linking a changing magnetic field induces a potential difference across its ends. The invention in the late nineteenth century of the transformer meant that electrical power could be transmitted more efficiently at a higher voltage but lower current. Efficient electrical transmission meant in turn that electricity could be generated at centralised power stations, where it benefited from economies of scale, and then be despatched relatively long distances to where it was needed.\nSince electrical energy cannot easily be stored in quantities large enough to meet demands on a national scale, at all times exactly as much must be produced as is required. This requires electricity utilities to make careful predictions of their electrical loads, and maintain constant co-ordination with their power stations. A certain amount of generation must always be held in reserve to cushion an electrical grid against inevitable disturbances and losses.\nElectricity is a very convenient way to transfer energy, and it has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Although electrification brought with it its own dangers, replacing the naked flames of gas lighting greatly reduced fire hazards within homes and factories. Public utilities were set up in many cities targeting the burgeoning market for electrical lighting. In the late 20th century and in modern times, the trend has started to flow in the direction of deregulation in the electrical power sector.\nThe resistive Joule heating effect employed in filament light bulbs also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries, such as Denmark, have issued legislation restricting or banning the use of resistive electric heating in new buildings. Electricity is however still a highly practical energy source for heating and refrigeration, with air conditioning/heat pumps representing a growing sector for electricity demand for heating and cooling, the effects of which electricity utilities are increasingly obliged to accommodate.\nElectricity is used within telecommunications, and indeed the electrical telegraph, demonstrated commercially in 1837 by Cooke and Wheatstone, was one of its earliest applications. With the construction of first intercontinental, and then transatlantic, telegraph systems in the 1860s, electricity had enabled communications in minutes across the globe. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process.\nThe effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A stationary motor such as a winch is easily provided with a supply of power, but a motor that moves with its application, such as an electric vehicle, is obliged to either carry along a power source such as a battery, or to collect current from a sliding contact such as a pantograph. Electrically powered vehicles are used in public transportation, such as electric buses and trains, and an increasing number of battery-powered electric cars in private ownership.\nElectronic devices make use of the transistor, perhaps one of the most important inventions of the twentieth century, and a fundamental building block of all modern circuitry. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square.\nA voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the greater the current. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. Death caused by an electric shock is referred to as electrocution. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times.\nElectricity is not a human invention, and may be observed in several forms in nature, a prominent manifestation of which is lightning. Many interactions familiar at the macroscopic level, such as touch, friction or chemical bonding, are due to interactions between electric fields on the atomic scale. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, or even sugar, generate a potential difference across their faces when subjected to external pressure. This phenomenon is known as piezoelectricity, from the Greek piezein (πιέζειν), meaning to press, and was discovered in 1880 by Pierre and Jacques Curie. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place.\n§Bioelectrogenesis in microbial life is a prominent phenomenon in soils and sediment ecology resulting from anaerobic respiration. The microbial fuel cell mimics this ubiquitous natural phenomenon.\nSome organisms, such as sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception, while others, termed electrogenic, are able to generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes. All animals transmit information along their cell membranes with voltage pulses called action potentials, whose functions include communication by the nervous system between neurons and muscles. An electric shock stimulates this system, and causes muscles to contract. Action potentials are also responsible for coordinating activities in certain plants.\nIn the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. \"Revitalization\" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored Frankenstein (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films.\nAs the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light, such as the workers who \"finger death at their gloves' end as they piece and repiece the living wires\" in Rudyard Kipling's 1907 poem Sons of Martha. Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the Tom Swift books. The masters of electricity, whether fictional or real—including scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla—were popularly conceived of as having wizard-like powers.\nWith electricity ceasing to be a novelty and becoming a necessity of everyday life in the later half of the 20th century, it required particular attention by popular culture only when it stops flowing, an event that usually signals disaster. The people who keep it flowing, such as the nameless hero of Jimmy Webb’s song \"Wichita Lineman\" (1968), are still often cast as heroic, wizard-like figures.\nAmpère's circuital law, connects the direction of an electric current and its associated magnetic currents.\n^ Diogenes Laertius. R.D. Hicks (ed.). \"Lives of Eminent Philosophers, Book 1 Chapter 1 \". Perseus Digital Library. Tufts University. Retrieved 5 February 2017. Aristotle and Hippias affirm that, arguing from the magnet and from amber, he attributed a soul or life even to inanimate objects.\n^ Aristotle. Daniel C. Stevenson (ed.). \"De Animus (On the Soul) Book 1 Part 2 (B4 verso)\". The Internet Classics Archive. Translated by J.A. Smith. Retrieved 5 February 2017. Thales, too, to judge from what is recorded about him, seems to have held soul to be a motive force, since he said that the magnet has a soul in it because it moves the iron.\n^ a b c Guarnieri, M. (2014). \"Electricity in the age of Enlightenment\". IEEE Industrial Electronics Magazine. 8 (3): 60–63. doi:10.1109/MIE.2014.2335431.\n^ Srodes, James (2002), Franklin: The Essential Founding Father, Regnery Publishing, pp. 92–94, ISBN 0-89526-163-4 It is uncertain if Franklin personally carried out this experiment, but it is popularly attributed to him.\n^ a b Guarnieri, M. (2014). \"The Big Jump from the Legs of a Frog\". IEEE Industrial Electronics Magazine. 8 (4): 59–61, 69. doi:10.1109/MIE.2014.2361237.\n^ Hertz, Heinrich (1887). \"Ueber den Einfluss des ultravioletten Lichtes auf die electrische Entladung\". Annalen der Physik. 267 (8): S. 983–1000. Bibcode:1887AnP...267..983H. doi:10.1002/andp.18872670827.\n^ \"The Nobel Prize in Physics 1921\". Nobel Foundation. Retrieved 2013-03-16.\n^ John Sydney Blakemore, Solid state physics, pp. 1–3, Cambridge University Press, 1985 ISBN 0-521-31391-0.\n^ Richard C. Jaeger, Travis N. Blalock, Microelectronic circuit design, pp. 46–47, McGraw-Hill Professional, 2003 ISBN 0-07-250503-6.\n^ \"The repulsive force between two small spheres charged with the same type of electricity is inversely proportional to the square of the distance between the centres of the two spheres.\" Charles-Augustin de Coulomb, Histoire de l'Academie Royal des Sciences, Paris 1785.\n^ Sewell, Tyson (1902), The Elements of Electrical Engineering, Lockwood, p. 18 . The Q originally stood for 'quantity of electricity', the term 'electricity' now more commonly expressed as 'charge'.\n^ a b Berkson, William (1974), Fields of Force: The Development of a World View from Faraday to Einstein, Routledge, p. 370, ISBN 0-7100-7626-6 Accounts differ as to whether this was before, during, or after a lecture.\n^ \"Lab Note #105 EMI Reduction – Unsuppressed vs. Suppressed\". Arc Suppression Technologies. April 2011. Retrieved March 7, 2012.\n^ Almost all electric fields vary in space. An exception is the electric field surrounding a planar conductor of infinite extent, the field of which is uniform.\n^ Paul J. Nahin (9 October 2002). Oliver Heaviside: The Life, Work, and Times of an Electrical Genius of the Victorian Age. JHU Press. ISBN 978-0-8018-6909-9.\n^ \"The Bumpy Road to Energy Deregulation\". EnPowered. 2016-03-28.\n^ a b c d e f g h Van Riper, op.cit., p. 71.\nLook up electricity in Wiktionary, the free dictionary.\nBasic Concepts of Electricity chapter from Lessons In Electric Circuits Vol 1 DC book and series."
  },
  {
    "id": "0ecdb8439f1360140995c6f5f6cc99c38cebb9216d1395e4",
    "content": "Weep Not, Child is a 1964 novel by Kenyan author Ngũgĩ wa Thiong'o. It was his first novel, published in 1964 under the name James Ngugi. It was among the African Writers Series. It was the first English language|English novel to be published by an East African. Thiong'o's works deal with the relationship between Africans and white settlers in colonial Kenya, and are heavily critical of colonial rule. Specifically, Weep Not, Child deals with the Mau Mau Uprising, and \"the bewildering dispossession of an entire people from their ancestral land.\" Ngũgĩ wrote the novel while he was a student at Makerere University.\n\nThe book is divided into two parts and eighteen chapters. Part one deals mostly with the education of Njoroge, while part two deals with the rising Mau Mau movement.\n\nPlot summary\n\nNjoroge, a little boy, is urged to attend school by his mother. He is the first one of his family able to go to school. His family lives on the land of Jacobo, an African made rich by his dealings with white settlers, namely Mr. Howlands, the most powerful land owner in the area. Njoroge's brother Kamau works as an apprentice to a carpenter, while Boro, the eldest living son, is troubled by his experiences while in forced service during World War II, including witnessing the death of his elder brother. Ngotho, Njoroge's father and a respected man in the surrounding area, tends Mr. Howlands' crops, but is motivated by his passion to preserve his ancestral land, rather than for any compensation or loyalty.\n\nOne day, black workers call for a strike to obtain higher wages. Ngotho is ambivalent about participating in the strike because he fears he will lose his job. However, he decides to go to the gathering, even though his two wives do not agree. At the demonstration, there are calls for higher wages. Suddenly, the white police inspector brings Jacobo to the gathering to pacify the native people. Jacobo tries to put an end to the strike. Ngotho attacks Jacobo, and the result is a riot where two people are killed. Jacobo survives and swears revenge. Ngotho loses his job and Njoroge’s family is forced to move. Njoroge’s brothers fund his education and seem to lose respect for their father.\n\nMwihaki, Jacobo's daughter and Njoroge's best friend, enters a girls' only boarding school, leaving Njoroge relatively alone. He reflects upon her leaving, and realizes that he was embarrassed by his father's actions towards Jacobo. For this reason, Njoroge is not upset by her exit and their separation. Njoroge switches to another school.\n\nFor a time, everyone's attention is focused on the upcoming trial of Jomo Kenyatta – a revered leader of the movement. Many blacks think that he is going to bring forth Kenya’s independence. But Jomo loses the trial and is imprisoned. This results in further protests and greater suppression of the black population.\n\nJacobo and a white landowner, Mr. Howlands, fight against the rising activities of the Mau Mau, an organization striving for Kenyan economic, political, and cultural independence. Jacobo accuses Ngotho of being the leader of the Mau Mau and tries to imprison the whole family. Meanwhile, the situation in the country is deteriorating. Six black men are taken out of their houses and executed in the woods.\n\nOne day Njoroge meets Mwihaki again, who has returned from boarding school. Although Njoroge had planned to avoid her due to the conflict between their fathers, their friendship is unaffected. Njoroge passes an important exam that allows him to advance to High School. His village is proud of him, and collects money to pay Njoroge's High School tuition.\n\nSeveral months later, Jacobo is murdered in his office by a member of the Mau Mau. Mr. Howlands has Njoroge removed from school for questioning. Both father and son are brutally beaten before release and Ngotho is left barely alive. Although there doesn't seem to be a connection between Njoroge's family and the murder, it is eventually revealed that Njoroge's brothers are behind the assassination, and that Boro is the real leader of the Mau Mau. Ngotho soon dies from his injuries and Njoroge finds out that his father was protecting his brothers. Kamau has been imprisoned for life. Only Njoroge and his two mothers remain free, and Njoroge is left as the sole provider of his two mothers. Njoroge fears that he cannot make ends meet; he gives up hope of continuing in school and loses faith in God.\n\nNjoroge asks Mwihaki's for support, but she is angry because of her father’s death. When he finally pledges his love to her, she refuses to leave with him, realizing her obligation to Kenya and her mother. Njoroge decides to leave town and makes an attempt at suicide; however, he fails when his mothers find him before he is able to hang himself. The novel closes with Njoroge feeling hopeless, and ashamed of cowardice.\n\nCharacters in Weep Not, Child\n Njoroge: the main character of the book whose main goal throughout the book is to become as educated as possible.\n Ngotho: Njoroge's father. He works for Mr.Howlands and is respected by him until he attacks Jacobo at a workers strike. He is fired and the family is forced to move to another section of the country. Over the course of the book his position as the central power of the family weakened, to the point where his self-realization that he has spent his whole life waiting for the prophecy (that proclaims the blacks will be returned their land) to come true rather than fighting for Kenyan independence, leads to his depression.\n Nyokabi and Njeri: the two wives of Ngotho. Njeri is Ngotho's first wife, and mother of Boro, Kamau, and Kori. Nyokabi is his second wife, and the mother of Njoroge and Mwangi.\n Njoroge has four brothers: Boro, Kamau, Kori and Mwangi (who is Njoroge's only full brother, who died in World War II).\n Boro: Son of Njeri who fights for the Allies in World War II. Upon returning his anger against the colonial government is compounded by their confiscation of the his land. Boro's anger and position as eldest son leads him to question and ridicule Ngotho, which eventually defeats their father's will (upon realizing his life was wasted waiting and not acting). It is eventually revealed that Boro is the leader of the Mau Mau (earlier alluded to as \"entering politics\") and murders Mr.Howlands. He is caught by police immediately after and is scheduled to be executed by the book's end. It is highly likely that it is also Boro who kills Jacobo.\n Mwihaki: Njoroge's best friend (and later develops into his love interest). Daughter of Jacobo. When it is revealed that his family killed Jacobo (most likely Boro), Mwihaki distances herself from Njoroge, asking for time to mourn her father and care for her mother.\n Jacobo: Mwihaki's father and an important landowner. Chief of the village.\n Mr. Howlands: A white settler who emigrated to colonial Kenya and now owns a farm made up of land that originally belonged to Ngotho's ancestors. Has three children: Peter who died in World War II before the book's beginning, a daughter who becomes a missionary, and Stephen who met Njoroge while the two were in high school.\n\nThemes and motifs\nWeep Not, Child integrates Gikuyu mythology and the ideology of nationalism that serves as catalyst for much of the novel's action. The novel explores the negative aspects of colonial rule over Kenya. Njoroge's aspiration to attend university is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt.\n\nThe novel also ponders the role of saviours and salvation. The author notes in his The River Between: \"Salvation shall come from the hills. From the blood that flows in me, I say from the same tree, a son shall rise. And his duty shall be to lead and save the people.\" Jomo Kenyatta, the first prime minister of Kenya, is immortalised in Weep Not, Child. The author says, \"Jomo had been his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel.\" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government.\n\nSee also\n\nThings Fall Apart\nDeath and the King's Horseman\n\nReferences\n\nExternal links\nOfficial homepage of Ngũgĩ wa Thiong'o\nBBC profile of Ngũgĩ wa Thiong'o\nWeep Not, Child at Google Books\n\nBritish Empire in fiction\nNovels set in colonial Africa\nHistorical novels\nKenyan English-language novels\nNovels by Ngũgĩ wa Thiong'o\nNovels set in Kenya\n1964 novels\nHeinemann (publisher) books\nPostcolonial novels\nAfrican Writers Series\n1964 debut novels"
  },
  {
    "id": "235a5c99cd7fae9e2b410ad99c1b1fafea43799d3f1138a8",
    "content": "KSTP (1500 AM; SKOR North) is a commercial AM radio station licensed to Saint Paul, Minnesota. It is the flagship AM radio station of Hubbard Broadcasting, which also owns several other television and radio stations across the United States.  KSTP has a sports radio format and is the ESPN Radio Network affiliate for Minneapolis-St. Paul.  The radio studios are  on University Avenue in Minneapolis, shared with sister stations KSTP-FM, KSTP-TV, KTMY, and KSTC-TV. On weekdays, KSTP airs local sports shows from 9 a.m. to 9 p.m. and carries ESPN programming weekday mornings, late nights and weekends. Some KSTP shows are simulcast on other sports radio stations in the region.\n\nKSTP runs the maximum power for AM stations, 50,000 watts.  It shares clear-channel, Class A status on 1500 AM with WFED in Washington, D.C.  KSTP broadcasts a directional signal at night, using a three-tower array, with its transmitter on U.S. Route 61 at Beam Avenue in Maplewood.  Programming is also heard on 250 watt FM translator K235BP at 94.9 MHz in Bemidji.\n\nHistory\n\nWAMD and KFOY\nKSTP's start in 1928 was the product of a merger between two pioneering Twin Cities stations: WAMD (\"Where All Minneapolis Dances\") in Minneapolis, first licensed on February 16, 1925 to Stanley E. Hubbard, and KFOY in St. Paul, first licensed on March 12, 1924 to the Beacon Radio Service in St. Paul.\n\nFollowing a few test transmissions, WAMD made its formal debut broadcast on February 22, 1925. (In later interviews Stanley Hubbard traced WAMD's start to April 1924.) It was located at the Marigold Dance Garden, and featured nightly \"Midnight Frolics\" broadcasts by the ballroom's orchestra. It is claimed that WAMD was the first radio station to be completely supported by running paid advertisements. Effective June 15, 1927, WAMD was assigned to 1330 kHz.\n\nOn November 11, 1927 WAMD's transmitter site at Oxboro Heath on Lyndale Avenue South burned down, two weeks after the station had been sold to the National Battery Company. An initial arrangement was made to carry WAMD's programs over WRHM (now WWTC), transmitting on WAMD's 1330 kHz frequency. Beginning on November 24, 1927 the WAMD broadcasts, still on 1330 kHz, were shifted to KFOY's facility in St. Paul. (At this time KFOY was assigned to 1050 kHz). The next day it was announced that National Battery had purchased KFOY, and as of December 1, 1927 both KFOY and WAMD were reassigned to 1350 kHz. WAMD continued making regular broadcasts until the end of March 1928, while KFOY, although it continued to be licensed for a few more months on a time-sharing basis with WAMD, ceased operations at this point.\n\nNational Battery Company\nIn mid-December 1927, the National Battery Company announced it had received permission from the Federal Radio Commission (FRC) to build a new station, with the call letters KSTP, operating from a transmitter site to be constructed three miles south of Wescott. The next month it was reported that the new station, still under construction, had been assigned to 1360 kHz. KSTP made its debut broadcast on March 29, 1928. Although technically it was a separate station from WAMD and KFOY, both of which were formally deleted on April 30, 1928, overall KSTP was treated as the direct successor to a consolidated WAMD and KFOY.\n\nHubbard became the merged station's general manager, acquiring controlling interest in 1941. A month after the merger, KSTP became an affiliate for the NBC Red Network.  It remained with NBC for 46 years. On November 11, 1928, under the provisions of the FRC's General Order 40, KSTP was assigned to a \"high-powered regional\" frequency of 1460 kHz. The only other station assigned to this frequency was WTFF in Mount Vernon Hills, Virginia (later WJSV, now WFED, Washington, D.C.). On February 7, 1933, the FRC authorized KSTP to increase its daytime power to 25 KW. In 1938 and 1939 KSTP also operated a high-fidelity AM \"experimental audio broadcasting station\" Apex station, W9XUP, originally on 25,950 kHz and later on 26,150 kHz. In 1941, as part of the implementation of the North American Regional Broadcasting Agreement, KSTP was assigned to its current \"clear channel\" frequency of 1500 kHz, with the provision that it and WJSV, as \"Class I-B\" stations, had to maintain directional antennas at night in order to mutually protect each other from interference. An FM station, KSTP-FM, was founded in 1946 but shut down in 1952.\n\nHubbard reportedly acquired an RCA TV camera in 1939, and started experimenting with television broadcasts.  But World War II put a hold on the development of television.  In 1948, with the war over, KSTP-TV became the first television station in Minnesota.  With KSTP 1500 already associated with NBC Radio, KSTP-TV became an NBC Television Network affiliate.  From 1946 to 1952, KSTP also had an FM counterpart.  KSTP-FM 102.1 was only on the air four years.  There were few radios equipped to receive FM signals in that era, and management decided to discontinue FM broadcasts.\n\nMOR and Top 40\nAs network programming moved from radio to television, KSTP programmed a full service Middle of the Road (MOR) radio format, in the shadow of its chief competitor, CBS Radio affiliate 830 WCCO. In 1965, a new FM station, reviving the KSTP-FM call sign, was put on the air, largely simulcasting the AM station.  But by the late 1960s, KSTP-FM began a separate format of beautiful music.  KSTP was the radio home of the Minnesota Vikings football team from 1970 to 1975. \n\nIn 1973, KSTP broke away from its longtime adult MOR sound and became one of four area stations at the time to program a Top 40 format. \"15 KSTP, The Music Station\" competed with Top 40 AM rivals WDGY, KDWB and later, WYOO. The competition would eventually shake itself out, with outrageous rocker WYOO dropping out after being sold in 1976, and then the staid WDGY switching to country music the following year.  As for uptempo hits station 15 KSTP, it went from a tight Top 40 format to leaning adult rock in 1978, to leaning adult contemporary in 1979, to evolving into adult contemporary/talk by 1980.  In 1982, it officially shifted to talk.  Most Top 40 rock music, by this time, had moved to the FM band.\n\nPast Personalities\n\nNotable hosts who have been on KSTP include John Hines, Jesse Ventura, Larry Carolla, Tom Barnard, Big Al Davis, Don Vogel, John MacDougall, Griff, Mike Edwards, Geoff Charles, Joe Soucheray, James Lileks, Leigh Kamman, Barbara Carlson, Peter Thiele, Tom Mischke, Jason Lewis, Chuck Knapp, Machine Gun Kelly, Charle Bush, Mark O'Connell and Paul Brand. These broadcasters were supported by producers such as Bruce Huff, Rob Pendleton, Alison Brown, Jean Bjorgen, David Elvin (who Vogel dubbed the \"Steven Spielberg of Talk Radio\"), Mitch Berg and others.\n\nThe station has, for the most part, emphasized local hosts over the years.  But in 1988, KSTP was one of Rush Limbaugh's first affiliates when his conservative talk show was rolled out for national syndication.  (Clear Channel-owned KTLK-FM took over rights to Limbaugh's show in January 2006). Other syndicated hosts previously heard on KSTP include Sean Hannity, Bruce Williams, Larry King, and Owen Spann.\n\nSports Radio\nKSTP switched to Sports Radio on February 15, 2010.  As the station had to wait for ESPN's contract with rival KFAN and its sister station KFXN to expire, it did not become an ESPN Radio affiliate until April 12, the same day that the Minnesota Twins were scheduled to play the first game in their new ball park, Target Field, against the Boston Red Sox.  As a result Coast to Coast AM and Live on Sunday Night, it's Bill Cunningham were retained during this period.  One ESPN Radio network program, The Herd with Colin Cowherd, was picked up by KSTP immediately following the format change.\n\nIn 2018, the station was approved for an FM translator on 94.1 FM, broadcasting from a transmitter atop the IDS Center in downtown Minneapolis. The two-watt signal threw most of its power to the west, preventing interference to low powered FM stations on the same channel including WFNU-LP in St. Paul. With only two watts of power, however, the signal was limited to the immediate downtown area surrounding the IDS Center.  It later acquired a 250 watt translator, K235BP at 94.9 MHz.  The original translator was discontinued.\n\nOn January 15, 2019, KSTP rebranded as \"SKOR North\" (a reference to the Vikings team song/chant, \"Skol, Vikings\"), with local programming between 12 noon and 7 pm. About a year later, in May of 2020, KSTP suspended most of its local programming and laid off nearly all of its local staff. Station management cited the economic toll of the coronavirus for the changes. Sports broadcasting continues, primarily composed of ESPN radio network broadcasts.\n\nSports Teams\n\nKSTP-AM served as the radio flagship for the Minnesota Vikings football team from 1970 to 1975.\n\nOn August 1, 2006, the station announced that it would be the new flagship station for the Minnesota Twins baseball team, effective with the start of the 2007 season. The Twins had been on rival WCCO since arriving in Minnesota in 1961.  KSTP served as the flagship for the Twins until the end of the 2012 season, when games moved to 96.3 KTWN-FM (now KMWA).  The Twins have since returned to WCCO 830.\n\nThe switch to a fairly weak FM station caused dissent among some listeners, particularly in communities that had trouble picking up KSTP 1500.  Although KSTP is the state's second most powerful AM station, it must operate directionally at night, delivering a reduced signal to parts of the market. WCCO, by comparison, offers a signal with a wider coverage area during the day than KSTP does, with WCCO's non-directional 50,000 watt signal. In response, the Twins have expanded the number of affiliates.\n\nOn March 9, 2011, KSTP announced it would be the new flagship for the University of Minnesota Golden Gophers men's and women's basketball and men's ice hockey, ending a 68-year run on WCCO. The rights have since moved to KFXN-FM, which already aired Gopher football.\n\nOn March 2, 2017, KSTP announced it would be the first radio broadcaster for Minnesota United FC.  The move brings live soccer action to 1500 AM.\n\nPrevious logos\n\nReferences\n\nExternal links\nKSTP website\n\nFCC History Cards for KSTP (covering 1928-1980)\nRadiotapes.com Historic Minneapolis/St. Paul airchecks dating back to 1924 including KSTP and other Twin Cities radio stations.\nRick Burnett's TwinCitiesRadioAirchecks.com has additional airchecks of KSTP and other Twin Cities radio stations from the '60s and '70s, including Chuck Knapp's 2nd show on KSTP.\n\nHubbard Broadcasting\nESPN Radio stations\nPeabody Award winners\nRadio stations in Minneapolis–Saint Paul\nRadio stations established in 1925\n1925 establishments in Minnesota\nMinnesota Kicks\nSports radio stations in the United States\nClear-channel radio stations"
  },
  {
    "id": "a013f691a7063527fa4e7a4b357081c76656af3cd402a27a",
    "content": "Brooksley Elizabeth Born (born August 27, 1940) is an American attorney and former public official who, from August 26, 1996, to June 1, 1999, was chair of the Commodity Futures Trading Commission (CFTC), the federal agency which oversees the U.S. futures and commodity options markets. During her tenure on the CFTC, Born lobbied Congress and the President to give the CFTC oversight of off-exchange markets for derivatives, in addition to its role with respect to exchange-traded derivatives, but her warnings were ignored or dismissed, and her calls for reform resisted by other regulators.<ref name=\"nytimes\">Goodman, Peter S. The Reckoning - Taking Hard New Look at a Greenspan Legacy, The New York Times, October 9, 2008.</ref> Born resigned as chairperson on June 1, 1999, shortly after Congress passed legislation prohibiting her agency from regulating derivatives.\n\nIn 2009, Born received the John F. Kennedy Profiles in Courage Award, along with Sheila Bair of the Federal Deposit Insurance Corporation, in recognition of the \"political courage she demonstrated in sounding early warnings about conditions that contributed\" to the 2007-08 financial crisis.\n\nEarly life and education\nBorn graduated from Abraham Lincoln High School (San Francisco, California) at the age of 16. She then attended Stanford University, where she majored in English and was graduated with the class of 1961.  She initially wanted to become a doctor, but a guidance counsellor at Stanford advised her against medicine, so she majored in English literature instead.\n\nShe then attended Stanford Law School, one of only seven women in her class.  She was the first female student ever to be named president of the Stanford Law Review. She received the \"Outstanding Senior\" award and graduated as valedictorian of the class of 1964.\n\nLegal career\nImmediately after law school Born was selected as a law clerk to judge Henry Edgerton of the U.S. Court of Appeals for the District of Columbia Circuit. It was during this time that she met her first husband, Jacob C. Landau, who was a journalist covering the Federal courts at the time. Following her clerkship, she became an associate at the Washington, D.C.-based international law firm of Arnold & Porter. Born was attracted to Arnold & Porter because it was one of the few major law firms to have a woman partner at that time, Carolyn Agger, who was the head of the tax practice. Born took a two-year leave of absence from Arnold & Porter to accompany her first husband to Boston, where he had received a fellowship. During that time she worked as a research assistant to law professor Alan Dershowitz.\n\nBorn's early career at Arnold & Porter focused on international trade law, in which she represented a number of Swiss industries and the government of Switzerland. She developed a practice representing clients in numerous complex litigation and arbitration cases involving financial market transactions. Among her high-profile cases was the matter of the Hunt Brothers attempt to corner the market in silver in the 1970s. She made partner at Arnold & Porter, after moving to a three-day schedule to help raise her second child, and eventually rose to be the head of the firm's derivatives practice.\n\nBorn was among the first female attorneys to systematically address inequities regarding how the laws treated women. Born and another female lawyer, Marna Tucker, taught what is considered to have been the first \"Women and the Law\" course at Catholic University’s Columbus School of Law. The class exclusively concerned prejudicial treatment of women under the laws of the United States, past and present.  Born and Tucker were surprised to discover that there was no textbook on the issue at the time.  Born is also one of the co-founders of the National Women's Law Center. Born also helped rewrite the American Bar Association rules to make it possible for more women and minorities to sit on federal bench.\n\nDuring her long legal career, and into her retirement, Born did much pro bono and other types of volunteer work. She was active in the American Bar Association, the largest professional organization of lawyers in the United States.  Initially Born was named a member of the governing council of the ABA's Individual Rights Section, eventually becoming chairperson.  Born and Tucker founded the ABA Women's Caucus, the first organization of female lawyers in the ABA.  She held several other senior positions in the ABA, including being named the first woman member of the ABA's Standing Committee on the Federal Judiciary.  As a member of the Judiciary Committee, Born provided testimony and opinion on persons nominated for federal judgeships. In 1980 she was named chair of the committee.  As chair of the committee, Born was invited to address the U.S. Congress regarding the nomination of Judge Sandra Day O'Connor to the U.S. Supreme Court.\n\nIn 1993, Born's name was floated as a possible candidate for Attorney General of the United States, but Janet Reno was nominated.\n\nIn July 2009, Nancy Pelosi appointed Brooksley Born as a commissioner to the Financial Crisis Inquiry Commission (FCIC).\n\nBorn and the OTC derivatives market\nBorn was appointed to the CFTC on April 15, 1994, by President Bill Clinton. Due to litigation against Bankers Trust Company by Procter and Gamble and other corporate clients, Born and her team at the CFTC sought comments on the regulation of over-the-counter derivatives, a first step in the process of writing CFTC regulations to supplement the existing regulations of the Federal Reserve System,  the Options Clearing Corporation, and the National Association of Insurance Commissioners. Born was particularly concerned about swaps, financial instruments that are traded over the counter between banks, insurance companies or other funds or companies, and thus have no transparency except to the two counterparties and the counterparties' regulators, if any.  CFTC regulation was strenuously opposed by Federal Reserve chairman Alan Greenspan, and by Treasury Secretaries Robert Rubin and Lawrence Summers. On May 7, 1998, former SEC Chairman Arthur Levitt joined Rubin and Greenspan in objecting to the issuance of the CFTC's concept release. Their response dismissed Born's analysis and focused on the hypothetical possibility that CFTC regulation of swaps and other OTC derivative instruments could create a \"legal uncertainty\" regarding such financial instruments,  hypothetically reducing the value of the instruments. They argued that the imposition of regulatory costs would \"stifle financial innovation\" and encourage financial capital to transfer its  transactions offshore. The disagreement between Born and the Executive Office's top economic policy advisors has been described not only as a classic Washington turf war, but also a war of ideologies,  insofar as it is possible to argue that Born's actions were consistent with Keynesian and neoclassical economics while Greenspan, Rubin, Levitt, and Summers consistently espoused neoliberal, and neoconservative policies.\n\nIn 1998, a trillion-dollar hedge fund called Long Term Capital Management (LTCM) was near collapse.  Using mathematical models to calculate debt risk, LTCM used derivatives to leverage $5 billion into more than $1 trillion, doing business with fifteen of Wall Street's largest financial institutions.  The derivative transactions were not regulated, nor were investors able to evaluate LTCM's exposures.  Born stated, \"I thought that LTCM was exactly what I had been worried about\".  In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance.  After intervention by the Federal Reserve, the crisis was averted.  In congressional hearings into the crisis, Greenspan acknowledged that language had been introduced into an agriculture bill that would prevent CFTC from regulating the derivatives which were at the center of the crisis that threatened the US economy.  U.S. Representative Maurice Hinchey (D-NY) asked \"How many more failures do you think we'd have to have before some regulation in this area might be appropriate?\"  In response, Greenspan brushed aside the substance of Born's warnings with the simple assertion that \"the degree of supervision of regulation of the over-the-counter derivatives market is quite adequate to maintain a degree of stability in the system\". Born's warning was that there wasn't any regulation of them.  Born's chief of staff, Michael Greenberger summed up Greenspan's position this way: \"Greenspan didn't believe that fraud was something that needed to be enforced, and he assumed she probably did. And of course, she did.\"  Under heavy pressure from the financial lobby, legislation prohibiting regulation of derivatives by Born's agency was passed by the Congress.  Born resigned on June 1, 1999.\n\nThe derivatives market continued to grow yearly throughout both terms of George W. Bush's administration. On September 15, 2008, the bankruptcy of Lehman Brothers forced a broad recognition of a financial crisis in both the US and world capital markets.  As Lehman Brothers' failure temporarily reduced financial capital's confidence, a number of newspaper articles and television programs suggested that the failure's possible causes included the conflict between the CFTC and the other regulators.Faiola, Anthony, Nakashima, Ellen and Drew, Jill. The Crash: Risk and Regulation - What Went Wrong, The Washington Post, October 15, 2008.\n\nBorn declined to publicly comment on the unfolding 2008 crisis until March 2009, when she said: \"The market grew so enormously, with so little oversight and regulation, that it made the financial crisis much deeper and more pervasive than it otherwise would have been.\" She also lamented the influence of Wall Street lobbyists on the process and the refusal of regulators to discuss even modest reforms.\n\nAn October 2009 Frontline documentary titled \"The Warning\"  described Born's thwarted efforts to regulate and bring transparency to the derivatives market, and the continuing opposition thereto. The program concluded with an excerpted interview with Born sounding another warning: \"I think we will have continuing danger from these markets and that we will have repeats of the financial crisis -- may differ in details but there will be significant financial downturns and disasters attributed to this regulatory gap, over and over, until we learn from experience.\"\n\nIn 2009 Born, along with Sheila Bair of the FDIC, was awarded the John F. Kennedy Profiles in Courage Award in recognition of the \"political courage she demonstrated in sounding early warnings about conditions that contributed\" to the 2007-08 financial crisis.  According to Caroline Kennedy, \"Brooksley Born recognized that the financial security of all Americans was being put at risk by the greed, negligence and opposition of  powerful and well connected interests.... The catastrophic financial events of recent months have  proved them [Born and Sheila Bair] right.\"  One member of the President's working group had a change of heart about Brooksley Born.  SEC Chairman Arthur Levitt stated \"I've come to know her as one of the most capable, dedicated, intelligent and committed public servants that I have ever come to know\", adding that \"I could have done much better. I could have made a difference\" in response to her warnings.\n\nIn 2010, a documentary film Inside Job further alleged that derivatives regulation was ineffective from the Clinton administration on. Along with fellow whistleblower, former IMF Chief Economist Raghuram Rajan, who was also scorned by the economic establishment, Brooksley Born was cited as one of the authorities arguing that financial derivatives increase economic risk.\n\n Personal life \nBorn is married to Alexander E. Bennett (also retired from Arnold & Porter).  She has five adult children - two from a previous marriage to Jacob Landau and three stepchildren. Notably, Born was named a partner at Arnold & Porter while working part-time so she could raise her two young children.  When both of her children were school-age, Born returned to practice full-time.\n\nReferences\n\nExternal links\nAttorney profile at Arnold & Porter\nBrooksley Born (2009 Winner) of the Profiles in Courage Award, with acceptance speech transcript and NECN video\n\nProfile at MarketsWiki\nSpeeches and statements\n\"Testimony Of Brooksley Born Chairperson of the CFTC Concerning The Over-The-Counter Derivatives Market\", before the House Committee On Banking And Financial Services, July 24, 1998.\n\"The Lessons of Long Term Capital Management L.P.\", Remarks of Brooksley Born, Chairperson of the CFTC, Chicago-Kent-IIT Commodities Law Institute, Chicago, Illinois, October 15, 1998.\n Interview: Brooksley Born for \"PBS Frontline: The Warning\", PBS, (streaming VIDEO 1 hour), October 20, 2009.\nArticles\nManuel Roig-Franzia. \"Credit Crisis Cassandra:Brooksley Born's Unheeded Warning Is a Rueful Echo 10 Years On\", The Washington Post, May 26, 2009\n Taibbi, Matt. \"The Great American Bubble Machine\", Rolling Stone'', July 9–23, 2009\n\n1940 births\nAmerican women lawyers\nArnold & Porter people\nClinton administration personnel\nColumbus School of Law faculty\nCommodity Futures Trading Commission personnel\nHeads of United States federal agencies\nLawyers from San Francisco\nLiving people\nStanford Law School alumni\n21st-century American women\n"
  },
  {
    "id": "d70ca68d05af951cd8ff2052095597d1b4dca557bfb0b40b",
    "content": "\\section{INTRODUCTION}\nThe Tevatron Collider Run II started in March 2002 and is expected\nto continue until the end of this decade. The Tevatron and the \ntwo detectors, CDF and D\\O, have been performing  well in 2004,\neach experiment is collecting data at the rate \nof $\\approx$10 pb$^{-1}$ per week.\nThe total  luminosity accumulated by August 2004 is $\\approx$500 pb$^{-1}$\nper detector.\nThe rich physics program includes the\nproduction and precision measurement of properties of  standard model (SM)\nobjects, as well as searches for phenomena beyond standard model.\nIn this brief review we focus on areas of most interest \nto the lattice community. We present\nnew results on the top quark mass\nand their implication for the mass of the SM Higgs boson, \non searches for the SM Higgs boson, on evidence for the $X(3872)$ state, \non searches for pentaquarks, and on $b$ hadron properties.\nAll Run II results presented here are preliminary. \n\n\\section{TOP QUARK MASS}\n\nThe experiments CDF and D\\O\\ published several direct  measurements of\nthe top quark pole mass, $\\ensuremath{M_{\\mathrm{top}}}$, \nbased on Run I data (1992-1996).\nThe ``lepton $+$ jets'' channel yields the most precise determination of\n$\\ensuremath{M_{\\mathrm{top}}}$. Recently, the\nD\\O\\ collaboration published a new measurement~\\cite{Mtop1-D0-l+j-new},\nbased on a powerful analysis technique yielding  greatly improved precision.\nThe differential probability \nthat the measured variables in any event correspond to the signal\nis calculated as a function of $\\ensuremath{M_{\\mathrm{top}}}$. \nThe maximum in the product of the individual event probabilities \nprovides the best estimate of $\\ensuremath{M_{\\mathrm{top}}}$.\nThe critical differences from previous analyses \nin the lepton $+$ jets decay channel lie in \nthe assignment of more \nweight to events that are well measured or more likely to correspond to  \n$t \\bar t$ signal, \nand  the handling of the combinations of final-state objects\n(lepton, jets, and imbalance in transverse momentum) \nand their identification with\ntop-quark decay products in an event. \nThe new combined value for the top-quark mass from Run I is \n$\\ensuremath{M_{\\mathrm{top}}}  =  178.0\\pm4.3~\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$.\n\nIn Run II, both collaborations  have been exploring several different techniques \nfor $\\ensuremath{M_{\\mathrm{top}}}$\nmeasurements. The best single CDF result comes from a dynamic likelihood method\n(DLM). The method is similar to\nthe technique used in Ref.~\\cite{Mtop1-D0-l+j-new}.\nThe result is $\\ensuremath{M_{\\mathrm{top}}} = 177.8^{+4.5}_{-5.0} (stat) \\pm  6.2 (syst) ~\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$.\nThe joint likelihood of the selected events is shown in Fig. ~\\ref{fig:cdf_tml}. \nThe Run II goal is a 1\\% uncertainty on $\\ensuremath{M_{\\mathrm{top}}}$. \n\n\n\n\n\\begin{figure}[htb]\n\\vspace*{-5mm}\n\\includegraphics[height=5.8cm,width=8.1cm]  {data_22ev_likelihood.eps}\n\\vspace*{-1.2cm}\n\\caption{The joint likelihood of top candidates(CDF).}\n\\label{fig:cdf_tml}\n\\end{figure}\n\n\n\n\n\\section{SEARCH FOR SM HIGGS BOSON}\n\n\nThe constraints on the SM Higgs ($H$)  boson  mass from\npublished  measurements, updated to include the new D\\O\\ top mass\nmeasurement~\\cite{Mtop1-D0-l+j-new}, are\n$M_H = 117 ^{+67}_{-45}~\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$, $M_H < 251~\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$ at 95\\% C.L.\nThe  new most likely  value of $M_H$\nis above the experimentally excluded range,\nand sufficiently low for $H$ to be observed at the Tevatron.\n\n\n\\begin{figure}[htb]\n\\vspace*{-5mm}\n\\includegraphics[height=7.5cm,width=7.8cm]  {d0_wbb_fig_3_err.eps}\n\\vspace*{-1.1cm}\n\\caption{Distribution of the dijet\ninvariant mass for $W+2 b$-tagged jets  events,\ncompared to the expectation (D\\O). \n}\n\\label{fig:d0_wbb_2tag}\n\\end{figure}\n\n\n\nD\\O\\  has conducted a search for $H$ at $M_H < 140~\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$ \nin the production channel  \n$p \\bar{p} \\rightarrow WH \\rightarrow  e \\nu b \\bar{b}$. \nThe experimental signature of  $WH \\rightarrow e \\nu b \\bar{b}$\nis a final state with \none high $p_T$ electron, two  $b$ jets, and\nlarge missing transverse energy  resulting from\nthe undetected neutrino.\nThe dominant backgrounds to $WH$ production\nare  $W b \\bar{b}$, $t \\bar{t}$ and single-top production.\nThe distribution \nof the dijet mass for events with two $b$-tagged jets is shown in\nFig.~\\ref{fig:d0_wbb_2tag}. \nAlso shown is the  expected contribution ($0.06$ events)  \nfrom the $b \\bar{b}$ decay of a\nSM Higgs boson with $M_H =$ 115 $\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$.\nNo events are observed in the  dijet mass window of 85--135  $\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$.\nD\\O\\ sets a limit on the cross section\nfor $\\sigma( p\\bar{p} \\rightarrow WH) \\times B(H \\rightarrow b \\bar{b}) $\nof 9.0 pb at the 95\\% C.L.,  for a 115  $\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$ Higgs boson.\nThe results for mass points 105, 125, and 135 $\\ensuremath{\\mathrm{ Ge\\kern -0.1em V }\\kern -0.2em /c^2 }$\n are 11.0, 9.1 and 12.2 pb,  respectively.\n\n\n\n\\begin{figure}[htb]\n\\vspace*{-1.2cm}\n\\includegraphics[height=0.33\\textheight,width=8.0cm]{whww_aps04_bw.eps}\n\n\\vspace*{-1.2cm}\n\\caption{95\\% limits on the $H$ production (CDF).}\n\\label{fig:cdf_whww}\n\\end{figure}\n\n\nCDF  has done  a similar search, allowing either an  electron or a muon  \nin the final state.  Both groups have also searched for $H$ produced in\ngluon-gluon fusion, with subsequent decay to a pair of $W$ bosons.\nThe CDF results for both channels  are shown in Fig.~\\ref{fig:cdf_whww}. \n\n\n\n\\section{THE STATE X(3872)}\n\n\n\\begin{figure}[htb]\n\n\\includegraphics[height=8.0cm,width=7.5cm]  {X3872cdfPRL1FullM.eps}\n\\vspace*{-1cm}\n\\caption{The $X(3872)$ signal (CDF).}\n\\label{fig:cdf_x}\n\\end{figure}\n\n\n\n\n The existence of the $X(3872)$ state discovered by \nthe Belle Collaboration~\\cite{Belle-X}\n has been confirmed \n in $p \\bar{p}$ collisions by  CDF~\\cite{cdf-X} (see Fig.~\\ref{fig:cdf_x})\nand D\\O~\\cite{d0-X}.\n It is still unclear whether this particle is a $c\\bar{c}$ state,\n or a more complex object.  When the data are separated according to\nproduction and decay variables, D\\O\\  finds no significant\ndifferences between the $X(3872)$ and\nthe $c \\bar{c}$ state $\\psi(2S)$.\nCDF has analysed the ``lifetime'' distribution of the $X(3872)$ events in order to\nquantify what fraction of this state arises from decay of $B$ hadrons, as opposed to\nthose produced promptly. The authors find that for the selected samples\n28.3$\\pm$1.0$(stat)\\pm$0.7$(syst)$\\% of $\\psi(2S)$ candidates are from $b$ decays,\nwhereas 16.1$\\pm$4.9$(stat)\\pm$2.0$(syst)$\\% of $X$ mesons arise from such decays.\n\n\n\n\n\n\\section{SEARCH FOR PENTAQUARKS}\n\n\n\n\\begin{figure}[htb]\n\n\\includegraphics[height=0.27\\textheight,width=7.6cm]  {mpks_1stminbias.eps}\n\\vspace*{-1.2cm}\n\n\\caption{Invariant mass distribution of an identified proton and a $K^0_s$ candidate. (CDF)\n}\n\\label{fig:pqtheta}\n\\end{figure}\n\n\n\n\\begin{figure}[htb]\n\n\\vspace*{-0.9cm}\n\\includegraphics[height=0.25\\textheight,width=8.0cm]  {CM_xicst_cc_1.eps}\n\\vspace*{-1.2cm}\n\\caption{Invariant mass distribution of the $(\\Xi^-,\\pi^+)$ system. (CDF) \n}\n\\label{fig:pqxi}\n\\end{figure}\n\n\n\\begin{figure}[htb]\n\\vspace*{-0.9cm}\n\n\\includegraphics[height=0.25\\textheight,width=7.6cm]  {theta_note_dstp_dedx_pt.eps}\n\\vspace*{-1.2cm}\n\\caption{Mass of the ($D^{*+}\\bar p$) system. The arrow indicates the position of \nthe $\\Theta_c$ state (CDF).}\n\\label{fig:pqthetac}\n\\end{figure}\n\n\n\nFollowing reports of evidence for exotic\nbaryons containing five quarks (pentaquarks), CDF has analysed \nits data for evidence of the following pentaquarks:\n$\\Theta^+$ ($uud\\bar d \\bar s$), doubly strange states \n$\\Xi_{3/2}$, charmed states $\\Theta_c$, and, most recently, \na state $(udus\\bar b)$, dubbed $R^+_s$, through its weak decay to $(J/\\psi, p)$. \nWith its excellent particle indentification and mass resolution,\nCDF has a unique capability to search for  pentaquark states.\nThe signals of known states: $\\phi$, $\\Lambda$,\n$\\Lambda(1520)$, $K^*$, $\\Xi$, \ncompare favorably with those provided\nby the authors of  the pentaquark evidence.\nThe group finds no evidence for pentaquark states, see Figs \n~\\ref{fig:pqtheta},{\\ref{fig:pqxi},\\ref{fig:pqthetac}.\nThis can be interpreted as an indication that the pentaquark production \nin $p \\bar p$ collisions is heavily suppressed compared to the conventional\nhadron production, or as an evidence against the existence of pentaquarks.\n\n\\clearpage\n\n\\section{RECENT B PHYSICS RESULTS}\n\n\n\\subsection{Spectroscopy}\n\nCDF has measured the mass of $b$ hadrons in exclusive $J/\\psi$ channels.\nThe measurements of the $B_s$ and $\\Lambda_b$ (Fig. \\ref{fig:masslb})\nmasses are the current world's best.\\\\\n\n$m(B^+)$ = 5279.10$\\pm$0.41$(stat)\\pm$0.36$(syst)$,\n\n$m(B^0)$ = 5279.63$\\pm$0.53$(stat)\\pm$0.33$(syst)$,\n\n$m(B_s)$ = 5366.01$\\pm$0.73$(stat)\\pm$0.33$(syst)$,\n\n$m(\\Lambda_b)$ = 5619.7$\\pm$1.2$(stat)\\pm$1.2$(syst)$ MeV/$c^2$.\\\\\n\n\n\\begin{figure}[htb]\n\\vspace*{-1mm}\n\\includegraphics[height=0.30\\textheight,width=7.5cm]  {lambdav1c.eps}\n\\vspace*{-1cm}\n\n\\caption{The mass spectrum of $\\Lambda_b$ candidates (CDF).}\n\\label{fig:masslb}\n\\end{figure}\n\n\nD\\O\\ reports the first observation of the excited $B$ mesons \n$B_1$ and $B^*_2$ as two separate states in fully reconstructed\ndecays to $B^{(*)}\\pi$. The mass of $B_1$ is measured to be\n5724$\\pm$4$\\pm$7 MeV/c$^2$, and the mass difference $\\Delta M$ between\n$B^*_2$ and $B_1$ is 23.6$\\pm$7.7$\\pm$3.9 MeV/c$^2$\n(Fig.  \\ref{fig:d0_bexc}).\n\nD\\O\\ observes semileptonic $B$ decays to narrow $D^{**}$ states,\nthe orbitally excited states  of the $D$ meson\nseen as resonances in the $D^{*+}\\pi^-$ invariant mass spectrum.\nThe $D^*$ mesons are reconstructed through the decay sequence \n$D^{*+} \\rightarrow D^0\\pi^+$, $D^0\\rightarrow K^-\\pi^+$.\nThe invariant mass  of oppositely charged $(D^*,\\pi)$ pairs\nis shown in Fig.  \\ref{fig:d0_dstst}.\nThe mass peak between 2.4 and 2.5 GeV/$c^2$ can be interpreted as two merged \nnarrow $D^{**}$ states, $D^0_1(2420)$ and $D^0_2(2460)$.\nThe combined branching fraction is \n$ {\\cal B}(B\\rightarrow D^0_1,D^0_2)\\cdot {\\cal B}(D^0_1,D^0_2\\rightarrow D^{*+}\\pi^-)=(0.280\\pm0.021(stat)\\pm0.088(syst)$\\%. The systematic error includes the unknown phase between the\ntwo resonances. Work is in progress on extracting the two Breit-Wigner\namplitudes.\n\n\n\\begin{figure}[htb]\n\\vspace*{-2mm}\n\\hspace*{-3mm}\n\\includegraphics[height=0.28\\textheight,width=8.3cm]  {B08F02.eps}\n\n\\vspace*{-1cm}\n\\caption{Mass difference $\\Delta M = M(B\\pi)-M(B)$ for exclusive $B$ decays.\nThe background-subtracted signal is a sum of \n$B^*_1 \\rightarrow B^* \\pi$, $B^* \\rightarrow B \\gamma $ (open area)\nand $B^*_2 \\rightarrow B^*\\pi$ $B^*\\rightarrow B \\gamma$ (lower peak in the shaded area)\nand $B^*_2 \\rightarrow B \\pi$ (upper peak in the shaded area)  \n(D\\O).}\n\\label{fig:d0_bexc}\n\\end{figure}\n\n\n\\begin{figure}[htb]\n\\includegraphics[height=0.25\\textheight,width=7.5cm]  {B05F03.eps}\n\n\\vspace*{-1cm}\n\\caption{The invariant mass distribution of\n$(D^*,\\pi)$ pairs, opposite sign (points) and same-sign (solid histogram).}\n\\label{fig:d0_dstst}\n\\end{figure}\n\n\n\n\n\n\n\\subsection{Lifetimes}\n\n\nCDF and D\\O\\ have measured  lifetimes of $b$ hadrons through the exclusively\nreconstructed decays $B^+ \\rightarrow J/\\psi K^+$, $B^0 \\rightarrow J/\\psi K^{*0}$,\n$B_s \\rightarrow J/\\psi \\phi$, \nand $\\Lambda_b \\rightarrow J/\\psi \\Lambda$\n(Fig. \\ref{fig:d0_lbctau}).\nThe latest results are:  \\\\\n\n\n\n $\\tau(B^+)$=1.65 $\\pm$ 0.08 $^{+0.096}_{-0.123}$  ps ~(D\\O\\ 2003),\n\n $\\tau(B^+)$=1.662 $\\pm$ 0.033  $\\pm$ 0.008  ps ~(CDF),\n\n $\\tau(B^0_d)$=1.473  $^{+0.052}_{-0.050}$ $\\pm$ 0.023    ps ~(D\\O).\n\n $\\tau(B^0_d)$=1.539 $\\pm$ 0.051  $\\pm$ 0.008  ps ~(CDF),\n\n $\\tau(B^0_s)$=1.444   $^{+0.098}_{-0.090}$ $\\pm$ 0.020   ps ~(D\\O),\n\n $\\tau(B^0_s)$=1.369 $\\pm$ 0.100 $\\pm$ $^{+0.008}_{0.010}$  ps ~(CDF),\n\n\n $\\tau(\\Lambda_b)$=1.221 $^{+0.217}_{-0.179}$ $\\pm$ 0.043  ps ~(D\\O),\n\n\n $\\tau(\\Lambda_b)$=1.25 $\\pm$ 0.26 $\\pm$ 0.10  ps ~(CDF 2003).\\\\\n\n\n\nThe measured lifetimes correspond to the following lifetime ratios:\\\\\n\n$\\tau(B^+)/\\tau(B^0_d)$   =  1.080$\\pm$0.042     ~(CDF),\n \n$\\tau(B^0_s)/\\tau(B^0_d)$ =  0.890$\\pm$0.072    ~(CDF),\n\n$\\tau(B^0_s)/\\tau(B^0_d)$ = 0.980$ ^{+0.075}_{-0.070}   \\pm$0.003    ~(D\\O),\n\n$\\tau(\\Lambda_b)/\\tau(B^0_d)$ =  0.874$ ^{+0.169}_{-0.142}   \\pm$0.028    ~(D\\O).\\\\\n\n\n\n\\begin{figure}[htb]\n\\includegraphics[height=0.3\\textheight,width=8.2cm]  {d0_lbctau_B11F02.eps}\n\\vspace*{-1cm}\n\n\\caption{ Fit projection on  $c\\tau$ for the $\\Lambda_b$ candidates.  (D\\O)}\n\\label{fig:d0_lbctau}\n\\end{figure}\n\n\nThe $B_s$ lifetime measurements listed above are results of\na single-lifetime fit to data, integrated over the decay angles.\nBecause  of the presence of  final\nstates  common to ${B_s^0}$\\ and its charge conjugate ${\\overline{B}_s^0}$,\nthe two meson states   are expected\nto mix in such a way that the two CP  eigenstates may have a relatively\nlarge lifetime difference.\nIt is possible to\nseparate the two CP components of ${B_s^0 \\rightarrow J/\\psi \\phi}$\\ and thus to measure the\nlifetime difference by studying the time evolution of the\npolarization states of the vector mesons in the final state.\nCDF has carried out a combined analysis of $B_s$ lifetimes \nand polarization amplitudes. The results for the lifetimes of the\nlow mass (CP even) and high mass (CP odd) eigenstates, and the relative \nwidth difference are:\\\\\n\n $\\tau_L = 1.05 ^{+0.16}_{-0.13} \\pm 0.02$ ~ps,\n \n $\\tau_H = 2.07 ^{+0.58}_{-0.46} \\pm 0.03$ ~ps,\n\n $\\Delta \\Gamma /\\overline \\Gamma   = 0.65 ^{+0.25}_{-0.33} \\pm 0.01$.\\\\\n\nFigure \\ref{fig:cdf_dg} shows  the scan of the likelihood function \nfor $\\Delta \\Gamma /\\overline \\Gamma$.\nPseudoexperiments tossed with $\\Delta \\Gamma /\\overline \\Gamma =0$\nyield the betting odds for observing the above results at\n1/315. For $\\Delta \\Gamma /\\overline \\Gamma = 0.12$ (SM prediction,\nwhich has recently been updated to 0.14$\\pm$0.05~\\cite{dg_un}) the betting odds are\n1/84.\n\n\\begin{figure}[htb]\n\\vspace*{-1mm}\n\\includegraphics[height=0.3\\textheight,width=8.2cm]  {cdf_scan-dg-un.eps}\n\n\\vspace*{-1cm}\n\\caption{Scan of the likelihood function \nfor $\\Delta \\Gamma /\\overline \\Gamma$ (CDF).\n}\n\\label{fig:cdf_dg}\n\\end{figure}\n\n\n\n\nD\\O\\ has used a novel technique to  measure the lifetime ratio\nof the charged and neutral $B$ mesons, exploiting the large\nsemileptonic sample. $B$ hadrons were reconstructed in the channels\n$B\\rightarrow \\mu^+ \\nu D^*(2010)^-X$, which are dominated by $B^0$ decays, \nand  $B\\rightarrow \\mu^+ \\nu D^0X$, which are dominated by $B^+$ decays.\nThe lifetime ratio was\nobtained from the variation of the ratio of the number of events in these two\nprocesses at different decay lengths.\nThe result is \\\\\n\n\n$\\tau(B^+)/\\tau(B^0_d)$   =  1.093$\\pm$0.021$\\pm$0.022.      ~(D\\O)\n\n\n\n\n\\subsection{Towards $B_s$ mixing}\n\nMeasurement of the $B_s$ oscillation frequency via ${B_s^0}$ -${\\overline{B}_s^0}$ ~mixing\nwill provide an important constraint on the CKM matrix. The oscillation\nfrequency is proportional to the mass difference between the mass eigenstates,\n$\\Delta m_s$, and is related to the CKM matrix through \n$\\Delta m_s \\propto |V_{tb}V_{ts}|$. When combined with the\n$B_d$ mass difference, $\\Delta m_d$ it helps in extraction of $|V_{td}|$,\nand thereby the CP violating phase. \n\nAs a benchmark for future $B_s$ oscillation measurement, both groups\nstudy  $B_d$ mixing, gaining an understanding of the different components\nof a $B$ mixing analysis (sample composition, flavor tagging, vertexing,\nasymmetry fitting). For a sample of partially reconstructed decays\n$B\\rightarrow D^*(2010)^+\\mu^-X$, D\\O\\ obtains \n$\\Delta m_d = 0.506 \\pm 0.055 (stat) \\pm  0.049 (syst))$ ps$^{-1}$ and\n$\\Delta m_d = 0.488 \\pm 0.066 (stat) \\pm  0.044 (syst))$ ps$^{-1}$\nwhen employing  opposite side muon tagging and the same side tagging,\nrespectively.\n\nThe CDF result for semileptonic channels is\n$\\Delta m_d = 0.536 \\pm 0.037 (stat) \\pm  0.009 (s.c.) \\pm 0.015 (syst)$ ps$^{-1}$.\nCDF also reports a result on $B$ oscillations using fully reconstructed\ndecays:\n$\\Delta m_d = 0.526 \\pm 0.056 (stat) \\pm  0.005 (syst))$ ps$^{-1}$.\n\nReconstructing $B_s$ decays into different final states is another\nimportant\n step in the ${B_s^0}$ -${\\overline{B}_s^0}$ ~mixing analysis.\nThanks to the  large muon and tracking coverage,   D\\O\\ is accumulating\na  high statistics sample of semileptonic $B_s$ decays.\nD\\O\\ reconstructs the $B_s \\rightarrow D^+_s \\mu^- X$ decays, with\n$D^+_s \\rightarrow \\phi \\pi^+ $ and\n$D^+_s \\rightarrow K^* K^+ $,\nat a rate of $\\approx$ 40(25) events per pb$^{-1}$,  respectively.\nFigure \\ref{fig:d0_bsdsphipi} shows the mass distribution of the\n$D^+_s \\rightarrow \\phi \\pi$ candidates.\n\n\n\\begin{figure}[htb]\n\\vspace*{-5mm}\n\\includegraphics[height=0.3\\textheight,width=8.0cm]  {blds-250.eps}\n\\vspace*{-1.2cm}\n\\caption{  $D^+_s \\rightarrow \\phi \\pi^+$  signal. (D\\O)}\n\\label{fig:d0_bsdsphipi}\n\\end{figure}\n\n\n\\begin{figure}[htb]\n\\vspace*{-10mm}\n\\hspace*{-4mm}\n\\includegraphics[height=0.35\\textheight,width=7.9cm]  {cdf_Bs-DsPi-PhiPi.eps}\n\n\\vspace*{-1.0cm}\n\\caption{ $B_s \\rightarrow D_s \\pi$, $D_s \\rightarrow \\phi \\pi$  signal.  (CDF)}\n\\label{fig:cdf_bsdsphipi}\n\\end{figure}\n\n\nCDF has clean signals for fully hadronic, flavor-specific  $B_s$ decays,\nproviding the best sensitivity to $B_s$ oscillations at high\n$\\Delta m_s$. Figure \\ref{fig:cdf_bsdsphipi} shows the signal for\nthe best channel, $B_s \\rightarrow D_s \\pi$, $D_s \\rightarrow \\phi \\pi$.\n\n\\clearpage\n\n\n\\subsection{Rare decays}\n\nThe purely leptonic decays $B_{d,s}^0 \\rightarrow \\mu^+\n\\mu^-$ are flavor-changing neutral current (FCNC) processes.\nIn the standard model, these decays are forbidden at the tree level and\nproceed at a very low rate through higher-order diagrams.\nThe latest SM prediction~\\cite{sm_ref3}\nis ${\\cal B}(B^0_s \\rightarrow \\mu^+ \\mu^-)=(3.42\\pm 0.54)\\times\n10^{-9}$, where the error is dominated by non-perturbative uncertainties. The\nleptonic branching fraction of the $B_d^0$ decay is suppressed by CKM matrix elements $|V_{td}/V_{ts}|^2$\nleading to a predicted SM branching fraction of $(1.00\\pm0.14)\\times 10^{-10}$.\nThe best published experimental bound (Fig.~\\ref{fig:cdf_bsmumu})\n for the branching fraction\nof $B^0_s$ $(B^0_d)$ is presently\n${\\cal B}(B^0_s \\, (B^0_d) \\rightarrow \\mu^+\\mu^-)<7.5\\times 10^{-7}\\, \n(1.9\\times 10^{-7})$ at the 95\\% C.L.~\\cite{cdfII}.\nThe decay amplitude of $B^0_{d,s} \\rightarrow \\mu^+ \\mu^-$ can be\nsignificantly enhanced in some extensions of the SM. \n\n\\begin{figure}[htb]\n\\includegraphics[height=8.3cm,width=7.9cm]  {cdfbsmumu_results_prl.eps}\n\n\\vspace*{-1cm}\n\\caption{Invariant mass for the events passing all requirements. (CDF)}\n\\label{fig:cdf_bsmumu}\n\\end{figure}\n\n\nAssuming no contributions \nfrom the decay $B^0_d\\rightarrow \\mu^+\\mu^-$ in the signal region,\nD\\O\\  finds the conservative upper limit on the branching fraction \nto be ${\\cal B}(B^0_s \\rightarrow \\mu^+ \\mu^-) \\leq 4.6\\times 10^{-7}$ \nat the 95\\% C.L. (Fig.~\\ref{fig:d0_bsmumu}).\n\n\n\n\n\n\n\\begin{figure}[htb]\n\\includegraphics[height=5.0cm,width=8.0cm]  {B06F03.eps}\n\\vspace*{-1cm}\n\\caption{Invariant mass for the events  passing all requirements. (D\\O)}\n\\label{fig:d0_bsmumu}\n\\end{figure}\n\n"
  },
  {
    "id": "91c198d5820bfba5caf396532a0b30a531e9d09a7a9005a4",
    "content": "\\section{Introduction}\n\nUltracold neutral plasmas studied in the laboratory offer access to a regime of plasma physics that scales to describe thermodynamic aspects of important high-energy-density systems, including strongly coupled astrophysical plasmas \\cite{VanHorn,Burrows}, as well as terrestrial sources of neutrons \\cite{Hinton,Ichimaru_fusion,Atzeni,Boozer} and x-ray radiation \\cite{Rousse,Esarey}.  Yet, under certain conditions, low-temperature laboratory plasmas evolve with dynamics that are governed by the quantum mechanical properties of their constituent particles, and in some cases by coherence with an external electromagnetic field.   \n\nThe relevance of ultracold plasmas to such a broad scope of problems in classical and quantum many-body physics has given rise to a great deal of experimental and theoretical research on these systems since their discovery in the late 90s.  A series of reviews affords a good overview of progress in the last twenty years \\cite{Gallagher,Killian_Science,PhysRept,Lyon}.  Here, we focus on the subset of ultracold neutral plasmas that form via kinetic rate processes from state-selected Rydberg gases, and emphasize in particular the distinctive dynamics found in the evolution of molecular ultracold plasmas.  \n\nWhile molecular beam investigations of threshold photoionization spectroscopy had uncovered relevant effects a few years earlier \\cite{Scherzer,Alt}, the field of ultracold plasma physics began in earnest with the 1999 experiment of Rolston and coworkers on metastable xenon atoms cooled in a magneto optical trap (MOT) \\cite{Killian}.  \n\nThis work and many subsequent efforts tuned the photoionization energy as a means to form a plasma of very low electron temperature built on a strongly coupled cloud of ultracold ions.  Experiment and theory soon established that fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes act to elevate the ion temperatures to around one degree Kelvin, and constrain the effective initial electron temperature to a range above 30 K \\cite{Kuzmin,Hanson,Laha}.  \n\nThis apparent limit on the thermal energy of the electrons can be more universally expressed for an expanding plasma by saying that the electron correlation parameter, $\\Gamma_e$, does not exceed 0.25, where, \n\\begin{equation}\n\\Gamma_e = \\frac{e^2}{4\\pi \\epsilon_0 a_{ws}}\\frac{1}{k_B T_e}\n\\label{eqn:gamma_e}\n\\end{equation}\ndefines the ratio of the average unscreened electron-electron potential energy to the electron kinetic energy.  $a_{ws}$ is the Wigner-Seitz radius, related to the electron density by, $\\rho_e = 1/(\\frac{4}{3} \\pi a_{ws}^3)$.  These plasmas of weakly coupled electrons and strongly coupled ions have provided an important testing ground for ion transport theory and the study of electron-ion collision physics \\cite{Strickler}.\n\nSoon after the initial reports of ultracold plasmas formed by direct photoionization, a parallel effort began with emphasis on the plasma that forms spontaneously by Penning ionization and electron-impact avalanche in a dense ultracold Rydberg gas \\cite{Mourachko}.  This process affords less apparent control of the initial electron temperature.  But, pulsed field-ionization measurements soon established that the photoionized plasma and that formed by the avalanche of a Rydberg gas both evolve to quasi-equilibria of electrons, ions and high-Rydberg neutrals \\cite{Rolston_expand,Gallagher}.  \n\nEarly efforts to understand plasmas formed by Rydberg gas avalanche paid particular attention to the process of initiation.  Evolution to plasma in effusive atomic beams was long known for high-Rydberg gases of caesium and well explained by coupled rate equations \\cite{Vitrant}.  But, low densities and ultracold velocity distributions were thought to exclude Rydberg-Rydberg collisional mechanisms in a MOT.  \n\nIn work on ultracold Rydberg gases of Rb and Cs, Gallagher, Pillet and coworkers describe the initial growth of electron signal by a model that includes ionization by blackbody radiation and collisions with a background of uncooled Rydberg atoms \\cite{Mourachko,Gallagher,Li,Comparat,Tanner}. This picture was subsequently refined to include many-body excitation and autoionization, as well as attractive dipole-dipole interactions \\cite{Viteau,Pillet}, later confirmed by experiments at Rice \\cite{Mcquillen}.  \n\nThe Orsay group also studied the effect of adding Rydberg atoms to an established ultracold plasma.  They found that electron collisions in this environment completely ionize added atoms, even when selected to have deep binding energies \\cite{Vanhaecke}.  They concluded from estimates of electron trapping efficiency that the addition of Rydberg atoms does not significantly alter the electron temperature of the plasma.  \n\nTuning pair distributions by varying the wavelength of the excitation laser, Weidem\\\"uller and coworkers confirmed the mechanical effects of van der Waals interactions on the rates of Penning ionization in ultracold $^{87}$Rb Rydberg gases \\cite{Amthor_mech}.  They recognized blackbody radiation as a possible means of final-state redistribution, and extended this mechanical picture to include long-range repulsive interactions \\cite{Amthor_model}.  This group later studied the effects of spatial correlations in the spontaneous avalanche of Rydberg gases in a regime of strong blockade, suggesting a persistence of initial spatial correlations \\cite{RobertdeSaintVincent}.  \n\nRobicheaux and coworkers have recently investigated the question of prompt many-body ionization from the point of view of Monte Carlo classical trajectory calculations \\cite{Goforth}.  For atoms on a regular or random grid driven classically by an electromagnetic field, they find that many-body excitation enhances prompt ionization by about twenty percent for densities greater than $5.6 \\times 10^{-3}/(n_0^2 a_0)^3$, where $n_0$ is the principal quantum number of the Rydberg gas and $a_0$ is the Bohr radius.  They observed that density fluctuations (sampled from the distribution of nearest neighbour distances) have a greater effect, and point to the possible additional influence of secondary electron-Rydberg collisions and the Penning production of fast atoms not considered by the model, but already observed by Raithel and coworkers \\cite{Knuffman}.  \n\nThe Raithel group also found direct evidence for electron collisional $\\ell$-mixing in a Rb MOT \\cite{Dutta}, and used selective field ionization to monitor evolution to plasma on a microsecond timescale in ultracold $^{85}$Rb $65d$ Rydberg gases with densities as low as $10^8$ cm$^{-3}$ \\cite{WalzFlannigan}.  Research by our group at UBC has observed very much the same dynamics in the relaxation of Xe Rydberg gases of similar density prepared in a molecular beam \\cite{Hung2014}.  In both cases, the time evolution to avalanche is well-described by coupled rate equations (see below), assuming an initializing density of Penning electrons determined by Robicheaux's criterion \\cite{Robicheaux05}, applied to an Erlang distribution of Rydberg-Rydberg nearest neighbours.  \n\nTheoretical investigations of ultracold plasma physics have focused for the most part on the long- and short-time dynamics of plasmas formed by direct photoionization \\cite{PhysRept,Lyon}.  In addition to studies mentioned above, key insights on the evolution dynamics of Rydberg gases have been provided by studies of Pohl and coworkers exploring the effects of ion correlations and recombination-reionization on the hydrodynamics of plasma expansion \\cite{Pohl:2003,PPR}.  Further research has drawn upon molecular dynamics (MD) simulations to reformulate rate coefficients for the transitions driven by electron impact between highly excited Rydberg states \\cite{PVS}, and describe an effect of strong coupling as it suppresses three-body recombination \\cite{Bannasch:2011}.  MD simulations confirm the accuracy of coupled rate equation descriptions for systems with $\\Gamma$ as large as 0.3.  Newer calculations suggest a strong connection between the order created by dipole blockade in Rydberg gases and the most favourable correlated distribution of ions in a corresponding strongly coupled ultracold plasma \\cite{Bannasch:2013}.  \n\nTate and coworkers have studied ultracold plasma avalanche and expansion theoretically as well as experimentally.  Modelling observed expansion rates, they recently found that $^{85}$Rb atoms in a MOT form plasmas with effective initial electron temperatures determined by initial Rydberg density and the selected initial binding energy, to the extent that these parameters determine the fraction of the excited atoms that ionize by electron impact in the avalanche to plasma \\cite{Forest}.  This group also returned to the question of added Rydberg atoms, and managed to identify a crossover in $n_0$, depending on the initial electron temperature, that determines whether added Rydberg atoms of a particular initial binding energy act to heat or cool the electron temperature \\cite{Crockett}.   \n\nOur group has focused on the plasma that evolves from a Rydberg gas under the low-temperature conditions of a skimmed, seeded supersonic molecular beam.  In work on nitric oxide starting in 2008 \\cite{Morrison2008,Plasma_expan,Morrison_shock,PCCP}, we established an initial kinetics of electron impact avalanche ionization that conforms with coupled rate equation models \\cite{Saquet2011,Saquet2012,Scaling,haenelCP} and agrees at early times with the properties of ultracold plasmas that evolve from ultracold atoms in a MOT.  We have also observed unique properties of the NO ultracold plasma owing to the fact that its Rydberg states dissociate \\cite{Haenel2017}, and identified relaxation pathways that may give rise to quantum effects \\cite{SousMBL,SousNJP}.  The remainder of this review focuses on the nitric oxide ultracold plasma and the unique characteristics conferred by its evolution from a Rydberg gas in a laser-crossed molecular beam.  \n\n\n\\section{Avalanche to strong coupling in a molecular Rydberg gas}\n\n\\subsection{The molecular beam ultracold plasma compared with a MOT}\n\nWhen formed with sufficient density, a Rydberg gas of principal quantum number $n_0>30$ undergoes a spontaneous avalanche to form an ultracold plasma \\cite{Li,Morrison2008,RobertdeSaintVincent}.  Collisional rate processes combine with ambipolar hydrodynamics to govern the properties of the evolving plasma.  For a molecular Rydberg gas, neutral fragmentation, occurs in concert with electron-impact ionization, three-body recombination and electron-Rydberg inelastic scattering.  Neutral dissociation combined with radial expansion in a shaped distribution of charged particles, can give rise to striking effects of self-assembly and spatial correlation \\cite{Schulz-Weiling2016,Haenel2017}.   \n\nThe formation of a molecular ultracold plasma requires the conditions of local temperature and density afforded by a high mach-number skimmed supersonic molecular beam.  Such a beam propagates at high velocity in the laboratory, with exceedingly well-defined hydrodynamic properties, including a propagation-distance-dependent density and sub-Kelvin temperature in the moving frame \\cite{MSW_tutorial}.  The low-temperature gas in a supersonic molecular beam differs in three important ways from the atomic gas laser-cooled in a magneto-optical trap (MOT).\n\nThe milli-Kelvin temperature of the gas of ground-state NO molecules entrained in a beam substantially exceeds the sub-100 micro-Kelvin temperature of laser-cooled atoms in a MOT.  However, the evolution to plasma tends to erase this distinction, and the two further characteristics that distinguish a beam offer important advantages for ultracold plasma physics:  Charged-particle densities in a molecular beam can exceed those attainable in a MOT by orders of magnitude.  A great many different chemical substances can be seeded in a free-jet expansion, and the possibility this affords to form other molecular ultracold plasmas, introduces interesting and potentially important new degrees of freedom governing the dynamics of their evolution.\n\n\n\\subsection{Supersonic molecular beam temperature and particle density}\n\nSeeded in a skimmed supersonic molecular beam, nitric oxide forms different phase-space distributions in the longitudinal (propagation) and transverse coordinate dimensions.  As it propagates in $z$, the NO molecules reach a terminal laboratory velocity, $u_{\\parallel}$, of about 1400 ${\\rm ms^{-1}}$, which varies with the precise seeding ratio.  \n\nThe distribution of $v_{\\parallel}$, narrows to define a local temperature, $T_{\\parallel}$, of approximately 0.5 K.  The beam forms a Gaussian spatial distribution in the transverse coordinates, $x$ and $y$.  In this plane, the local velocity, $v_{\\perp}(r)$ is defined for any radial distance almost entirely by the divergence velocity of the beam, $u_{\\perp}(r)$.  Phase-space sorting cools the temperature in the transverse coordinates, $T_{\\perp}$ to a value as low as $\\sim 5$ mK \\cite{MSW_tutorial}.  \n\nThe stagnation pressure and seeding ratio determine the local density distribution as a function of $z$.  For example, expanding from a stagnation pressure of 500 kPa with a 1:10 seeding ratio, a molecular beam propagates 2.5 cm to a skimmer and then 7.5 cm to a point of laser interaction, where it contains NO at a peak density of $1.6 \\times 10^{14}$ cm$^{-3}$.  \n\nHere, crossing the molecular beam with a laser beam tuned to the transition sequence, ${\\rm X} ~^2 \\Pi_{1/2} ~N'' = 1 \\xrightarrow{\\omega_1} {\\rm A} ~^2\\Sigma^+ ~N'=0  \\xrightarrow{\\omega_2} n_0 f(2)$ forms a Gaussian ellipsoidal volume of Rydberg gas in a single selected principal quantum number, $n_0$, orbital angular momentum, $\\ell = 3$, NO$^+$ core rotational quantum number, $N^+ = 2$ and total angular momentum neglecting spin, $N=1$.  \n\nA typical $\\omega_1$ pulse energy of 2 $\\mu$J and a Gaussian width of 0.2 mm serves to drive the first step of this sequence in a regime of linear absorption.  Overlapping this volume by an $\\omega_2$ pulse with sufficient fluence to saturate the second step forms a Rydberg gas ellipsoid with a nominal peak density of $5 \\times 10^{12}$ cm$^{-3}$  \\cite{Morrison2008,MSW_tutorial}.  Fluctuations in the pulse energy and longitudinal mode of $\\omega_1$ cause the real density to vary.  For certain experiments, we find it convenient to saturate the $\\omega_1$ transition, and vary the density of Rydberg gas by delaying $\\omega_2$.  An $\\omega_1$-$\\omega_2$ delay, $\\Delta t$, reduces the Rydberg gas density by a precise factor, $e^{-\\Delta t/\\tau}$, where $\\tau$ is the 200 ns radiative lifetime of NO ${\\rm A} ~^2\\Sigma^+ ~N'=0$ \\cite{Carter,Hancock}.\n\n\n\\subsection{Penning ionization}\n\nThe density distribution of a Rydberg gas defines a local mean nearest neighbour distance, or Wigner-Seitz radius of $ a_{ws} =  \\left(3/4 \\pi \\rho \\right)^{1/3} $, where $\\rho$ refers to the local Rydberg gas density.  For example, a Rydberg gas with a density of $ \\rho_0=0.5 \\times 10^{12}$ cm$^{-3} $ forms an Erlang distribution \\cite{Torquato.1990} of nearest neighbour separations with a mean value of $ 2 a_{ws}=1.6$  $\\mu$m.  \n\nA semi-classical model \\cite{Robicheaux05} suggests that 90 percent of Rydberg molecule pairs separated by a critical distance, $ r_c = 1.8 \\cdot 2 n_0^2 a_0 $ or less undergo Penning ionization within 800 Rydberg periods.  We can integrate the Erlang distribution from $ r=0 $ to the critical distance $r = r_c$ for a Rydberg gas of given $n_0$, to define the local density of Penning electrons ($ \\rho_e$ at $t=0$) produced by this prompt interaction, for any given initial local density, $\\rho_0$ by the expression:\n\\begin{equation}\n\\rho_e(\\rho_0,n_0) = \\frac{0.9}{2} \\cdot 4 \\pi \\rho_0 ^2\\int_0^{r_{c}} r^2 \\mathrm{e}^{-\\frac{4\\pi}{3}\\rho_0 r^3}\\mathrm{d}r \\quad.\n\\label{eqn:Erlang}\n\\end{equation}\n\nEvaluating this definite integral yields an equation in closed form that predicts the Penning electron density for any particular initial Rydberg density and principal quantum number.\n\\begin{equation}\n\\rho_e(\\rho_0,n_0) =\\frac{0.9 \\rho_0}{2}(1-\\mathrm{e}^{-\\frac{4\\pi}{3}\\rho_0 r_c^3}) \\quad.\n\\label{Eq:PenDens}\n\\end{equation}\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[scale=0.33]{Penning_Latice.pdf}\n\\caption{Distributions of ion-ion nearest neighbours following Penning ionization and electron-impact avalanche simulated for a predissociating molecular Rydberg gas of initial principal quantum number, $n_0$, from 30 to 80, and density of 10$^{12}$ cm$^{-3}$.  Dashed lines mark corresponding values of $a_{ws}$. Calculated by counting ion distances after relaxation to plasma in 10$^6$-particle stochastic simulations. Integrated areas proportional to populations surviving neutral dissociation.}\n\\label{fig:PL}\n\\end{figure}\n\nPrompt Penning ionization acts on the portion of the initial nearest-neighbour distribution in the Rydberg gas that lies within $r_c$.  When a molecule ionizes, its collision partner relaxes to a lower principal quantum number, $n'<n_0/\\sqrt{2}$.  This close-coupled interaction disrupts the separability of Rydberg orbital configurations in the Penning partner.  This causes mixing with core penetrating states that are strongly dissociative.  Penning partners are thus very likely to dissociate, leaving a spatially isolated distribution of ions.  We refer to the spatial correlation that results as a Penning lattice \\cite{Sadeghi:2014}.  The extent of this effect varies depending on the local density and the selected initial principal quantum number.  Figure \\ref{fig:PL} shows the degree to which Rydberg gases with initial principal quantum numbers from 30 to 80 form a Penning lattice for an initial density of $1 \\times 10^{12} ~{\\rm cm}^{-3}$.  \n\n\\subsection{Spontaneous electron-impact avalanche}\n\nThe electrons produced by prompt Penning ionization start an electron impact avalanche.  The kinetics of this process are well described by a set of coupled rate equations that account for state-to-state electron-Rydberg inelastic scattering, electron-impact ionization and three-body ion-electron recombination \\cite{PPR,Saquet2011,Saquet2012,Scaling} using detailed rate coefficients,  $k_{ij}$, $k_{i,ion}$ and $k_{i,tbr}$ validated by MD simulations \\cite{PVS}.  \n\\begin{eqnarray}\n-\\frac{d\\rho_i}{dt}&=&\\sum_{j}{k_{ij}\\rho_e\\rho_i}-\\sum_{j}{k_{ji}\\rho_e\\rho_j} \\nonumber\\\\\n&& +k_{i,ion}\\rho_e\\rho_i-k_{i,tbr}\\rho^3_e \n  \\label{level_i}\n\\end{eqnarray}\n\\noindent and,\n\\begin{equation}\n\\frac{d\\rho_e}{dt}=\\sum_{i}{k_{i,ion}\\rho_e^2}-\\sum_{i}{k_{i,tbr}\\rho^3_e}\n  \\label{electron}\n\\end{equation}\n\nThe relaxation of Rydberg molecules balances with collisional ionization to determine an evolving temperature of avalanche electrons to conserve total energy per unit volume. \n\\begin{equation}\nE_{tot}=\\frac{3}{2}k_BT_e(t)\\rho_e(t)-R\\sum_i{\\frac{\\rho_i(t)}{n_i^2}},\n  \\label{energy}\n\\end{equation}\nHere, for simplicity, we neglect the longer-time effects of Rydberg predissociation and electron-ion dissociative recombination \\cite{Saquet2012}.\n\nSuch calculations show that the conversion from Rydberg gas to plasma occurs on a timescale determined largely by the local Penning electron density, or Penning fraction, $P_f = \\rho_e/\\rho_0$, which depends on the local density of Rydberg molecules and their initial principal quantum number.  \n\nAvalanche times predicted by coupled rate equation calculations range widely.  For example, in a model developed for experiments on xenon, simulations predict that a Rydberg gas with $n_0 = 42$ at a density of $8.8 \\times 10^8 ~{\\rm cm}^{-3}$ ($P_f = 6 \\times 10^{-5}$) avalanches with a half time of  40 $\\mu$s \\cite{Hung2014}.  At an opposite extreme, rate equations estimate that a Rydberg gas of NO with $n_0=60$ at a density of $1 \\times 10^{12} ~{\\rm cm}^{-3}$ ($P_f = 0.3$) rises to plasma in about 2 ns \\cite{Saquet2012}.  \n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width= .49 \\textwidth]{SFI_n=49.pdf}\n   \\caption{Contour plots showing SFI signal as a function the applied field for an $nf(2)$ Rydberg gas with an initial principal quantum number, $n_0=49$.  Each frame represents 4,000 SFI traces, sorted by initial Rydberg gas density.  Ramp field beginning at 0 and 150 ns (top, left to right), and  300 and 450 ns (bottom) after the $\\omega_2$ laser pulse.  The two bars of signal most evident at early ramp field delay times represent the field ionization of the $49f(2)$ Rydberg state respectively to NO$^+$ X $^1\\Sigma^+$ cation rotational states, $N^+=0$ and 2.  The signal waveform extracted near zero applied field represents the growing population of plasma electrons.  \n   }\n\\label{fig:SFI}\n\\end{figure}\n\nSelective field ionization (SFI) probes the spectrum of binding energies in a Rydberg gas.  Applied as a function of time after photoexcitation, SFI maps the evolution from a state of selected initial principal quantum number, $n_0$, to plasma \\cite{Haenel2017}.  Figure \\ref{fig:SFI} shows SFI spectra taken at a sequence of delays after the formation of $49f(2)$ Rydberg gases of varying density.    \n\nHere, we can see that a $49f(2)$ Rydberg gas with an estimated initial density $\\rho_0 = 3 \\times 10^{11} ~{\\rm cm}^{-3}$ relaxes to plasma on a timescale of about 500 ns.  Observations such as these agree well with the predictions of coupled rate-equation calculations.  We can understand this variation in relaxation dynamics with $\\rho_0$ and $n_0$ quite simply in terms of the corresponding density of prompt Penning electrons these conditions afford to initiate the avalanche to plasma.  \n\nFigure \\ref{fig:scaled_rise} illustrates this, showing how rise times predicted by coupled rate-equation simulations for a large range of initial densities and principal quantum number match when plotted as a function of time scaled by the ultimate plasma frequency and fraction of prompt Penning electrons.  The dashed line gives an approximate account of the scaled rate of avalanche under all conditions of Rydberg gas density and initial principal quantum number in terms of the simple sigmoidal function:\n\n\\begin{equation}\n\\frac{\\rho_e}{\\rho_0} = \\frac{a}{b+e^{-c\\tau}},\n  \\label{scaledEq1}\n\\end{equation}\nwhere,\n\\begin{equation}\n\\tau = t \\omega_e P_f^{3/4},\n  \\label{scaledEq2}\n\\end{equation}\nin which $\\omega_e$ is the plasma frequency after avalanche, $P_f$ is the fraction of prompt Penning electrons, and $a = 0.00062$,  $b =   0.00082$ and $c =     0.075$ are empirical coefficients.  \n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width= .4 \\textwidth]{sim_analytical_density.pdf}\n   \\caption{Rise in fractional electron density as a function of time scaled by the plasma frequency, $\\omega_e$ and fraction, $\\rho_e(t=0)/\\rho_0 = P_f$, of prompt Penning electrons.  Simulation results shown for $n_0 = 30$, 50 and 70 with initial densities, $\\rho_0 = 10^9,~10^{10},~10^{11},~{\\rm and}~10^{12}~{\\rm cm}^{-3}$.  \n   }\n\\label{fig:scaled_rise}\n\\end{figure}\n\n\n\\subsection{Evolution to plasma in a Rydberg gas Gaussian ellipsoid}\n\nAs outlined above, the local density and principal quantum number together determine the rate at which a Rydberg gas avalanches to plasma.  Our experiment crosses a 2 mm wide cylindrically Gaussian molecular beam with a 1 mm diameter TEM$_{00}$ $\\omega_1$ laser beam to produce a Gaussian ellipsoidal distribution of molecules excited to the A $^2\\Sigma^+$ $v=0, ~N'=0$ intermediate state.  A larger diameter $\\omega_2$ pulse then drives a second step that forms a Rydberg gas in a single $n_0f(2)$ state with the spatial distribution of the intermediate state.  \n\nWe model this shaped Rydberg gas as a system of 100 concentric ellipsoidal shells of varying density \\cite{haenelCP}.  Coupled rate equations within each shell describe the avalanche to plasma.  This rate process proceeds from shell to shell with successively longer induction periods, determined by the local density as detailed above.  The rising conversion of Rydberg molecules to ions plus neutral dissociation products conserves the particle number in each shell.  We assume that local space charge confines electrons to shells, conserving quasi-neutrality.  Electrons exchange kinetic energy at the boundaries of each shell, which determines a single plasma electron temperature.  \n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width= .5 \\textwidth]{shell_model_100}\n   \\caption{(top frame) Cross-sectional contour diagram in the $x,y$ plane for $z=0$ describing the distribution of ion plus electron density over 100 shells of Gaussian ellipsoid with initial dimensions, $\\sigma_x= 0.75$ mm and $\\sigma_y= \\sigma_z = 0.42$ mm and an initial $n_0 = 50$ Rydberg gas density, $\\rho_0 = 2 \\times 10^{11}$ cm$^{3}$ after an evolution time of 100 ns.  (bottom frame) Curves describing the (dashed) ascending ion and (solid) descending Rydberg gas densities of each shell as functions of evolution time, for $t=20$, 40, 60, 80 and 100 ns.  \n   }\n\\label{fig:shell}\n\\end{figure}\n\nThe upper frame of Figure \\ref{fig:shell} shows contours of NO$^+$ ion density after 100 ns obtained from a shell-model coupled rate-equation simulation of the avalanche of a Gaussian ellipsoidal Rydberg gas of nitric oxide with a selected initial state, $50f(2)$ and a density of $2 \\times 10^{11}$ cm$^{-3}$.  Here, we simulate a relaxation that includes channels of predissociation at every Rydberg level and redistributes the energy released to electrons, which determines a uniform rising electron temperature for all shells.  \n\nFor comparison, the lower frame plots curves describing the ion density of each shell as a function of time from 20 to 100 ns, as determined by applying Eq \\ref{scaledEq1} for the local conditions of initial Rydberg gas density.  This numerical approximation contains no provision for predissociation.  Coupled rate-equation simulations for uniform volumes show that predissociation depresses yield to some degree, but has less effect on the avalanche kinetics \\cite{Saquet2012}.  Therefore, we can expect sets of numerically estimated shell densities, scaled to agree with the simulated ion density at the elapsed time of 100 ns to provide a reasonable account of the earlier NO$^+$ density profiles as a function of time.   \n\n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width= .4 \\textwidth]{Shell_pop}\n   \\caption{Global population fractions of particles as they evolve in the avalanche of a shell-model ellipsoidal Rydberg gas with the initial principal quantum number and density distribution of Figure \\ref{fig:shell}\n   }\n\\label{fig:shell_yields}\n\\end{figure}\n\nFor each time step, the difference, $\\rho_0 - \\rho_e$ defines the neutral population of each shell.  We assign a fraction of this population to surviving Rydberg molecules, such that the total population of NO$^*$ as a function of time agrees with the prediction of the shell-model simulation, as shown in Figure \\ref{fig:shell_yields}.  We consider the balance of this neutral population to reflect NO$^*$ molecules that have dissociated to form N($^4$S) $+$ O($^3$P).  Figure \\ref{fig:shell} plots these surviving Rydberg densities as a function of radial distance for each evolution time.  At the initial density of this simulation, note at each time step that a higher density of Rydberg molecules encloses the tail of the ion density distribution in $x$.   \n\n\\subsection{Plasma expansion and NO$^+$ - NO$^*$ charge exchange as an avenue of quench}\n\nWe regard the ions as initially stationary.  The release of electrons creates a radial electric potential gradient, which gives rise to a force, $-e\\nabla \\phi_{k,j}(t)$, that accelerates the ions in shell $j$ in direction $k$ according to \\cite{Sadeghi.2012}:\n\n\\begin{align}\n\\frac{-e}{m'}\\nabla \\phi_{k,j}(t) = & \\frac{\\partial u_{k,j}(t)}{\\partial t} \\notag  \\\\\n= & \\frac{k_BT_e(t)}{m'\\rho_j(t)} \\frac{\\rho_{j+1}(t) - \\rho_j(t)}{r_{k,j+1}(t) - r_{k,j}(t)},\n  \\label{dr_dt}\n\\end{align}\n\n\\noindent where $\\rho_j(t)$ represents the density of ions in shell $j$.  \n\nThe instantaneous velocity, $u_{k,j}(t)$ determines the change in the radial coordinates of each shell, $r_{k,j}(t)$, \n\\begin{equation}\n\\frac{\\partial r_{k,j}(t)}{\\partial t}=u_{k,j}(t) = \\gamma_{k,j}(t) r_{k,j}(t),\n  \\label{dr_dt}\n\\end{equation}\n\\noindent which in turn determines shell volume and thus its density, $ \\rho_j(t)$.   \nThe electron temperature supplies the thermal energy that drives this ambipolar expansion.  Ions accelerate and $T_e$ falls according to: \n\n\\begin{equation}\n\\frac{3k_B}{2}\\frac{\\partial T_e(t)}{\\partial t}= -\\frac{m'}{\\sum_{j}{N_j}}\\sum_{k,j}{N_j u_{k,j}(t)\\frac{\\partial u_{k,j}(t)}{\\partial t}},\n  \\label{dr_dt}\n\\end{equation}\n\\noindent where we define an effective ion mass, $m'$, that recognizes the redistribution of the electron expansion force over all the NO$^+$ charge centres by resonant ion-Rydberg charge exchange, which occurs with a very large cross section \\cite{PPR}.  \n\\begin{equation}\nm' =\\left (1+ \\frac{\\rho^*_{j}(t)}{ \\rho_j(t)}\\right) m ,\n  \\label{dr_dt}\n\\end{equation}\n\\noindent in which $\\rho^*_{j}(t)$ represents the instantaneous Rydberg density in shell $j$.\n\nThe initial avalanche in the high-density core of the ellipsoid leaves few Rydberg molecules, so this term has little initial effect.  Rydberg molecules predominate in the lower-density wings.  There, momentum sharing by charge exchange assumes a greater importance.  \n\nWe see this most directly in the $\\omega_2$ absorption spectrum of transitions to states in the $n_0 f(2)$ Rydberg series, detected as the long-lived signal that survives a flight time of 400 $\\mu$s to reach the imaging detector.  The balance between the rising density of ions and the falling density of Rydberg molecules depends on the initial density of electrons produced by prompt Penning ionization.  As clear from Eq \\ref{Eq:PenDens}, this Penning fraction depends sensitively on the principal quantum number, and for all principal quantum numbers, on the initial Rydberg gas density.  \n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width= .4 \\textwidth]{w2_spectra}\n   \\caption{Double-resonant spectra of nitric oxide Rydberg states in the $nf$ series converging to NO$^+$ $v=0$, $N^+=2$ (designated, $nf(2)$), derived from the late-peak signal obtained after a flight time of 400 $\\mu$s by scanning $\\omega_2$ for $\\omega_1$ tuned to NO A $^2\\Sigma^+$ $v=0$, $N'=0$ for initial $nf(2)$ densities from top to bottom of 0.07, 0.10, 0.13, 0.19, 0.27, 0.30, 0.32 and $3 \\times 10^{12}$ cm$^{3}$.  \n  }\n\\label{fig:w2_spectra}\n\\end{figure}\n\nFigure \\ref{fig:w2_spectra} shows a series of $\\omega_2$ late-signal excitation spectra for a set of initial densities.  Here, we see a clear consequence of the higher-order dependence of Penning fraction - and thus the NO$^+$ ion - NO$^*$ Rydberg molecule balance - on $n_0$, the $\\omega_2$-selected Rydberg gas initial principal quantum number.  This Penning-regulated NO$^+$ ion - NO$^*$ Rydberg molecule balance appears necessary as a critical factor in achieving the long ultracold plasma lifetime required to produce this signal.  We are progressing in theoretical work that explains the stability apparently conferred by this balance.  \n\n\n\\subsection{Bifurcation and arrested relaxation}\n\nAmbipolar expansion quenches electron kinetic energy as the initially formed plasma expands.  Core ions follow electrons into the wings of the Rydberg gas.  There, recurring charge exchange between NO$^+$ ions and NO$^*$ Rydberg molecules redistributes the ambipolar force of the expanding electron gas, equalizing ion and Rydberg velocities.  This momentum matching effectively channels electron energy through ion motion into the overall $\\pm x$ motion of gas volumes in the laboratory.  The internal kinetic energy of the plasma, which at this point is defined almost entirely by the ion-Rydberg relative motion, falls.  Spatial correlation develops, and over a period of 500 ns, the system forms the plasma/high-Rydberg quasi-equilibrium dramatically evidenced by the SFI results in Figure \\ref{fig:SFI}.  \n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width= .4 \\textwidth]{Bifurcation.pdf}\n   \\caption{$x,y$ detector images of ultracold plasma volumes produced by 2:1 aspect ratio ellipsoidal Rydberg gases with selected initial state, $40f(2)$ after a flight time of 402 $\\mu$s over a distance of 575 mm.  Lower frame displays the distribution in $x$ of the charge integrated in $y$ and $z$.  Both images represent the unadjusted raw signal acquired in each case after 250 shots.  \n   }\n\\label{fig:bifurcation}\n\\end{figure}\n\nIn the wings, momentum redistribution owing to cycles of ion-Rydberg charge transfer retards radial expansion \\cite{Pohl2003,PPR}.  By redirecting electron energy from ambipolar acceleration to $\\pm x$ plasma motion, NO$^+$ to NO$^*$ charge exchange dissipates electron thermal energy.  This redistribution of energy released in the avalanche of the Rydberg gas to plasma, causes the ellipsoidal Rydberg gas to bifurcate \\cite{Schulz-Weiling2016,Haenel2017}, forming very long-lived, separating charged-particle distributions.  We capture the electron signal from these recoiling volumes on an imaging detector as pictured in Figure \\ref{fig:bifurcation}.  Here, momentum matching preserves density and enables ions and Rydberg molecules to relax to positions that minimize potential energy, building spatial correlation.  \n\nThe semi-classical description of avalanche and relaxation outlined above forms an important point of reference from which to interpret our experimental observations.  The laser crossed molecular beam illumination geometry creates a Rydberg gas with a distinctively shaped high-density spatial distribution.  This initial condition has an evident effect on the evolution dynamics.  We have developed semi-classical models that explicitly consider the coupled rate and hydrodynamic processes governing the evolution from Rydberg gas to plasma using a realistic, ellipsoidal representation of the ion/electron and Rydberg densities \\cite{haenelCP}.  No combination of initial conditions can produce a simulation that conforms classically with the state of arrested relaxation we observe experimentally. \n\n\n\\subsection{A molecular ultracold plasma state of arrested relaxation}\n\nThus, we find that spontaneous avalanche to plasma splits the core of an ellipsoidal Rydberg gas of nitric oxide. As ambipolar expansion quenches the electron temperature of this core plasma, long-range, resonant charge transfer from ballistic ions to frozen Rydberg molecules in the wings of the ellipsoid quenches the ion-Rydberg molecule relative velocity distribution. This sequence of steps gives rise to a remarkable mechanics of self-assembly, in which the kinetic energy of initially formed hot electrons and ions drives an observed separation of plasma volumes. These dynamics redistribute ion momentum, efficiently channeling electron energy into a reservoir of mass-transport. This starts a process that evidently anneals separating volumes to a state of cold, correlated ions, electrons and Rydberg molecules. \n\nWe have devised a three-dimensional spin model to describe this arrested state of the ultracold plasma in terms of two, three and four-level dipole-dipole energy transfer interactions (spin flip-flops), together with Ising interactions that arise from the concerted pairwise coupling of resonant pairs of dipoles \\cite{SousMBL,SousNJP}.  \n\nThe Hamiltonian includes the effects of onsite disorder owing to the broad spectrum of states populated in the ensemble and the unique electrostatic environment of every dipole.  Extending ideas developed for simpler systems \\cite{Burin1,Sondhi}, one can make a case for slow dynamics, including an arrest in the relaxation of NO Rydberg molecules to predissociating states of lower principal quantum number.\n\nSystems of higher dimension ought to thermalize by energy transfer that spreads from rare but inevitable ergodic volumes (Griffiths regions) \\cite{Sarang2, Roeck_griffith, RareRegions_rev, Thermal_inclusions}.  However, a feature in the self-assembly of the molecular ultracold plasma may preclude destabilization by rare thermal domains:  Whenever the quenched plasma develops a delocalizing Griffiths region, the local predissociation of relaxing NO molecules promptly proceeds to deplete that region to a void of no consequence.\n\nIn summary, the classical dynamics of avalanche and bifurcation appear to create a quenched condition of low temperature and high disorder in which dipole-dipole interactions drive self-assembly to a localized state purified by the predissociation of thermal regions.  We suggest that this state of the quenched ultracold plasma offers an experimental platform for studying quantum many-body physics of disordered systems. \\\\\n\n\\section{Acknowledgments}\nThis work was supported by the US Air Force Office of Scientific Research (Grant No. FA9550-17-1-0343), together with the Natural Sciences and Engineering research Council of Canada (NSERC), the Canada Foundation for Innovation (CFI) and the British Columbia Knowledge Development Fund (BCKDF). \n\n"
  },
  {
    "id": "6b35731428ea6d9b480338b90572d21690c2fbb89ebba249",
    "content": "Paper Info\n\nTitle: Deep Spiking Neural Networks with High Representation Similarity Model Visual Pathways of Macaque and Mouse\nPublish Date: 22 May 2023\nAuthor List: Zhengyu Ma (from Department of Networked Intelligence, Peng Cheng Laboratory), Yu Liutao (from Department of Networked Intelligence, Peng Cheng Laboratory), Huihui Zhou (from Department of Networked Intelligence, Peng Cheng Laboratory), Allen Brain\nAuthor Affiliation: CORNet-S ConvNeXt-Tiny ConvNeXt-Small EfficientNet, AlexNet RegNetY, ResNet34 ConvNeXt-Base CORNetSEW, ResNet8 ResNet101 SEW-ResNet18 ViT-L, GoogLeNet SEW-ResNet34 SEW-ResNet8 Wide\n\nFigure\n\nFigure 1: To conduct neural representation similarity experiments, we apply three similarity metrics to a layer-by-layer comparison between the responses of models and the neural activities of visual cortex.\nFigure 2: For three datasets and three similarity metrics, each point indicates the final representation similarity score of a model.Each pair of SEW ResNet and ResNet with the same depth are linked by a gray solid line.In almost all conditions, SEW ResNet outperforms ResNet by a large margin.\nFigure3: For three datasets and three similarity metrics, we plot the trajectories of similarity score with model layer depth.The models are divided into two groups: ResNet and SEW ResNet.The normalized layer depth ranges from 0 (the first layer) to 1 (the last layer).Because the depths of models are not the same, we first discretize the normalized depth into 50 bins, and then apply the cubic spline interpolation to the scores of each model, yielding the smooth trajectories shown in the plot.The fine, semitransparent lines are the trajectories of each model.The thick lines are the average trajectories among each group.\nFigure 5: For Macaque-Synthetic dataset, trajectories of similarity score with model layer depth are plotted.The models are divided into two groups: ViT and CNN&SNN.The normalized layer depth ranges from 0 (the first layer) to 1 (the last layer).The calculation and plotting of the trajectories are the same as Figure 3.\nFigure6: The basic block of SpikingMobileNet.\"PW CONV\" is the pointwise convolution and \"DW CONV\" is the depthwise convolution.\"SN\" is the spiking neuron.\nFigure 7: Overall model rankings of the similarity scores on Allen Brain mouse dataset.The similarity scores of CNNs, SNNs and vision transformers are shown by blue, green and orange bars, respectively.\nFigure 9: Overall model rankings of the similarity scores on Macaque-Synthetic dataset.\nFigure 10: The Spearman's rank correlation between the overall model rankings of different metrics.There is a strong correlation between SVCCA and TSVD-Reg, but RSA has weaker correlations with them.\nThe correlation between the similarity scores and the model depth.r is Spearman's rank correlation coefficient.\"-\" indicates that there is no significant correlation.\nArchitectures of SNNs.\"sn\" denotes the spiking neuron.\"g = 32\" denotes the grouped convolutions with 32 groups.The hyper-parameters of the spike-element-wise block are shown in the brackets with the number of stacked blocks outside.\n\nabstract\n\nDeep artificial neural networks (ANNs) play a major role in modeling the visual pathways of primate and rodent. However, they highly simplify the computational properties of neurons compared to their biological counterparts. Instead, Spiking Neural Networks (SNNs) are more biologically plausible models since spiking neurons encode information with time sequences of spikes, just like biological neurons do.\nHowever, there is a lack of studies on visual pathways with deep SNNs models. In this study, we model the visual cortex with deep SNNs for the first time, and also with a wide range of state-of-the-art deep CNNs and ViTs for comparison. Using three similarity metrics, we conduct neural representation similarity experiments on three neural datasets collected from two species under three types of stimuli.\nBased on extensive similarity analyses, we further investigate the functional hierarchy and mechanisms across species. Almost all similarity scores of SNNs are higher than their counterparts of CNNs with an average of 6.6%. Depths of the layers with the highest similarity scores exhibit little differences across mouse cortical regions, but vary significantly across macaque regions, suggesting that the visual processing structure of mice is more regionally homogeneous than that of macaques.\nBesides, the multi-branch structures observed in some top mouse brain-like neural networks provide computational evidence of parallel processing streams in mice, and the different performance in fitting macaque neural representations under different stimuli exhibits the functional specialization of information processing in macaques.\nTaken together, our study demonstrates that SNNs could serve as promising candidates to better model and explain the functional hierarchy and mechanisms of the visual system. Originally, the prototype of deep neural networks is inspired by the biological vision system . To date, deep neural networks not only occupy an unassailable position in the field of computer vision , but also become better models of the biological visual cortex compared to traditional models in the neuroscience community (Khaligh-Razavi and Kriegeskorte 2014; .\nThey have been successful at predicting the neural responses in primate visual cortex, matching the hierarchy of ventral visual stream (Güc ¸lü and van Gerven 2015; , and even controlling neural activity . Moreover, as training paradigms of mice and techniques for collecting neural activity (de Vries et al. 2020) have been greatly improved, there is a strong interest in exploring mouse visual cortex.\nDeep neural networks also play an important role in revealing the functional mechanisms and structures of mouse visual cortex . Compared to biological networks, Artificial Neural Networks discard the complexity of neurons . Spiking Neural Networks, incorporating the concept of time and spikes, are more biologically plausible models .\nTo be more specific, because of their capabilities of encoding information with spikes, capturing the dynamics of biological neurons, and extracting spatio-temporal features, deep SNNs are highly possible to yield brain-like representations ). However, deep SNNs have not been employed to model visual cortex due to the immaturity of training algorithms.\nRecently, a state-ofthe-art directly trained deep SNN , makes it possible to use deep SNNs as visual cortex models. Contributions. In this work, we conduct large-scale neural representation similarity experiments on SNNs and other high-performing deep neural networks to study the brain's visual processing mechanisms, with three datasets and three similarity metrics (Figure ).\nSpecifically, to the best of our knowledge, we are the first to use deep SNNs to fit complex biological neural representations and explore the biological visual cortex. We summarize our main contributions in four points as follows. • We find that SNNs outperform their counterparts of CNNs with the same depth and almost the same architectures in almost all experiments.\nIn addition, even with very different depths and architectures, SNNs can achieve top performance in most conditions. • By making a more direct comparison between macaques and mice for the first time, we reveal the differences in the visual pathways across the two species in terms of the homogeneity of visual regions and the increases of receptive field sizes across cortical visual pathways, which is consistent with previous physiological work.\n• The multi-branch structures in neural networks benefit neural representation similarity to mouse visual cortex, providing computational evidence that parallel information processing streams are widespread between cortical regions in the mouse visual system. • Comparing the results of two macaque neural datasets under different stimuli, we reveal that the macaque vision system may have functional specialization for processing human faces and other natural scenes.\nAltogether, as the first work to apply deep SNNs to fit neural representations, we shed light on visual processing mechanisms in both macaques and mice, demonstrating the potential of SNNs as a novel and powerful tool for research on the visual system. Our codes and appendix are available at https://github.com/Grasshlw/SNN-Neural-Similarity.\nThere are plenty of computational models of macaque and mouse visual systems for exploring the visual processing mechanisms recently. We summarize some of the outstanding work in the following. The network models of macaque visual system. In the early days, studies basically used simple feedforward neural networks as the models of the macaque visual system (Khaligh-Razavi and Kriegeskorte 2014; .\nRecently, some bio-inspired or more complex models achieved better performance in fitting the neural representations of macaque visual cortex . proposed a brainlike shallow CNN with recurrent connections to better match the macaque ventral visual stream. By mimicking the primary stage of the primate visual system, VOneNets ) performed more robustly in image recognition while better simulating macaque V1.\nMoreover, the representations learned by unsupervised neural networks ) also effectively matched the neural activity of macaque ventral visual stream. Although the above work developed many bio-inspired structures, the networks are still traditional ANNs in nature. Our work introduces deep SNNs for the first time to explore the visual processing mechanisms of macaque visual system.\nThe network models of mouse visual system. Largescale mouse neural dataset provided an experimental basis for model studies of mouse visual system (de Vries et al. 2020; . conducted comparisons between the representations of mouse visual cortex and the VGG16 trained on the Im-ageNet dataset. In , they developed a single neural network to model both the dorsal and ventral pathways with showing the functional specializations.\nWhat's more, a large survey of advanced deep networks ) revealed some hierarchy and functional properties of mice. Similar to the studies of macaque visual system, deep SNNs have never been used to model the mouse visual system. In this work, we not only use SNNs as one of the candidates to fit the representations of mouse visual cortex, but also conduct direct comparisons between macaques and mice to further investigate the functional hierarchy and mechanisms of the two species.\nOur work is conducted with three neural datasets. These datasets are recorded from two species under three types of stimuli. More specifically, there are neural responses of mouse visual cortex to natural scene stimuli, and responses of macaque visual cortex to face image and synthetic image stimuli. Allen Brain mouse dataset.\nIt is part of the Allen Brain Observatory Visual Coding dataset ) col-lected using Neuropixel probes from 6 regions simultaneously in mouse visual cortex. Compared to two-photon calcium imaging, Neuropixel probes simultaneously record the spikes across many cortical regions with high temporal resolution.\nIn these experiments, mice are presented with 118 250-ms natural scene stimuli in random orders for 50 times. Hundreds to thousands of neurons are recorded for each brain region. To get the stable neurons, we first concatenate the neural responses (average number of spikes in 10-ms bins across time) under 118 images for each neuron, and then preserve the neurons whose split-half reliability across 50 trials reaches at least 0.8.\nMacaque-Face dataset. This dataset ) is composed of neural responses of 159 neurons in the macaque anterior medial (AM) face patch under 2,100 real face stimuli, recorded with Tungsten electrodes. For this dataset, we compute the average number of spikes in a time window of 50-350ms after stimulus onset and exclude eleven neurons with noisy responses by assessing the neurons' noise ceiling.\nThe details of the preprocessing procedure are the same as . Macaque-Synthetic dataset. This dataset is also about macaque neural responses which are recorded by electrodes under 3,200 synthetic image stimuli, and used for neural prediction in the initial version of Brain-Score . The image stimuli are generated by adding a 2D projection of a 3D object model to a natural background.\nThe objects consist of eight categories, each with eight subclasses. The position, pose, and size of each object are randomly selected. 88 neurons of V4 and 168 neurons of IT are recorded. The neural responses are preprocessed to the form of average firing rate and can be downloaded from Brain-Score. Since the core visual function of macaque and mouse visual cortex is to recognize objects, the basic premise of model selection is that the model has good performance on object recognition tasks (e.g.\nclassification on ImageNet). Based on this premise, we employ 12 SNNs, 43 CNNs, and 26 vision transformers, all of which are pretrained on the Ima-geNet dataset and perform well in the classification task. As for SNNs, we use SEW ResNet as the base model, which is the deepest and SOTA directly trained SNN .\nFurthermore, by combining the residual block used in SEW ResNet and the hierarchy of the visual cortex, we build several new SNNs and train them on the ImageNet using SpikingJelly ) (see Appendix A for model structures and the details of model training). As for CNNs and vision transformers, we use 44 models from the Torchvision model zoo , 22 models from the Timm model zoo ) and 3 models from the brain-like CNNs, CORnet family ).\nIn the feature extraction procedures of all models, we feed the same set of images used in biological experiments to the pretrained models and obtain features from all chosen layers. Different from CNNs and vision transformers, the features of SNNs are spikes in multiple time steps. To obtain the representation similarity between biological visual cortex and computational models, we apply three similarity metrics to computing similarity scores: representational similarity analysis (RSA) , regression-based encoding method and singular vector canonical correlation analysis (SVCCA) .\nRSA has already been widely used to analyze neural representations of a model and a brain to different stimuli at the population level, while the regression-based encoding method directly fits the model features to neural activity data. SVCCA is originally proposed to compare features of deep neural networks, and then Buice 2019) used it to compare representation matrices from mouse visual cortex and DNNs, which demonstrated its effectiveness.\nWith the same model and same cortical region, we use these metrics for a layer-by-layer comparison to compute the similarity scores. The maximum similarity score across layers for a given cortical region is considered to be the level of representation similarity between the model and the cortical region.\nFinally, in a given dataset, we take the average score of all cortical regions as the final similarity score for each model, which gives the overall model rankings. The implementation of each similarity metric is as follows. RSA. For two response matrices R ∈ R n×m from each layer of models and each cortical region, where n is the number of units/neurons and m is the number of stimuli, we calculate the representational similarity between the responses to each pair of image stimuli using the Pearson correlation coefficient r, yielding two representational dissimilarity matrices (RDM ∈ R m×m , where each element is the correlation distance 1 − r).\nThen, the Spearman rank correlation coefficient between the flattened upper triangles of these two matrices is the metric score. Regression-Based Encoding Method. Firstly, we run truncated singular value decomposition (TSVD) to reduce the feature dimension of model layers to 40. Secondly, the features after dimensionality reduction are fitted to the representations of each neuron by ridge regression.\nFinally, we compute the Pearson correlation coefficient between the predicted and ground-truth representations of each neuron and take the mean of all correlation coefficients as the metric score. More specifically, we apply leave-one-out crossvalidation to obtain predicted representations of each neuron.\nFor simplicity, we name this method 'TSVD-Reg'. SVCCA. For both the responses of model layers and cortical regions, we use TSVD to reduce the dimension of unit/neuron to 40, yielding two reduced representation matrices. Then we apply canonical correlation analysis (CCA) to these two matrices to obtain a vector of correlation coefficients (the length of the vector is 40).\nThe metric score is the mean of the vector. Because of the invariance of CCA to affine transformations , in this procedure, we only need to ensure that the stimulus dimension is consistent and aligned, even if the unit/neuron dimension is different. Dimensionality reduction plays an important role in this method to make the number of model features comparable to the number of neurons in cortical regions, since the former usually far exceeds the latter.\nIn addition, dimensionality reduction helps to determine which features are important to the original data, while CCA suffers in important feature detection. Using just CCA performs badly, which has been proven by . To check how similar the models are to the visual cortex's mechanisms in visual processing, we rank the final similarity scores of all models and conduct comparisons among three types of models (CNNs, SNNs, and vision transformers).\nSpecially, we focus on comparing SNN (SEW ResNet) and CNN (ResNet) with the same depth and almost the same architectures (Figure ). The final similarity score of a model is the average similarity score across all cortical regions. (The overall rankings can be found in Appendix B and the comparisons among three types of models are shown in Appendix C.)\nAllen brain mouse dataset. No single model achieves the highest final similarity scores with all three metrics. For a fair comparison, we apply the paired t-test to SEW ResNet and ResNet with the same depth. For all three metrics, SEW ResNet performs better than ResNet by a large margin (t = 5.857, p = 0.004; t = 7.666, p = 0.002; t = 7.592, p = 0.002) 1 . 1 The results of the three similarity metrics are separated by semicolons, in the order of SVCCA, TSVD-Reg, and RSA.\nOther Macaque-Face dataset. For both SVCCA and TSVD-Reg, Wide-SEW-ResNet14 and Wide-SEW-ResNet8 achieve the first and second highest final similarity scores respectively. But for RSA, TNT-S and Inception-ResNet-V2 take their place and outperform other models by a large margin. As for SEW ResNet and ResNet, the former performs significantly better than the latter for both SVCCA and TSVD-Reg (t = 8.195, p = 0.001; t = 7.528, p = 0.002).\nHowever, the difference is not significant for RSA (t = 1.117, p = 0.327). Specifically, the similarity score of SEW ResNet152 is only slightly higher than that of ResNet152, and at the depth of 50 and 101, SEW ResNet's scores are lower than ResNet's. Macaque-Synthetic dataset. Similar to the results of Allen Brain dataset, no model performs best for all three metrics.\nSEW ResNet performs moderately better than ResNet (t = 3.354, p = 0.028; t = 3.824, p = 0.019; t = 2.343, p = 0.079). The only contrary is that SEW ResNet18 performs worse than ResNet18 for RSA. Further, to check the details of comparison between the SNNs and their CNN counterparts, we analyze the trajectories of similarity score across model layers (Figure ).\nAs for ResNet and SEW ResNet with the same depth, the trends of their similarities across model layers are almost the same, but the former's trajectory is generally below the latter's. In other words, the similarity scores of SEW ResNet are higher than those of ResNet at almost all layers. Taken together, the results suggest that when the overall results that appear below also correspond to the three metrics in this order, unless the correspondence is stated in the text.\narchitectures and depth are the same, SNNs with spiking neurons perform consistently better than their counterparts of CNNs with an average increase of 6.6%. Besides, SEW ResNet14 also outperforms the brain-like recurrent CNN, CORnet-S, with the same number of layers (see more details in Appendix B). Two properties of SNNs might contribute to the higher similarity scores.\nOn the one hand, IF neurons are the basic neurons of spiking neural networks. The IF neuron uses several differential equations to roughly approximate the membrane potential dynamics of biological neurons, which provides a more biologically plausible spike mechanism for the network. On the other hand, the spiking neural network is able to capture the temporal features by incorporating both time and binary signals, just like the biological visual system during information processing.\nTo figure out the distinctions in the functional hierarchy between macaques and mice, for each cortical region, we obtain the normalized depth of the layer that achieves the highest similarity score in each model. Then, we divide models (excluding vision transformers) into two groups based on their depths and conduct investigations on these two groups separately.\nA nonparametric ANOVA is applied to each group for testing whether layer depths change significantly across cortical regions. For mouse visual cortex (Figure (a)), taking the deep model group as an example, ANOVA shows overall significant changes in depth across cortical regions for TSVD-Reg and RSA (Friedman's χ 2 = 49.169,\np = 2.0 × 10 −9 ; χ 2 = 19.455, p = 0.002). But there is no significant change for SVCCA (χ 2 = 8.689, p = 0.122). According to these results, the differences in depth across regions are indeterminacy and irregular. Meanwhile, the trends of layer depth between some regions contradict the hierarchy observed in physiological experiments of mice (those between VISp and VISrl for TSVD-Reg and between VISal and VISpm for RSA).\nHowever, for macaque visual cortex (Figure (b)), there are significant differences (t = −5.451, p = 6.5 × 10 −6 ; t = −8.312, p = 2.8 × 10 −9 ; t = −3.782, p = 6.9 × 10 −4 , also taking the deep model group as an example) between V4 and IT, and the trend is consistent with the information processing hierarchy in primate visual cortex.\nThe comparative analyses of the best layer depths of the shallow and deep model groups also exhibit the differences between macaques and mice. For mouse visual cortex, the best layer depths of shallow models are significantly higher than those of deep models. Compared to deep models, most shallow models achieve the top similarity scores in intermediate and even later layers.\nDifferently, for macaque visual cortex, the depth of models has little effect on the depth of the most similar layer. What's more, we find that the most similar layer of mouse visual cortex always occurs after the 28 × 28 feature map is downsampled to 14 × 14, which leads to the layer depths' difference between shallow and deep models.\nNevertheless, the best layer of macaque IT appears in the last part of networks, where the feature map has been downsampled more times. In summary, our results might reveal two distinctions in the functional hierarchy between macaques and mice. First, there is a distinct functional hierarchical structure of macaque ventral visual pathway, while there might be no clear sequential functional hierarchy in mouse visual cortex.\nOne explanation is that the mouse visual cortex is organized into a parallel structure and the function of mouse cortical regions are more generalized and homogeneous than those of macaques. Another possibility would be that even though the sequential relations exist among mouse cortical regions as proposed in anatomical and physiological work, they are too weak for the current deep neural networks to capture.\nAdditionally, mice perform more complex visual tasks than expected with a limited brain capacity . Consequently, the neural responses of mouse visual cortex may contain more information not related to object recognition that neural networks focus on. Secondly, it is well known that the units in the neural networks get larger receptive fields after downsampling, and through the analyses of differences between two groups of models based on depth, we find the feature map of the best layer for mouse is downsampled fewer times than that for macaque.\nBased on these results, we provide computational evidence that the increased ratio of the receptive field size in cortical regions across the mouse visual pathway is smaller than those across the macaque visual pathways, which echoes some physio- Macaque-Face dataset --- Table : The correlation between the similarity scores and the number of parameters.\nr is Spearman's rank correlation coefficient. \"-\" indicates that there is no significant correlation. To explore the processing mechanisms in the visual cortex of macaques and mice, we investigate the model properties from the whole to the details. As shown in Table and 2, we first measure the correlation between the similarity scores and the sizes (i.e. the number of trainable parameters and the depth) of network models.\nFor Allen Brain mouse dataset, there are significant negative correlations between the similarity scores and the number of parameters for three metrics while there is no correlation with the depth. Conversely, for the two macaque neural datasets, the similarity scores are highly correlated with the depth of networks, but not with the number of parameters.\nSpecifically, there is a positive correlation for Macaque-Face dataset while a negative correlation for Macaque-Synthetic dataset. (We also apply the linear regression to analyze the correlation between the similarity scores and the model size. The results are consistent with Spearman's rank correlation and are shown in Appendix E).\nBased on these results, we further investigate more detailed properties of neural networks to explain the processing mechanisms in the visual cortex. For the mouse dataset, on the one hand, the best layer depths show non-significant changes across the mouse cortical regions as mentioned in the previous section.\nOn the other hand, the similarity scores of the mouse dataset are only correlated with the number of model parameters but not with the depth of models. It calls into the question whether any detailed structures in the neural networks help to reduce the number of parameters and improve its similarity to mouse visual cortex.\nTherefore, we explore the commonalities between models that have the top 20% representation similarities (see Appendix D) for Allen Brain dataset. As expected, the top models contain similar structures, such as fire module, inception module, and depthwise separable convolution. All these structures essentially process information through multiple branches/channels and then integrate the features from each branch.\nThe models with this type of structure outperform other models (t = 2.411, p = 0.024; t = 3.030, p = 0.007; t = 1.174, p = 0.247). Moreover, we apply the depthwise separable convolution to SNNs, which yields a positive effect. The representation similarity of Spiking-MobileNet is higher than SEW-ResNet50 with a similar depth (+0.8%; +3.9%; +12.1%).\nIn fact, some studies using multiple pathways simulate the functions of mouse visual cortex to some extent . Our results further suggest that not only the mouse visual cortex might be an organization of parallel structures, but also there are extensive parallel information processing streams between each pair of cortical regions .\nFor the two macaque datasets with different stimuli, not only are the model rankings significantly different, but also the correlations between the similarity scores and the model depth are totally opposite. These results corroborate the following two processing mechanisms in macaques: the ventral visual stream of primate visual cortex possesses canonical coding principles at different stages; the brain exhibits a high degree of functional specialization, such as the visual recognition of faces and other objects, which is reflected in the different neural responses of the corresponding region (although the face patch AM is a sub-network of IT, they differ in the neural representations).\nBesides, as shown in Figure , The calculation and plotting of the trajectories are the same as Figure . the similarity scores of vision transformers reach the maximum in the early layers and then decrease. Differently, the scores of CNNs and SNNs keep trending upwards, reaching the maximum in almost the last layer.\nOn the other hand, Appendix C shows that vision transformers perform well in Macaque-Face dataset but poorly in Macaque-Synthetic dataset. Considering the features extraction mechanism of vision transformers, it divides the image into several patches and encodes each patch as well as their internal relation by self-attention.\nThis mechanism is effective for face images that are full of useful information. However, the synthetic image consists of a central target object and a naturalistic background. When vision transformers are fed with this type of stimuli, premature integration of global information can lead to model representations containing noise from the unrelated background.\nWhat's more, when we take all models with the top 20% representation similarities as a whole for analyses, as described in the above paragraph, the properties that enable networks to achieve higher neural similarity are not yet clear. Taken together, the computational mechanism of the better models may reveal core processing divergence to different types of stimuli in the visual cortex.\nIn this work, we take large-scale neural representation similarity experiments as a basis, aided by analyses of the similarities across models and the visual cortical regions. Compared to other work, we introduce SNNs in the similarity analyses with biological neural responses for the first time, showing that SNNs achieve higher similarity scores than CNNs that have the same depth and almost the same architectures.\nAs analyzed in Section 3.1, two properties of SNNs might serve as the explanations for their high similarity scores. The subsequent analyses of the models' simulation performance and structures indicate significant differences in functional hierarchies between macaque and mouse visual cortex. As for macaques, we observed a clear sequential hi-erarchy.\nHowever, as for mouse visual cortex, some work ) exhibits that the trend of the model feature complexity roughly matches the processing hierarchy, but other work suggests that the cortex ) is organized into a parallel structure. Our results are more supportive of the latter. Furthermore, we provide computational evidence not only that the increased ratio of the receptive field size in cortical regions across the mouse visual pathway is smaller than those across the macaque visual pathway, but also that there may be multiple pathways with parallel processing streams between mouse cortical regions.\nOur results also clearly reveal that the processing mechanisms of macaque visual cortex differ to various stimuli. These findings provide us with new insights into the visual processing mechanisms of macaque and mouse, which are the two species that dominate the research of biological vision systems and differ considerably from each other.\nCompared to CNNs, the study of task-driven deep SNNs is just in its initial state. Although we demonstrate that SNNs outperform their counterparts of CNNs, SNNs exhibit similar properties as CNNs in the further analyses. In this work, we only build several new SNNs by taking the hints from the biological visual hierarchy, while many well-established structures and learning algorithms in CNNs have not been applied to SNNs yet.\nIn addition, the neural datasets used in our experiments are all collected under static image stimuli, lacking rich dynamic information to some certain, which may not fully exploit the properties of SNNs. Given that SNNs perform well in the current experiments, we hope to explore more potential of SNNs in future work.\nIn conclusion, as more biologically plausible neural networks, SNNs may serve as a shortcut to explore the biological visual cortex. With studies on various aspects of SNNs, such as model architectures, learning algorithms, processing mechanisms, and neural coding methods, it's highly promising to better explain the sophisticated, complex, and diverse vision systems in the future.\n\nImplementation Details of SNNs Spiking Neuron Model\n\nFor all SNNs, we use the Integrate-and-Fire (IF) model as the spiking neuron model, which acts as the activation layer in neural networks. As mentioned in , V t , X t and S t denote the state (membrane voltage), input (current) and output (spike) of the spiking neuron model respectively at time-step t, and the dynamics of the IF model can be described as follows:\n(1) (2) (3) While V t is the membrane voltage after the trigger of a spike, H t is also the membrane voltage, but after charging and before a spike firing. Θ(x) is the unit step function, so S t equals 1 when H t is greater than or equal to the threshold voltage V thresh and 0 otherwise. Meanwhile, when a spike fires, V t is reset to V reset .\nHere, we set V thresh = 1 and V reset = 0. In addition, because Θ(x) is non-differentiable at 0, the surrogate gradient method is applied to approximate the derivative function during back-propagation. Here, we use the inverse tangent function as the surrogate gradient function and the derivative function is\n(5) In our experiments on SNNs, we not only use SEW ResNet proposed by ), but also build several new SNNs. On the one hand, we improve the spike-elementwise block in SEW ResNet with new architectures referring to studies on ResNet , as shown in Table . On the other hand, as the multi-branch structures in CNNs increase neural representation similarity to mouse visual cortex, we use depthwise separable convolutions and follow the overall architecture of MobileNetV2 to build the SpikingMobileNet, the basic block of which is shown in Figure .\nOur implementation is based on SpikingJelly , an open-source framework of deep SNN. We use the ImageNet dataset to pre-train the new SNNs. Following the settings for training SEW ResNet , we train the models for 320 epochs on 8 GPUs (NVIDIA V100), using SGD with a mini-batch size of 32. The momentum is 0.9 and the weight decay is 0. The initial learning rate is 0.1 and we decay it with a cosine annealing, where the maximum number of iterations is the same as the number of epochs.\nFor all SNNs, we set the simulation duration T = 4.\n\nOverall model rankings\n\nThe results of model rankings are shown in Figure , 8 and 9. We also apply the Spearman's rank correlation to the overall model rankings of different metrics, which is shown in Figure .\n\nScore Comparisons among Model Groups\n\nWe conduct comparisons of similarity scores among CNNs, SNNs, and vision transformers. The results are shown in Figure .\n\nOverall CNN rankings\n\nThe results of CNN rankings are shown in Figure , 13 and 14.\n\nCorrelations between the Model Sizes and the Similarity Scores\n\nThe results of linear regression to model sizes and the similarity scores are shown in Figure , 16 and 17.\n\nThe ImageNet Accuracy and the Similarity Scores\n\nThe results are shown in Figure ."
  },
  {
    "id": "daa4eb9d8b28a987b1c2c049200634cdc510636b19a64ccd",
    "content": "Paper Info\n\nTitle: Force Feedback Control For Dexterous Robotic Hands Using Conditional Postural Synergies\nPublish Date: Unkown\nAuthor List: Dimitrios Dimou, José Santos-Victor, Plinio Moreno\n\nFigure\n\nFig. 1.Example of modeling the contacts and friction during manipulation.\nFig. 2. Schematic representation of the proposed force controller.The input is the state (GRASP or RELEASE) and the force readings.Based on that the grasp size is adjusted by a value C and is given to the posture mapping function along with the desired grasp type.A finger configuration is then generated and commanded to the robot.\nFig. 3. Our control algorithm in Python-like pseudocode.\nFig. 4. Our first experiment.The robot picks up a bottle, transports it, and places down on the desk.In the bottom part of the figure, you can see the control signals during this task.\nFig. 5.The household objects used in our experiments.\nUnder the pictures of the execution you can see the signals recorded by the controller: the average normal force applied by all fingers (blue line), the thresholds f threshold high n .(purple dashed line) and f threshold low n.(yellow dashed line), the average tangential force (green), and the grasp size used in each time-step (red).The task is divided four stages: 1) (red part) the initial grasp of the object, in this stage the force controller closes the grasp until the applied normal\nFig.6.In the upper row of images, you can see our second experiment.The robot picks up the chips can, rotates it 90 degrees, and places back down.In the middle row, for our third experiment, the robot picks up the chips can, rotates it 90 degrees, and hands it over to a person.In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pinch grasp.\n\nabstract\n\nWe present a force feedback controller for a dexterous robotic hand equipped with force sensors on its fingertips. Our controller uses the conditional postural synergies framework to generate the grasp postures, i.e. the finger configuration of the robot, at each time step based on forces measured on the robot's fingertips.\nUsing this framework we are able to control the hand during different grasp types using only one variable, the grasp size, which we define as the distance between the tip of the thumb and the index finger. Instead of controlling the finger limbs independently, our controller generates control signals for all the hand joints in a (lowdimensional) shared space (i.e.\nsynergy space). In addition, our approach is modular, which allows to execute various types of precision grips, by changing the synergy space according to the type of grasp. We show that our controller is able to lift objects of various weights and materials, adjust the grasp configuration during changes in the object's weight, and perform object placements and object handovers.\n\nINTRODUCTION\n\nTo perform complex manipulation tasks in unstructured environments, humans use tactile feedback from their fingers. This feedback is provided by tactile afferents located in the skin of the hand. Particularly, for handling small objects with precise movements, the afferents located in the fingertips are used, which have high density and adapt fast to pressure changes .\nThese afferents provide information about the characteristics of the exerted contact forces, such as the magnitude and the direction. For anthropomorphic robots to be able to perform dexterous tasks similar force feedback signals must be used to alleviate problems arising from uncertainty in measurements, and handle external perturbations.\nFor example, using open-loop position control to lift a heavy object may fail due to slip without any feedback mechanism to provide tactile information. Previous works have used tactile sensors to design force controllers that use slip prediction to update the desired normal forces applied by the fingertips.\nThe slip predictors are based on machine learning models such as neural networks and random forests to classify multi-modal signals from a tactile sensor. In all previous works, each finger was separately controlled by an independent force controller. In addition, they required labeled data to train the slip predictors and because each finger is controlled independently is not obvious how to implement different anthropomorphic grasp types.\nIn this work we develop a force controller that takes as input the force readings of the fingertips and computes the grasp size which is then used along with a grasp type label to generate a grasp posture with the desired characteristics. To avoid slippage the desired normal contact force is calculated to be proportional to the tangential contact forces.\nThe applied normal force is then controlled using the size of the grasp as a control variable. Larger grasp sizes mean less force is applied to the object. So the grasp size is calculated from the error between the desired normal force and the actual measured normal force. The grasp size is then given to the posture sampler that generates a grasp posture, i.e. the finger joint angles.\nThe posture sampler is modeled with a conditional Variational Auto-Encoder (cVAE) based on the framework proposed in . With this framework we abstract away the low-level control of the fingers and generate hand postures based on high-level properties such as the type and the size of the grasp. So it works as a mapping function that takes as input a low-dimensional vector and the grasp type and size as conditional variables and maps them to a set of joint angles.\nWe show that with our controller we can control a dexterous robotic hand to lift objects of different weights using three precision grasps. Our controller is also able to compensate and retain a stable grasp during changes in the objects' weight, for example when filling up a cup or emptying it. In addition we show how with the addition of the hand pose information we can use the controller to calculate if the tangential force is due to gravity or due to a support surface and use this information to perform handovers and place down objects on surfaces.\nWe perform several real-world experiments with a dexterous robotic hand to showcase the capabilities of our controller and support our design choices. To sum up our main contributions are • We develop a controller for a dexterous robotic hand that uses force feedback and the conditional synergies framework to perform dexterous manipulation tasks.\n• We show that with our controller we can easily use different precision grasp types, by changing only the grasp type variable which is given to the grasp posture mapping function. • We demonstrate by incorporating information about the world pose of the hand we can use our controller to perform additional tasks such as placing down and handing over objects.\nRoboticists have looked for inspiration in humans for developing methods for complex object manipulation . Neuroscientists have studied for a long time the processes that allow humans to use tactile feedback to perform complex manipulation tasks. Humans tend to adjust the grip force according to the object's weight, its friction and they use a safety margin to account for uncertainties .\nTo gather information about the tactile states they use multiple afferents that are located in the skin of the fingers . There are different afferents in different parts of the hand depending on their usage, e.g. fast adapting afferents in the fingertips for precise manipulation. Based on signals from these afferents, humans encode simple contact events into action phases, such as grasping, lifting or releasing, which they combine in order to perform more complex and long-horizon manipulation tasks .\nIn robotics tactile sensors have been used for object stabilization and slip prediction in a variety of settings. For example, in , a compliant anthropomorphic prosthetic hand was controlled using force sensing to maintain object stability and avoid slip. In , they develop a control approach that uses integrated force and spatial tactile signals to avoid slip with unknown objects in real world settings.\nIn , , grasp quality metrics are computed based on the tactile feedback from the robots fingertips. In these works, simple two or three fingered grippers were considered for simple grasping tasks. Force control with anthropomorphic robotic hands has also been explored in more recent works. In , they employ three slip prediction methods to estimate when slip starts and based on the force signals at that moment they calculate the friction coefficient value.\nBased on the calculated friction coefficient, they design a force controller that independently controls each finger to achieve a desired normal force. The desired normal contact force is set to be proportional to the tangential contact force and a safety margin based on the evidence found in . In , they train a random forest to classify the contact states into the classes: no contact, contact, slip.\nBased on this classification signal, when slip is detected they increase the desired normal contact force to avoid it. In they train a recurrent neural network to estimate slip and the object material from the readings of a Biotac sensor. The force controller is increasing the desired normal contact force when slip is detected.\nAll these works , , use tactile feedback sensors to predict slip. They collect labeled data, on which they train their models. This approach is based on complex and expensive tactile sensors, and the process of collecting data is cumbersome. In addition, the data do not cover all possible hand poses, which would be impractical.\nIn contrast, in our work we do not rely on slip prediction, we avoid slip by defining a tangential force gain and a safety margin that work for a large number of objects. Furthermore, instead of independently controlling each finger we use a synergistic framework to generate grasp postures, that is conditioned on two variables: the grasp type and the grasp size.\nThis way, instead of controlling the values of each joint of each finger, we control only the two conditional variables greatly simplifying the control pipeline. This also, gives us the ability to use different grasp types in our manipulation tasks by changing only the grasp type variable. In also a synergistic framework was used to prevent an object from slipping from a humanoid hand, but they modeled only one synergy for a tripod grasp and they used the forces on the robotic arm as feedback, while we use force feedback from the fingertips.\nOur control algorithm could also be applied to different hands as it does not depend on the hands configuration. Finally, in previous approaches only lifting tasks had been considered. In our work we demonstrate that our approach can be used to perform more complex tasks, such as placing objects on surfaces and performing handovers, which was not done in previous works.\nOur goal in this work is to design a control algorithm for an anthropomorphic robotic hand to perform dexterous manipulation skills such as lifting and placing down objects. Our control algorithm will use tactile feedback from the force sensors on the fingertips of the hand to decide the forces that need to be applied to the object in each step of the task.\nGiven the desired forces to be applied, the size of the grasp will be computed. Given the grasp size and a desired grasp type, the posture generator will generate a grasp posture, i.e. the hand configuration, such that the force constraints are satisfied. To model the contacts and friction we use Coulombs' law, which states that in order to avoid slip, the normal contact force f n to the contact surface of an object, times the fiction coefficient µ, has to be larger than the tangential force f t :\nµf n ≥ f t You can see an example in Figure , where an object is pressed against a wall by an applied normal force f n , and we have the tangential force f t = mg due to gravity. In order for the object to remain stable we need to apply a normal force: where µ is the friction coefficient between the object and the wall.\nIn the case of a dexterous hand manipulating an object, we want the normal forces applied by all fingers to be greater than the tangential force divided by the friction coefficient of the materials of the object and the fingertip. Since it is hard to accurately compute the friction coefficient between all possible object materials previous works have used multi-modal tactile sensors like the BioTac sensor, which provides information about the pressure, skin deformation, and temperature, to predict slip and based on that signal to increase the applied normal force.\nIn our work we use the FTS3 sensors which is a low-cost sensor that measures the 3D force applied in each fingertip. In addition, previous works gathered labeled datasets in order to train their slip prediction models which is time-consuming and limits the possible orientations of the hand, because gathering labeled data for all possible orientations is impractical.\nTo overcome this we experimentally selected the parameters that determine the value of the applied normal force such that we avoid slip for all objects in our dataset, from the lightest to the heaviest. In order to guarantee contact between the fingertip and the object, in the beginning of the grasping phase, we use an offset f of f set n as the minimum normal force applied by each finger.\nIn they also suggest that humans use an additional safety margin which is proportional to the tangential force, f margin n ∝ f t . So the final desired normal contact force becomes: where G is the gain that includes the friction coefficient and the additional safety margin. To alleviate the effects of noise in the sensors, the running average of the measured normal force f n and tangential force f t is used, as a low pass filter.\nSo for each force measurement we have the following relation: where α ∈ (0, 1) is a parameter that determines how much new measurements affect the value, and is experimentally selected. Given the measured normal force f n from the fingertip sensors we can compute the error f err n = f des n − f n . We use this error signal to control the grasp size variable g size , that we use as a conditional variable in our posture mapping function.\nThe grasp size represents the distance between the thumb and the index finger in a grasp posture. So a smaller grasp size will result in a tighter grasp and greater normal force applied to the surface of the object. We use a linear controller for the grasp size variable that is implemented as follows: where K is a parameter that controls the rate of decrease of the grasp size, and is experimentally selected.\nSo when the error between the desired normal force and the actual normal force is large the grasp size decreases so tighter grasp postures are generated in order to apply more normal force. In practice, in order to avoid oscillations in the grasp size we use the desired normal force as a high threshold that we want the measured normal force to be below:\nIf the normal force is below that threshold the grasp size does not change even if there are small oscillations in the measured tangential and normal forces. Also, in order to avoid the hand applying too much force that damages the hardware or the object we use a low threshold, that is: where w threshold is the width of the threshold in mN .\nIf the measured normal force is below the grasp size increases in order to apply less force. So the final grasp size variable for grasping is calculated as follows: where This is similar to the deadband control method , where instead of having a fixed reference point, an operating range is set. If the response is in this range, the controller does not exert any correction.\nIn our case, the operating range changes according to the force signals from the robot's fingertips. The grasp posture mapping function is based on the conditional postural synergies model presented in . It uses a conditional Variational Auto-Encoder model to generate grasps postures conditioned on additional variables such as the grasp size.\nIn this work we augment this model to also generate grasp postures conditioned on the grasp type. The model is trained on a set of labeled grasp samples acquired by teleoperating a robotic hand using a data-glove. Using this model we are able to abstract away the low-level control of each joint of each finger and generate grasps based on more general characteristics such as the type and the size of the grasp.\nIn this way we can control all the fingers jointly by a single value, the grasp size, thus greatly reducing the control parameters. In addition we are able to use the same control algorithm for different precision grasp types, by changing the grasp type conditional variable. Finally, we can modify our controller to release objects instead of grasping them.\nGiven the pose of the hand in the world coordinate frame, which we can acquire from the robotic arm that is attached to, we can use the forward kinematics of the hand to compute the poses of each fingertip. Then using the force readings of each fingertip we can calculate the global direction of the net tangential force.\nIf the angle between the direction of the net tangential force and the direction of gravity is less than 90 degrees, i.e. the net tangential force's direction is towards the ground, we assume that the tangential force is due to gravity pulling the object, so the force controller tries to grasp it. If the angle is more than 90 degrees, i.e. the net tangential force's direction is upward, it means that something is pushing (or pulling) the object upward, in which case we assume that the object is touching on a support surface or someone is pulling the object so the controller increases the grasp size given to the posture mapping function proportionally to the normal force measured thus slowly releasing the object.\nOpening the grasp is done by controlling the grasp size variable as follows: That way we can place objects on surfaces but also perform robot to human handovers, where the robot holds the object and the human grasps the object and slightly pushes or pulls it up, signaling to the robot that there is a support surface.\nThe robot then slowly releases the object by opening its grasp. We showcase these scenarios in the experiments' section. Based on these observations, we present our force controller in Figure . The hand starts in an open pre-grasp position, a latent point is sampled from the prior distribution of the posture mapping function, and given the desired grasp type and the grasp size a grasp posture, i.e. the joint angles of the fingers, is sampled.\nThe initial grasp size is set to the maximum value, and when the force controller comes into effect and depending on the state of the system and the forces on the fingertips grasp size changes by some value C, according to equations 1,2, until the desired normal force is achieved. To choose between grasping or releasing an object we use a finite state machine formulation.\nWhen the hand reaches the desired grasp pose, which we assume is provided, the GRASP state is activated, in which the controller tries to grasp the object. When the controller detects that the tangential force applied to the object is coming from a support surface the state changes to the RELEASE state, in which the controller releases the object by opening the grasp.\nYou can see the full algorithm in Python-like pseudocode in Figure . To summarize, the advantages of our controller compared with previous approaches are threefold: 1) instead of controlling each joint of each finger of the hand we use only two variables, the grasp size and the grasp type, which allows us to perform multiple grasp types by changing only one variable while the grasp size variable is common among all grasp types, that greatly reduces the complexity of the control process compared to independently controlling a 21 DoF hand to perform different grasp types, 2) we do not rely on slip prediction for controlling the desired normal force, which involves gathering labeled data and works only for the hand poses in the training dataset, and 3) we can use our controller to also release objects instead of only grasping them.\n\nExperimental Set-up.\n\nFor our experiments we used the Seed Robotics RH8D Hand , which is a robotic hand with 7 DoFs. The hand is equipped with the FTS-3 force sensors in each fingertip, which are high resolution tactile sensors that provide the 3D force applied in each fingertip. The sensor provides data at a rate of 50Hz. For the experiments the hand was mounted on a Kinova Gen3 7DoF robot.\nTo train the posture mapping function we used the CyberGlove to teleoperate the hand and collect 468 grasps belonging to three precision grasp  types: tripod, pinch, lateral tripod. The architecture of the cVAE model was the same as in , with the addition of the grasp type as a conditional variable, which was one-hot encoded.\nWe used 10 household objects shown in Figure . With the heaviest object weighing 380g and the lightest 1g. During the experiments the trajectories of the arm were prerecorded, while the hand was controlled online by our control algorithm.\n\nParameter tuning.\n\nTo select the values of the parameters in our controllers we conducted preliminary experiments where we tested lifting and releasing several objects, with different physical properties. To select the value of the normal offset force f of f set n , we used an empty plastic cup as our test object, and we choose a value such that the fingers do not deform the cup.\nThe final value of the parameter was set to -50 mN. To select the values of the gain G and the rate of decrease K, of the grasp size, we experimented with the heaviest object in our dataset, which is the mustard bottle and weighs 380g. The gain G was set to 2.0 such that the desired normal force would be enough to hold the object.\nThe rate of change of the grasp size was set to 100.0, based on the operating frequency of the force sensor and the range of values of the tangential force. For the tangential force averaging process we used a parameter value of α t = 0.7, because we want the controller to be sensitive to fast changes in its value, that can arise for example during lifting an object.\nFor the normal force averaging process we used a parameter value of α n = 0.5, as we do not want it to be affected by noise that could make the controller overconfident.\n\nExperiments.\n\nTo explore the capabilities of our controller, we demonstrate five experiments of increasing complexity: 1) we picked and placed a bottle using a tripod grasp, 2) we picked, rotated and placed a chips can on a box using a tripod grasp, 3) we picked, rotated and handed over the chips can to a person using a tripod grasp, 4) we picked, rotated and handed over a brown foam brick to a person using a pinch grasp, 5) a person handed over a plastic cup to the robot, filled it with coins to increase its weight, and the robot then handed it back to the person using a tripod grasp.\nYou can see the execution of the first experiment in  In the middle row, for our third experiment, the robot picks up the chips can, rotates it 90 degrees, and hands it over to a person. In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pinch grasp.\nFig. . In our fifth experiment, a person hands over an empty plastic cup to the robot, throws coins in it to increase its weight while the robot adjusts its grip to stabilize the object, and then hand overs the cup back to the person. force is below the offset f of f set n , 2) (green part) the robot lifts the object, as it tries to lift the tangential force increases, increasing the threshold, so the grasp size decreases to apply more normal force, 3) (orange part) the robot transports the object, you can see, in point A in the Figure, a perturbation in the tangential force when the robot begins to move, the controller responds by decreasing the grasp thus stabilizing the object, and 4) (blue part) the robot enters the releasing phase, where it lowers the arm until it detects that the tangential force is due to a support surface, then it stops lowering the arm and increases the grasp size slowly releasing the object.\nIn point B in the Figure, you can see that there is noise in the tangential force, due to the arm moving to place the object on the table, that is also reflected in the desired normal force. Because we use the desired normal force as a threshold and not as a reference signal this noise is not manifested in the control of the grasp size.\nYou can see the execution of the second experiment in the upper part of Figure . This experiment demonstrates the ability of the controller to handle arbitrary hand poses. The experiment is divided in four parts: 1) the robot enters the GRASP phase and the force controller generates grasps to achieve a normal contact force below the f of f set n threshold, 2) the robot lifts the object and adjusts the grasp size to avoid the object falling, 3) the hand rotates to place the chips can on the horizontal position, and 4) the robot enters the RELEASE phase, and the arm lowers until the object touches the box, when the hand detects the supporting surface, it starts to slowly release the object.\nYou can see the execution of the third experiment in the middle part of Figure . This experiment demonstrates the ability of the controller to perform robot to human handovers. The experiment is divided in four parts: 1) the robot enters the GRASP phase and the force controller generates grasps to achieve a normal contact force below the f of f set n threshold, 2) the robot lifts the object and adjusts the grasp size to avoid the object falling, 3) the hand rotates to place the chips can on the vertical position, and 4) the robot enters the RELEASE phase, the arm stays still, the human grasps the object from the bottom and slightly pushes it up, the hand then detects that there is a supporting surface and starts to slowly release the object.\nYou can see the execution of the fourth experiment in the bottom part of Figure . This experiment is similar to previous one, but the grasp type that the robot uses is a pinch grasp, that involves only the thumb and the index finger. To perform this we only had to alter the grasp type conditional variable that was given to the posture mapping function.\nYou can see the execution of the fifth experiment in the bottom part of Figure . In the first part (blue) of the experiment the robot closes its grasp, by reducing the grasp size, until the normal force is below the force offset. In the next three parts (pink, green, red) the person throws coins in the cup to increase its weight.\nYou can see in the signal plots that each time coins are added the tangential force decreases so the normal force threshold decreases too. The grasp sizes then decreases as well in order to apply more normal force. This experiment demonstrates the ability of the controller to handle perturbations in the weight of the object during grasping.\n\nCONCLUSION\n\nIn summary, we presented a controller that uses force feedback integrated with conditional synergies to control a dexterous robotic hand to grasp and release objects. We demonstrated that our controller can lift objects of different weights and materials while avoiding slip, react online when the weight of the object changes, place them down on surfaces, and hand them over to humans.\nIn addition, the control architecture is modular, so the synergy grasp mapping component can be easily changed in order to control several precision grasp types. However, our experiments also revealed various limitations of our controller. For example our method fails to stabilize the object when rotational slip occurs.\nIn addition hardware limitations such as, slow update rates and noise in the force measurements can create problems that result in the object falling. In future work we plan to incorporate additional sensing modalities, such as vision to alleviate some of these issues."
  },
  {
    "id": "3c8e9fef2eae8f49aae38b7314a3425b99c3ca8b051e8293",
    "content": "What is this game all about? (short version) Do you like the board game RISK®? Then chances are you’ll like QONQR. Your job is to join a faction and help your faction (team) take over the world. QONQR is an artificial intelligence that appeared on the Internet. We don’t know where it came from, or its purpose, but we know it is powerful. The Legion want to destroy QONQR because they believe it will enslave and exterminate humanity. The Swarm believe QONQR will advance the human race, and we should protect it. The Faceless don’t care, and want to steal the technology for their own uses. Pick a side, recruit your friends, and help your faction capture the towns and cities in which you live, work, and play. What is this game all about? (long version) Right now an invisible war is raging all around you. At stake: the Past, Present, and Future. A rogue Artificial Intelligence has been detected infiltrating the world’s networked infrastructure. Initial hacking of the source code has revealed incredible new technology. It is not certain whether this AI seeks to enhance or destroy humanity. It is only certain that it is here, and that is has a name: QONQR.Those who first detected the presence of QONQR on the global networks have argued fiercely over its intentions. They have split into viciously rival Factions, each co-opting the known QONQR technologies for their own ends. Even now the Factions battle over the entire globe seeking to gather resources and prepare for the coming power struggle. Whether you accept it or not, the war is here. Your survival, prosperity, and even glory depend on the choices you make and the skill you demonstrate from this point forward. You will be asked to join one of three Factions. The choice should not be made lightly. Your Faction Alignment will define your place in the war over QONQR. THE LEGION unite under the shared goals of destroying QONQR and saving humanity by crushing the nascent AI before it can mature. They are led by AGENT SUNDAY, a former commander of the NSA's Turing Task Force which has been valiantly stamping out dangerous AIs for years. THE SWARM are convinced that QONQR promises an era of unprecedented technological advancement and human prosperity. Nanobot weaponry expert KIMYO NAGUMO leads this faction in the battle to defend QONQR and assemble its future tech, accelerating humanity's path into the future. THE FACELESS are a loosely organized faction of militant hackers who want QONQR's technology for their own ends, but want to prevent the unavoidable nightmare of human slavery they believe it portends. When they choose to communicate, they do so through an anonymous vigilante who goes by the name PROMETHEUS. . What do I do first? Create a base, then launch. Launch nanobots until your fingers hurt. How do I create a base? On iOS there is a Base icon in the menu bar. For Windows Phone, you will find a bases button on the home screen. These take you to the Base Screen where you can see how many bases you have available to you. Once you create a base, be sure harvest often by returning to the list of your bases. You can see how full each base is by checking the fill percentage icon. Bases stop collecting once they are full. What is the point of creating bases and harvesting resources? You need bases to earn money. Bases collect rare elements over time which you can then harvest for your faction in exchange for qredits. Qredits can be used to purchase ordinance (like nanomissiles) and upgrades which will help you capture and hold battle zones more easily. What do you mean, “launch nanobots?” Nanobots are the invisible soldiers generated by your device (which has been transformed into “Scope” by advanced QONQR technology). Nanobots fight for control of the battle zones around you.. From the home screen click “Current Zone”. Once you have selected your zone, you will be able to deploy nanobots there.. Initially, you are just a level 1 recruit. You are only going to get a small attack formation with a limited range.. Other solders have to practice with rifles before they get tanks; you are no different. Once you prove your mettle, you’ll get access to bigger weapons. Soon you’ll be lobbing missiles hundreds of miles. How do I capture a zone? If you play for the Legion and are launching nanobots into a zone controlled by the Swarm, you will capture the zone for the Legion as soon as you have destroyed enough of the enemy that your nonbots outnumber theirs. If you are the person who causes the zone to change control to the Legion, you will be listed as the Capturer of the Zone. The Person with the most nanobots in the zone is the zone leader. What is my current zone? How do you know? Your current zone is determined by your proximity to the nearest zone center. So, while you might be inside the governmental boundaries of a city, your scope (phone) could tell you your current zone is a different city, if that city’s center is closer. So I just keep deploying? Yes, in the early levels of the game, just keep deploying and harvesting your bases. You will earn XP (experience points) proving your loyalty to your Faction.\tYou will level up quickly and soon have access to many more options. So all I can do is just attack? You only get assault bots to start. They are the most basic type of nanobot formation. As you level up you will get many more options, including bots that are good at defense, energy attacks, long-range deployments, formations that will buff the shields for all your faction-mates in the zone, and many more. My faction already controls this zone should I still attack? Yes, assault bots won’t attack your friends. You will increase the bot counts for that zone, which will deter opponents. Attack bots can defend your zone; they just aren’t very good at it. As you level up you will unlock defensive formations that are better for deploying if your faction already holds the zone. I’ll never knock out my enemy at this rate! In the first few levels your impact might feel minimal, but every deployment helps you gain experience. It won’t take long to level up if you keep at it. If you are unlucky enough to be in a zone where someone has already built up huge defenses, you may be in for a long fight. But remember, your scope moves with you. Go explore the world and find softer targets. Once you level up, it won’t seem like a toy gun against a battleship. You’ll get your big weapons once you prove yourself. We have already seen operatives brag about taking down 1,000,000 nanobots in just a couple days. Nothing is impossible. How do I attack a different zone? As you level up you will unlock more and stronger formations. Those new formations will have range. While at the early levels you can only attack nearby zones, as you level up your attacks will go 10-20 miles (roughly 15-30 Km) and you will eventually gain access to nanomissiles that can go hundreds of miles. What should I buy in the depot first? The smallest thing to buy is a refresh. We give you some of these as you level up so you can try them out. Refreshes will refill (or partially fill depending on the size of your tank) your bot bar or your energy bar. But after that, it depends on your goals. There is much to choose from. Do you want to be able to deploy more nanobots on every launch? Do you want to boost your offensive or defensive bots? Do you want to be able to launch missiles into towns far away? All of these things are possible. Look through the depot and see what you like. Most of the time you will need to buy an upgrade before you can buy the ordinance. For example, missiles are fairly inexpensive, but you need to buy the MX Rack Missile Launcher before you can launch them. Buy the upgrade first. What is the difference between qredits and cubes in the depot? A qredit (aka: credit, which looks like the child of a Q and € ) is the type of currency you earn in the game by harvesting your bases. Cubes (aka: power cubes) are purchased with real money in the Bank section of the Depot. We want everyone to be able to do everything in the game for free, by earning qredits, but for those who want to move a bit faster, you can purchase cubes to speed things along. Purchasing cubes is how QONQR makes money. We very much appreciate your support. Every purchase you make helps us to keep making improvements in the game. Future enhancements will enable you to earn cubes in game. Why can’t I create another base? Additional bases become available as you level up. At the start, you will get a new base every 5 levels. If you don’t have any more bases available to build, you will need to level up. If you have a base available, but aren’t allowed to use it, it is because you already have a base in that zone. You can only have one base in a zone. Get yourself to another zone, then create your base there. My bases are collecting credits at different rates. Your bases collect resources faster if your faction controls the zone. Do your best to either put your bases in zones you can control by yourself, or find zones with strong players in the same faction and put your bases there to maximize your credit collection. The game says I’m in a town that doesn’t exist. QONQR tracks almost 3 million battle zones of varying strategic value in 250 countries. Sometimes those zones include locations that haven’t existed for over 100 years. That’s pretty cool if you ask us. If you find a zone that looks like a duplicate or is just plain wrong, however, let us know on the forums under “Zone Corrections”. How do I move my current position on the map? You might need to take a car, bus, plane or train depending on which zone you are trying to get to, but if you want to move on the map, you need to move in the real world. QONQR is a location-based game, which means you play where you are. However we don't want to make you move to play, we want you to plan when you move. QONQR goes with you as you move through the daily activities in your life. Where do I find the Strategy Guide? Here: http://community.qonqr.com/index.php?/topic/1191-the-official-qonqr-strategy-guide/ How do I win? That is for you to decide. There is still much to discover. We don’t even know if QONQR is good or evil. Why is it here? What is its purpose? Help your faction further its goals and unlock all the secrets of QONQR!\nCongratulations to the Swarm on their overwhelming victory in Atlantis in May 2015 -- taking and retaining all Atlantis zones from beginning to end is hard to argue with -- most convincing -- well done Swarm!\nRumor has it that the Duggers have 20 scopes. Some families really do have a wife and ten kids... Ha Anyways.... Multi scoping is generally not encouraged, usually if you mention that you do it on the forums or in groupme your not going to be a very liked person, even within your own faction. What tends to happen is if you start multiscoping then your enemies start doing it, then you get into a war where ever person has 6 phones and nobody is happy. I've seen or heard about situations like this in a lot of different locations.\nGreat job, faceless! It was actually an exciting Atlantis and I prefer it that way. Way to bring your A game.\nYou know what we need in this game? Nano-Nukes!!\nAttention (insert faction here), We, the (insert faction here) are tired of the way you constantly (circle one).. A. Cube rage us B. Bully us with numbers C. Talk mean to us It hurts our feelings because (circle one).. A. We don't cube B. We don't have as many allies C. We have no sense of humor and/or no backbone Please refrain from participating in the above selected actions for above selected reasons so as the game is enjoyable for (insert faction here). Regards, (insert name and faction here) There we go...this should streamline the entire complaint process of the forums. Copy and reuse as needed. You're welcome.\nSilver, you mentioned in a blog that there are levels beyond 150, is it safe to say there is no level limit anymore or is that for us to discover? Another question, I'm not sure if I'm the only one noticing this but it seems like bot decay doesn't work against Zone Assault bots. I've hit two players I know have been inactive for well over 6 months but my attack had the same effect it did yesterday before the update. However I did notice against Deflection bots I am getting 2x the kill power. Also in response to decontaminatoR. Paid gaming isn't just something for adults. When I was 13 the best options for handheld gaming was GameBoy or GameGear and the games at the time cost anywhere between $29-$39 dollars. I had a paper route to pay for my games. So, no offense, but you can afford $0.99 for a game. You don't even need a paper route, just check under your couch cushions and I'm sure you'll find a few quarters.\nI want to start by thanking Faceless. This round of Atlantis, Legion and Faceless were doubled in sized and probably spending by Swarm. I contacted some great players from the other side and put together a nonaggression pact, this pact was one of the most impressive agreements I've seen in the more than two years of playing. Hundreds of people worldwide stuck to this agreement and put past feelings behind us. It was awesome to see both sides stick so closely to each other in fighting against Swarm. I want to really thank everyone who showed honor by standing behind me and the faceless command when we suggested that, the people who really gave it a chance and then most importantly, to all the players who honored it. Faceless, thank you very much! We stood no chance of winning without you! The battle came down to literally one launch in one of three zones. I'd say that with that being said everyone fought incredibly hard, so I want to give Swarm the respect they deserve. You guys really show out and play to win. Good game, 2 against 1 is not easy, no matter how large your crew is. And Legion, we had many late, late nights, many very long days. You guys killed it this month! We didn't take home a trophy, but I would say we all have something to be proud of! The other leaders who helped me coordinate everything were awesome! So many great people kept everything moving forward 24 hours a day for the whole week. Thank you to everyone who gave it your all for the whole week even when we saw that Swarm had 4x our bots at the end of just one day. Many people would have given up, but we held in and **** near won! I've won Atlantis battles with Faceless and with Legion but I will say, nothing was as fun or incredible as this round! You guys are fantastic and I can't wait to do it all again next month! Hopefully we have more Legion and Faceless show up for the next round, but I know that even if we don't, we will figure out a way, just as warriors do. PEW! PEW! PEW!\nI must start with an announcement: Camels are not the only animal in the middle east. You have overused it already. It's saddens me that anyone would still find it funny. Get a bit of originality. For anyone not wanting to read all this drivel, skip to the end (hint: Bold stuff). At what point have I bullied anyone? When was the last aggressive or threatening message you or any of your members received from me? Never. Legion and Swarm (in the UK) have teamed up because Faceless are dominating in London. That makes sense. It's a three sided fight and if one force becomes too powerful, the other two can join forces to try and take them down. But: We are dominating in the London area while your alliance is attacking players outside of that area. Then you expect me to sit back and do nothing. You are specifically using me as an excuse for why you have teamed up but then you're attacking players outside of my reach, sometimes with Europe involved. If that is not reason enough to attack you then I'm not sure what is. You lot cube, multiscope and have multi faction accounts and still complain. I don't complain about anything you do. Play the way you want to, ill do the same. I am never rude to anyone, always polite no matter what the message is, never brag about what I do, can do or have done, never threaten anyone, try and keep in touch with the few swarm or legion who are civilised to me, listen to any message form any side and if I can help in any way, I try my best to. Formed agreements with enemy (that I didn't need to) that ended up slapping me in the face. I have even in the past taken the time to find out who some of the younger players are so I know to avoid them. When I'm in a different country I try and find who the bullies are. Those players who threaten and brag and laugh at others. Those are my targets (if they exist in those areas). I don't see how anyone can think I'm a bully. Is it your money? Its none of your business how I spend it. Either way your numbers are way off. 700 dollars a day? Did you just decide to blindly strike the number pad to come up with that figure? Best part of one of your posts is saying that the devs should worry about my welfare. What concerns you about me spending my money? Maybe I'll self harm due to overspending? I wont have enough to buy food because I bought too many cubes? I don't understand what they are supposed to worry about. I have issues? At what point did you deduce this oh mighty psychologist? Wait a minute are you my bank account manager? What do you know about how much I can or cant afford? Random guy spends money on something he enjoys. The end. Told you last time to give up on all the drama but I guess thank you for the concern. You play to make me spend more? And? What do you think that accomplishes? In fact how do you even make me spend? You don't even attack me. You sulk when I'm in the UK and when I fly out and you find out, you call in Europe to help you take an zone or two. As they say, whatever floats your boat. As for Atlantis: Please try again. I quit Atlantis when we were winning. I have on occasion involved myself after I was asked to help out but on the whole I give it a miss. The last time (months ago) was the last 10 minutes of Atlantis and Legion fought hard. We lost. I was not the only Faceless player to quit Atlantis. Quite a few of us thought it lasted too long and had too many zones to fight over. I hear the duration has been reduced. You can't honestly believe I changed my sleeping pattern for the game. I was in California for a month. I was jet lagged. My sleeping pattern was a bit off when I got back. You're not even accurate about when I deploy. Pay attention. You should know this by now: I play as and when I want, sometimes every 20 minutes and sometimes I do long stretches and sometimes I'm busy and don't deploy for hours. You wont always win, and nor will I. Try and get satisfaction form at least trying to win or putting up a good fight. About the limit on cubing. Please. I suggested that last year. Twice. Unfortunately multiscoping is allowed and so prevalent that it kind of ruins the idea. These are the facts: YOU and YOUR side threatened Faceless members who support London. One of the members threatened is actually London based. What do you expect him to do while his city is under attacked? A few of your members don't know when to keep quiet. They sent messages to us threatening specific players and telling them their zones will be dropped just because they have helped London. Basically if they help London then they get their zones wiped. And you have the cheek to call me a bully? We stood up to your members specifically because they were trying to bully. That is the reason we went strong months ago and took those big zones. How is this me bullying you? This is me answering your threats. I didn't attack those zones \"just because i can\", it's just because I should. Because of your threats. You can blame your members for the loss of those 2 or 3 big zones. These are the basics: You attack one of our zones. We look at the list of attackers and pick one of the players who deployed the most, find a zone of his or hers and attack it. We don't need to justify our attacks with \"because there are Faceless players within 30 miles\". What, every time we attack a zone we need to send some letter explaining why? You attack us or we attack you, for any reason. That's the game. When some of you were cheating and you could not win you complained, when you cube and lose you complain, when you invite all of Europe to attack and win you still complain. I just think you like to make a fuss. Last words (for now): You think I attack zones as a means of getting attention? I get enough of it form your threads. The only attention seeker here is you with your victim attitude and pity us posts.\nI've had some very angry emails today from a couple users who are upset their rival achieved the ability to switch factions freely having played for one of the factions for only 1 hour. I've been accused of doing favors, changing the rules, and various other backhanded deals. It appears it comes down reading the rules. You do not have to play for every faction for 60 days in order to earn free switching status. Here is a common scenario many players have used to achieve free switching status and avoiding playing for one faction they despise. 1. Start with Swarm 2. Switch to Legion (earn Spy) play for 60 days 3. Switch back to Swarm (earn Double Agent) play for 60 days 4. Switch to Faceless (earn Mercenary) immediately switch back to Swarm or Legion Below is the complete text on the switch nanobots screen. It is the same text that has been there from Day 1 with the exception of the level 100 rules that went into place earlier this year, where you could switch as much as you want before level 100 , but those switches don't count towards the medals. This text has been part of this description since Jan 15, 2013. \"Players that earn all three spy awards, may once again switch factions at any time as they could during the training levels 1 through 99.\" Prior to Jan 15, the text said this. \"Players that earn all three awards may be given the opportunity to switch factions more quickly in future updates (contact support for more information)\" I pulled that right out of source control, which includes the entire change history. Here is the complete text from this page. http://portal.qonqr....r/SwitchFaction WARNING: Defection has consequences! Self-destruct will be initiated on all your nanobots. Without the self-destruct, you would be required to battle against your former self to regain control of your zones. You will lose the capture and leadership of any zones you currently hold. Lifetime captures will be unaffected. If you are still completing the training levels and have not reached Level 100, you may switch as often as you like to find the faction that suits you best. Once you have reached Level 100 switching factions has rewards, but also has additional consequences beyond the self-destruct of all your nanobots. Defection will usually result in a demotion in rank. This is accomplished through awards with negative rank points. Those awards are: Spy - First switch to an opposing faction (-20 points) Double Agent - Return to a faction from which you had previously defected (-20 points) Mercenary - Become a member of all three factions (-20 points) Other Faction Change Details: You may not switch factions again until at least 60 days have passed since your last faction switch. Defection point penalties are applied only once per award Players that earn all three spy awards, may once again switch factions at any time as they could during the training levels 1 through 99. The decision to switch factions is one that must be made with strong determination. Nanobots cannot be reanimated once destroyed. You will retain your earned experience, level, formations, qredits, cubes, and upgrades. However, as far as your zones go, you will be starting over.\n...has got to be one of the funniest moments I've seen in qonqr yet lol.\nThe **** change operation was a success!\nAs a relatively new player for faceless in a region dominated by swarm i can understand the OP. However, judging from the numbers i see here on a regular basis i think you are asking a bit much. My idea would be the opposite approach. Why not add a weapon with extremely short range, let's say like 5km that acts like a bomb and make it much stronk? That would add some serious home advantage. Or alternativley make attack formations lose power over range (exclude nanos and plasma). Maybe something like that would allow newcomers to at least get a foothold in their homezones. It's just an idea, maybe i overlooked something?\nThis is by design. Some day it is possible (I said someday) we could offer skins for your scope. So we will need a uniform color scheme. You can tell the formation families based on the shape of the box. Trapezoid is attack, diamond is defense, and octagon is support. It will take some time to get comfortable with the change.\nGeophysical based game. Anyways, probably not a bad idea but, considering the issues with the three platforms and the development of blue for those platforms, I doubt the resources are available for development on a new platform. Seen the blog? Its Qonqr meets wheel of fortune!\nI am happy to announce that today I both completed the training and captured my first zone.\n^THIS so now that qonqr has been thoroughly funded, can we have blue now? Or is that not happening still lol.\nAtleast I am not legion and there for we can have this intelligent discussion rather than just compete over who has the best words XD ohhhhh someone bring the bill to legion cuz someone just served them extra double order of stir fried SNAPPPPPP If the devs wont make zone dueling for us I hope out there somewhere are those who would empty a zone and challenge one on one to a local battle. I'd like to see the transcript of deployments made / moves made as well that would be neat I think such events would be cool. I suppose if people give up on atlantis as it works now they can schedule their own tournements in empty atlantis zones.. have a team clear the zone.. put 1 vs 1 or teams vs teams.. like fisticuffs challenges.. find out what these warriors are really made of!\n@Qonqrd everyone you know must face palm every time you make a post. Its embarrassing. Mega cubers or whatever you want to call them are not great for the opposing team surrounding them but are great for the game itself (money) and for the team they are part of. Multiscopers are not not great for the opposing team surrounding them and bring nothing to the game but are great for the team they are part of. Both have a negative impact on enemy teams/players but only one benefits the game itself. Both can make people want to quit out of frustration. And that's not great for the game. @OP unlimited refresh is over powered. Its frustrating to fight against a ridiculous amount of refreshes. Unfortunately i dont see anything changing unless this game gets a lot more people playing. More people might mean more money for the company from various sources. More money from various players might mean they can limit the players who spend a ton and still generate a healthy income. The main issue i see with limiting refreshes is someone multiscoping and spending money. He now has two, three, four accounts to refresh with and gets the advantage. Its tricky.\ney dun new ho to yet it uff.\nYet you complain almost everyday here, on your website, Twitter, and YouTube channel that the game needs to change because cubing has such an impact.\nIt was fun. Swarm had me scared at first, but it turned into kind of a bullying match between us and legion. Last hour became p obvious which way it was gonna go. Legion rly stepped up their game in the end there, respect.\nWe are investigating this. Here is what we know: Several of the accounts used the same password. Most of the accounts belonged to people who knew each other personally. The accounts were all switched from the same IP Addresses. The person who logged in, got into each account on the first attempt, so they knew the password for each account. What you should know: QONQR never stores passwords, not even in the logs. Passwords are hashed (one way encrypted) and can never be decrypted When you authenticate to our servers, we hash the password you gave us and compare it to the encrypted password in the database to see if they match. Access to our database in the could is restricted tightly and we are confident no one breached the system. What you should do: Don't use the same password as other people you play with. Don't share your password with anyone.\nI heard all the French players fled to the UK after one German player accidentally shot a single missile into France.\nMost factions now use GroupMe or Line as their means of communication, the forums are too slow as a means of communication and insecure for specific faction conversations. Think of the forums are more of a gaming information resource rather than a means of communication. Contact the top players of your faction in the leader boards of your state and they will likely point you in the right direction to chatting with your local faction. The developers are also building some sort of new chat system into the game, we don't know much about it but apparently beta testing for the chat will be happening very soon (next couple of weeks) according to their timelines.\nA way to honor the dead? Nah, how bout a way to dishonor the dead.\nCould just build it up and retain their capture. Remember Bizzy, staq to the heavens.\nI just read this entire thread. I am now tuckered.\nDoes anyone know if Bot Booster has an effect on Seekers and how much dmg they do to attacking players? Also on the topic of seekers, does the amount of skeers in a battlefield have any effect on how much damage they do?\nInteractive map of real-time zone captures.\nYou know that is something I didn't factor in there. Time. The player who can consistently and constantly launch wins against the guy who casually picks up the phone on occasion or has to work away from a cell phone for 8 hours. Good point. And yes I can't argue skill doesn't factor in, it just seems like less of a factor than other games is all.\nI cant see why a closed forum, open only to registered Qonqr accounts, cant be used. **** spammers!\nGotta love synclock! I suspect linjin has a problem. Maybe the dvs should look into it.\nNo, Naamah...I clearly understood what you were trying to convey. I'll even go as far as to agree that what your facing now is, while fully allowed and deemed completely acceptable by the developers, unbalanced and wrong. However..the imbalance isn't in the game and isn't something that, from a business standpoint, is likely to be regulated. The game is fair..the advantages are provided to all players. It's the players themselves that throw the balance into chaos because, as you've said, not everyone can afford spending several thousand bucks a year in only one game. Truthfully, in my opinion, the moment you admitted to buying cubes yourself your complaint became silly..because I'm sure there's a player out there, who has bought NO cubes, who can make this same complaint about you that you are making about others here. I know these things because I've read them so...many...times in this forum. There was likely a time, ages ago when I was a young Massune, that I even posted a few myself. That was the purpose of my post..I was poking fun at addition of yet another cuber/bully/trash talk complaint on the forum. It wasn't directed at your personal plight so much as the idea that someone, yet again, finds it necessary to lobby for a spending cap on the only real way for this game to make money. As to your specific problem..you, like all those that have raised this topic before you, have few options to rectify the issue. Here are a few that seem to have worked for others..fight harder, recruit better, spend less time complaining and more time organizing, budget for more cubes or quit. I'd rather not see you opt for the latter..but to each their own.\nI agree, we should find a way to honor the dead, but I don't think keeping their towers infinitely is necessarily the solution. The game must go on. I'm pretty sure the point of bot decay was to clear the game of inactive player bots so that new players can have a chance to rise up, not to dishonor the bots of dead players.\nThe following are frequently asked questions about the new server update (so far) It still says Training Complete on my iPhone. -\tDownload the update from iTunes It still says Training Complete on my Android. -\tSorry, Android will not be updated again until the QONQR Blue beta is released My XP per launch keeps going down -\tThis is XP throttling and is intended to limit the ability for people to leve1 from 1 to 100 in a single day through heavy cubing. The XP throttle was introduced with the original version of QONQR in 2012,and the throttle formula is the same for levels above 100. The throttle resets at midnight UTC every day. How do I buy the Bot Regeneration Accelerator? -\tCurrently the Bot Regen Accelerator can only be purchased through http://portal.qonqr.com. Go to the Depot and review your scope upgrades. The new QONQR Blue clients will allow for this purchase to be made in the app using your mobile billing. I don’t have a PayPal account -\tFor users interested in purchasing the Regen upgrade, but who do not have a PayPal account, PayPal does give you the option to checkout using your billing information without creating an account. PayPal is not allowed in my country, or I don’t have a credit or debit card -\tPlease contact support@QONQR.com for alternate options Is Bot Regen Accelerator counted as part of the 100% scope upgrades? -\tYes, but there is a bug that does not increase scope upgrade percent when you purchase this upgrade, that will be fixed in the coming days. For all other questions, please read the 7 blog posts prior to 7/29/2015 for information on what was included in the update today.\nBye Fack, its been a pleasure being allied and against you.\nThe two big issues that are both killing the game slowly and keeping it from growing exponentially are cube injustice and new player ramp. The game obviously also needs to provide a consistent and growing revenue stream as well. I think Silver needs to rethink how revenue is generated if he is going to address cube injustice and new player ramp. For revenue generation I would suggest a model that doesn't give a significant combat advantages. Download and play for free from level 0-99 Pay small monthly fee to get full functionality or play for free at 50% of offensive/defensive funtionality Still buy cubes, but cubes are used for following: - Credit Boost: harvest more credits for a period of time - Range Extension: ability to use standard attack/defense formations at extended ranges - Base Share: get 100% credit attainment even in bases owned by another faction - Purchase additional ordinance - Zone Name Change - ability to customize zone names. \"Breggland\" - Faction Change with Bots - pay for the ability to keep up to 50% of your bots with faction change - Experience Boost: %increase in experienced gained while leveling - Other: anything that helps grow a player or provides enjoyment, but doesn't tip the battle capability of a scope. New Player Ramp/Integration into Game - Offer paid immediate ramp package: one price to become 100 with full upgrades - Like the changes in Blue - Create new zones in Metro areas that only 0-99 level can launch into, with statewide ranges Understand that catering to those who have money and like to use it for an advantage is a good business model and for those people it might be ok to offer very expensive options: - Shield generators: temporary energy shield that adds X% increase to defense or stops X% of damage - EMP's: turns Absorbs off for X minutes. Does not destroy, just turns off - Chain Lighting: Does damage across multiple players in a zone From a development standpoint I have no idea what is possible, easy or hard, but the general idea is to make the playing field more fair for the standard player while maintaining and growing a business revenue stream.\nYou need to come to the Northeast US. We handle our business like no other."
  },
  {
    "id": "ed61bdde19a3446389e989c06ab4209f464f9484d42dbd1c",
    "content": "The 1951 Ohio State Buckeyes baseball team represented the Ohio State University in the 1951 NCAA baseball season. The head coach was Marty Karow, serving his 1st year.\n\nThe Buckeyes lost in the College World Series, defeated by the Texas A&M Aggies.\n\nRoster\n\nSchedule \n\n! style=\"\" | Regular Season\n|- valign=\"top\" \n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 1 || March 16 || at  || Unknown • San Antonio, Texas || 15–3 || 1–0 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 2 || March 17 || at B. A. M. C. || Unknown • San Antonio, Texas || 7–8 || 1–1 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 3 || March 19 || at  || Clark Field • Austin, Texas || 0–8 || 1–2 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 4 || March 20 || at Texas || Clark Field • Austin, Texas || 3–4 || 1–3 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 5 || March 21 || at  || Unknown • Houston, Texas || 14–6 || 2–3 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 6 || March 22 || at Rice || Unknown • Houston, Texas || 2–3 || 2–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 7 || March 23 || at  || Unknown • Fort Worth, Texas || 4–2 || 3–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 8 || March 24 || at TCU || Unknown • Fort Worth, Texas || 7–3 || 4–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 9 || March 24 || at  || Unknown • St. Louis, Missouri || 10–4 || 5–4 || 0–0\n|-\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 10 || April 6 ||  || Varsity Diamond • Columbus, Ohio || 2–0 || 6–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 11 || April 7 ||  || Varsity Diamond • Columbus, Ohio || 15–1 || 7–4 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 12 || April 14 ||  || Varsity Diamond • Columbus, Ohio || 0–1 || 7–5 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 13 || April 20 ||  || Varsity Diamond • Columbus, Ohio || 10–9 || 8–5 || 1–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 14 || April 21 || Minnesota || Varsity Diamond • Columbus, Ohio || 7–0 || 9–5 || 2–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 15 || April 24 || at  || Unknown • Oxford, Ohio || 3–4 || 9–6 || 2–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 16 || April 27 || at  || Hyames Field • Kalamazoo, Michigan || 2–3 || 9–7 || 2–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 17 || April 28 || at Western Michigan || Hyames Field • Kalamazoo, Michigan || 5–7 || 9–8 || 2–0\n|-\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 18 || May 1 || at  || Unknown • Athens, Ohio || 7–6 || 10–8 || 2–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 19 || May 4 ||  || Varsity Diamond • Columbus, Ohio || 12–6 || 11–8 || 3–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 20 || May 5 || Purdue || Varsity Diamond • Columbus, Ohio || 14–4 || 12–8 || 4–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 21 || May 8 ||  || Varsity Diamond • Columbus, Ohio || 6–8 || 12–9 || 4–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 22 || May 9 || at Dayton || Unknown • Dayton, Ohio || 11–2 || 13–9 || 4–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 23 || May 12 ||  || Varsity Diamond • Columbus, Ohio || 6–5 || 14–9 || 5–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 24 || May 12 || Indiana || Varsity Diamond • Columbus, Ohio || 5–2 || 15–9 || 6–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 25 || May 15 || Ohio || Varsity Diamond • Columbus, Ohio || 6–0 || 16–9 || 6–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 26 || May 18 || at  || Northwestern Park • Evanston, Illinois || 1–3 || 16–10 || 6–1\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 27 || May 19 || at Northwestern || Northwestern Park • Evanston, Illinois || 10–3 || 17–10 || 7–1\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 28 || May 22 || at Cincinnati || Carson Field • Cincinnati, Ohio || 8–4 || 18–10 || 7–1\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 29 || May 25 ||  || Varsity Diamond • Columbus, Ohio || 4–1 || 19–10 || 8–1\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 30 || May 25 || Michigan || Varsity Diamond • Columbus, Ohio || 3–6 || 19–11 || 8–2\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 31 || May 30 || Miami (OH) || Varsity Diamond • Columbus, Ohio || 3–4 || 19–12 || 8–2\n|-\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 32 || June 1 || at  || Old College Field • East Lansing, Michigan || 8–0 || 20–12 || 9–2\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 33 || June 2 || at Michigan State || Old College Field • East Lansing, Michigan || 9–8 || 21–12 || 10–2\n|-\n\n|-\n|-\n! style=\"\" | Postseason\n|- valign=\"top\"\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 34 || June 8 || Western Michigan || Varsity Diamond • Columbus, Ohio || 1–0 || 22–12 || 10–2\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 35 || June 8 || Western Michigan || Varsity Diamond • Columbus, Ohio || 2–4 || 22–13 || 10–2\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 36 || June 9 || Western Michigan || Varsity Diamond • Columbus, Ohio || 3–2 || 23–13 || 10–2\n|-\n\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 37 || June 13 || Oklahoma || Omaha Municipal Stadium • Omaha, Nebraska || 8–9 || 23–14 || 10–2\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 38 || June 13 || Texas A&M || Omaha Municipal Stadium • Omaha, Nebraska || 2–3 || 23–15 || 10–2\n|-\n\nAwards and honors \nDick Hauck\n First Team All-Big Ten\n\nStewart Hein\n First Team All-Big Ten\n\nReferences \n\nOhio State Buckeyes baseball seasons\nOhio State Buckeyes baseball\nBig Ten Conference baseball champion seasons\nOhio State\nCollege World Series seasons"
  },
  {
    "id": "e02f6a69d7b2a96a3aa6cd84a9189c2d552f6fb089f216e1",
    "content": "Hugh Hilton Goodwin (December 21, 1900 – February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  during the Mariana Islands campaign. Goodwin then served consecutively as Chief of Staff, Carrier Strike Group 6 and as Air Officer, Philippine Sea Frontier and participated in the Philippines campaign in the later part of the War.\n\nFollowing the War, he remained in the Navy and rose to the flag rank and held several important commands including Vice Commander, Military Air Transport Service, Commander, Carrier Division Two and Commander, Naval Air Forces, Continental Air Defense Command.\n\nEarly life and career\n\nHugh H. Goodwin was born on December 21, 1900, in Monroe, Louisiana and attended Monroe High School there (now Neville High School). Following the United States' entry into World War I in April 1917, Goodwin left the school without receiving the diploma in order to see some combat and enlisted the United States Navy on May 7, 1917. He completed basic training and was assigned to the battleship . Goodwin participated in the training of armed guard crews and engine room personnel as the Atlantic Fleet prepared to go to war and in November 1917, he sailed with the rest of Battleship Division 9, bound for Britain to reinforce the Grand Fleet in the North Sea.\n\nAlthough he did not complete the last year of high school, Goodwin was able to earn an appointment to the United States Naval Academy at Annapolis, Maryland in June 1918. While at the academy, he earned a nickname \"Huge\" and among his classmates were several future admirals and generals including: Hyman G. Rickover, Milton E. Miles, Robert E. Blick Jr., Herbert S. Duckworth, Clayton C. Jerome, James P. Riseley, James A. Stuart, Frank Peak Akers, Sherman Clark, Raymond P. Coffman, Delbert S. Cornwell, Frederick J. Eckhoff, Ralph B. DeWitt, John Higgins, Vernon Huber, Albert K. Morehouse, Harold F. Pullen, Michael J. Malanaphy, William S. Parsons, Harold R. Stevens, John P. Whitney, Lyman G. Miller and George J. O'Shea.\n\nGoodwin graduated with Bachelor of Science degree on June 3, 1922, and was commissioned Ensign in the United States Navy. He was subsequently assigned to the battleship  and took part in the voyage to Rio de Janeiro, Brazil, before he was ordered to the Naval Torpedo Station at Newport, Rhode Island for submarine instruction in June 1923. Goodwin completed the training several weeks later and was attached to the submarine . He then continued his further training aboard submarine  and following his promotion to Lieutenant (junior grade) on June 3, 1925, he qualified as submariner.\n\nHe then served aboard submarine  off the coast of California, before he was ordered for the recruiting duty to San Francisco in September 1927. While in this capacity, Goodwin applied for naval aviation training which was ultimately approved and he was ordered to the Naval Air Station Pensacola, Florida in August 1928. Toward the end of the training, he was promoted to lieutenant on December 11, 1928, and upon the completion of the training in January 1929, he was designated Naval aviator.\n\nGoodwin was subsequently attached to the Observation Squadron aboard the aircraft carrier  and participated in the Fleet exercises in the Caribbean. He was transferred to the Bureau of Aeronautics in Washington, D.C. in August 1931 and served consecutively under the architect of naval aviation William A. Moffett and future Chief of Naval Operations Ernest J. King.\n\nIn June 1933, Goodwin was ordered to the Naval War College at Newport, Rhode Island, where he completed junior course in May of the following year. He subsequently joined the crew of aircraft carrier  and served under Captain Arthur B. Cook and took part in the Fleet exercises in the Caribbean and off the East Coast of the United States.\n\nHe was ordered back to the Naval Air Station Pensacola, Florida in June 1936 and was attached to the staff of the Base Commandant, then-Captain Charles A. Blakely. When Blakely was succeeded by William F. Halsey in June 1937, Goodwin remained in Halsey's staff and was promoted to Lieutenant Commander on December 1, 1937. He also completed correspondence course in International law at the Naval War College.\n\nGoodwin was appointed Commanding officer of the Observation Squadron 1 in June 1938 and attached to the battleship  he took part in the patrolling of the Pacific and \nWest Coast of the United States until September 1938, when he assumed command of the Observation Squadron 2 attached to the battleship .\n\nWhen his old superior from Lexington, now Rear Admiral Arthur B. Cook, was appointed Commander Aircraft, Scouting Force in June 1939, he requested Goodwin as his Aide and Flag Secretary. He became Admiral Cook's protégé and after year and half of service in the Pacific, he continued as his Aide and Flag Secretary, when Cook was appointed Commander Aircraft, Atlantic Fleet in November 1940.\n\nWorld War II\n\nFollowing the United States' entry into World War II, Goodwin was promoted to the temporary rank of Commander on January 1, 1942, and assumed duty as advisor to the Argentine Navy. His promotion was made permanent two months later and he returned to the United States in early 1943 for duty as assistant director of Planning in the Bureau of Aeronautics under Rear admiral John S. McCain. While still in Argentina, Goodwin was promoted to the temporary rank of Captain on June 21, 1942.\n\nBy the end of December 1943, Goodwin was ordered to Astoria, Oregon, where he assumed command of newly commissioned escort carrier USS Gambier Bay. He was responsible for the initial training of the crew and was known as a strict disciplinarian, but the crew appreciated the skills he taught them that prepared them for combat. Goodwin insisted that everyone aboard has to do every job right every time and made us fight our ship at her best.\n\nDuring the first half of 1944, Gambier Bay was tasked with ferrying aircraft for repairs and qualified carrier pilots from San Diego to Pearl Harbor, Hawaii, before departed on May 1, 1944, to join Rear admiral Harold B. Sallada's Carrier Support Group 2, staging in the Marshalls for the invasion of the Marianas.\n\nThe air unit, VC-10 Squadron, under Goodwin's command gave close air support to the initial landings of Marines on Saipan on June 15, 1944, destroying enemy gun emplacements, troops, tanks, and trucks. On the 17th, her combat air patrol (CAP) shot down or turned back all but a handful of 47 enemy planes headed for her task group and her gunners shot down two of the three planes that did break through to attack her.\n\nGoodwin's carrier continued in providing of close ground support operations at Tinian during the end of July 1944, then turned her attention to Guam, where she gave identical aid to invading troops until mid-August that year. For his service during the Mariana Islands campaign, Goodwin was decorated with Bronze Star Medal with Combat \"V\".\n\nHe was succeeded by Captain Walter V. R. Vieweg on August 18, 1944, and appointed Chief of Staff, Carrier Division Six under Rear admiral Arthur W. Radford. The Gambier Bay was sunk in the Battle off Samar on October 25, 1944, during the Battle of Leyte Gulf after helping turn back a much larger attacking Japanese surface force.\n\nGoodwin served with Carrier Division Six during the Bonin Islands raids, the naval operations at Palau and took part in the Battle of Leyte Gulf and operations supporting Leyte landings in late 1944. He was later appointed Air Officer of the Philippine Sea Frontier under Rear admiral James L. Kauffman and remained with that command until the end of hostilities. For his service in the later part of World War II, Goodwin was decorated with Legion of Merit with Combat \"V\". He was also entitled to wear two Navy Presidential Unit Citations and Navy Unit Commendation.\n\nPostwar service\n\nFollowing the surrender of Japan, Goodwin assumed command of Light aircraft carrier  on August 24, 1945. The ship was tasked with air missions over Japan became mercy flights over Allied prisoner-of-war camps, dropping food and medicine until the men could be rescued. She was also present at Tokyo Bay for the Japanese surrender on September 2, 1945.\n\nGoodwin returned with San Jacinto to the United States in mid-September 1945 and he was detached in January 1946. He subsequently served in the office of the Chief of Naval Operations until May that year, when he entered the instruction at National War College. Goodwin graduated in June 1947 and served on Secretary's committee for Research on Reorganization. Upon promotion to Rear admiral on April 1, 1949, Goodwin was appointed Chief of Staff and Aide to Commander-in-Chief, Atlantic Fleet under Admiral William H. P. Blandy.\n\nRevolt of the Admirals\n\nIn April 1949, the budget's cuts and proposed reorganization of the United States Armed Forces by the Secretary of Defense Louis A. Johnson launched the wave of discontent between senior commanders in the United States Navy. Johnson proposed the merging of the Marine Corps into the Army, and reduce the Navy to a convoy-escort force.\n\nGoodwin's superior officer, Admiral Blandy was call to testify before the House Committee on Armed Services and his harsh statements for the defense of the Navy, costed him his career. Goodwin shared his views and openly criticized Secretary Johnson for having power concentrated in a single civilian executive, who is an appointee of the Government and not an elected representative of the people. He also criticized aspects of defense unification which permitted the Joint Chiefs of Staff to vote on arms policies of individual services, and thus \"rob\" the branches of autonomy.\n\nThe outbreak of the Korean War in summer 1950 proved the proposal of Secretary Johnson as incorrect and he resigned in September that year. Also Secretary of the Navy, Francis P. Matthews resigned one month earlier.\n\nLater service\n\nDue to the Revolts of the admirals, Blandy was forced to retire in February 1950 and Goodwin was ordered to Newport, Rhode Island for temporary duty as Chief of Staff and Aide to the President of the Naval War College under Vice admiral Donald B. Beary in April 1950. Goodwin was detached from that assignment two months and appointed member of the General Board of the Navy. He was shortly thereafter appointed acting Navy Chief of Public Information, as the substitute for Rear Admiral Russell S. Berkey, who was relieved of illness, but returned to the General Board of the Navy in July that year. Goodwin served in that capacity until February 1951, when he relieved his Academy class, Rear admiral John P. Whitney as Vice Commander, Military Air Transport Service (MATS).\n\nWhile in this capacity, Goodwin served under Lieutenant general Laurence S. Kuter and was co-responsible for the logistical support of United Nations troops fighting in Korea. The MATS operated from the United States to Japan and Goodwin served in this capacity until August 1953, when he was appointed Commander Carrier Division Two. While in this assignment, he took part in the Operation Mariner, Joint Anglo-American exercise which encountered very heavy seas over a two-week period in fall 1953.\n\nGoodwin was ordered to the Philippines in May 1954 and assumed duty as Commander, U.S. Naval Forces in the Philippines with headquarters at Naval Station Sangley Point near Cavite. He held that command in the period of tensions between Taiwan and China and publicly declared shortly after his arrival, that any attack on Taiwan by the Chinese Communists on the mainland would result in US participation in the conflict. The naval fighter planes under his command also provided escort for passing commercial planes. Goodwin worked together with retired Admiral Raymond A. Spruance, then-Ambassador to the Philippines, and accompanied him during the visits to Singapore, Bangkok and Saigon in January 1955.\n\nOn December 18, 1955, Goodwin's classmate Rear admiral Albert K. Morehouse, then serving as Commander, Naval Air Forces, Continental Air Defense Command (CONAD), died of heart attack and Goodwin was ordered to CONAD headquarters in Colorado Springs, Colorado to assume Morehouse's position. While in this capacity, he was subordinated to Army General Earle E. Partridge and was responsible for the Naval and Marine Forces allocated to the command designated for the defense of the Continental United States.\n\nRetirement\n\nGoodwin retired on June 1, 1957, after 40 years of active service and was advanced to the rank of Vice admiral on the retired list for having been specially commended in combat. A week later, he was invited back to his Monroe High School (now Neville High School) and handed a diploma showing that he had been graduated with the class of 1918. He then settled in Monterey, California where he taught American history at Stevenson school and was a member of the Naval Order of the United States.\n\nVice admiral Hugh H. Goodwin died at his home on February 25, 1980, aged 79. He was survived by his wife, Eleanor with whom he had two children, a daughter Sidney and a son Hugh Jr., who graduated from the Naval Academy in June 1948, but died one year later, when the Hellcat fighter he was piloting collided with another over the Gulf of Mexico during training.\n\nDecorations\n\nHere is the ribbon bar of Vice admiral Hugh H. Goodwin:\n\nReferences\n\n1900 births\n1980 deaths\nPeople from Monroe, Louisiana\nMilitary personnel from Louisiana\nUnited States Naval Academy alumni\nNaval War College alumni\nUnited States Naval Aviators\nUnited States Navy personnel of World War I\nUnited States Navy World War II admirals\nUnited States Navy vice admirals\nUnited States submarine commanders\nRecipients of the Legion of Merit"
  },
  {
    "id": "191c14b84f0c8cdad3297f2bee552fb089178995208d7185",
    "content": "Paper Info\n\nTitle: Conflict Optimization for Binary CSP Applied to Minimum Partition into Plane Subgraphs and Graph Coloring\nPublish Date: 25 Mar 2023\nAuthor List: Loïc Crombez (from LIMOS, Université Clermont Auvergne), Guilherme Da Fonseca (from LIS, Aix-Marseille Université), Florian Fontan (from Independent Researcher), Yan Gerard (from LIMOS, Université Clermont Auvergne), Aldo Gonzalez-Lorenzo (from LIS, Aix-Marseille Université), Pascal Lafourcade (from LIMOS, Université Clermont Auvergne), Luc Libralesso (from LIMOS, Université Clermont Auvergne), Benjamin Momège (from Independent Researcher), Jack Spalding-Jamieson (from David R. Cheriton School of Computer Science, University of Waterloo), Brandon Zhang (from Independent Researcher), Da Zheng (from Department of Computer Science, University of Illinois at Urbana-Champaign)\n\nFigure\n\nFigure 1: A partition of the input graph of the CG:SHOP2022 instance vispecn2518 into 57 plane graphs.It is the smallest instance of the challenge with 2518 segments.On top left, you see all 57 colors together.On top right, you see a clique of size 57, hence the solution is optimal.Each of the 57 colors is then presented in small figures.\nFigure 2: Number of colors over time for the instance vispecn13806 using different values p.The algorithm uses σ = 0.15, easy vertices, q max = 59022, but does not use the BDFS nor any clique.\nFigure 3: Number of colors over time with different values of q max obtained on the instance vispecn13806.Parameters are σ = 0.15, p = 1.2, no clique knowledge, and no BDFS.\nFigure 4: Number of colors over time with and without clique knowledge and BDFS obtained on the instance vispecn13806.Parameters are σ = 0.15, p = 1.2, and q max = 1500000.\nFigure 5: Number of colors over time for the instance vispecn13806 for different values of σ.In both figures the algorithm uses p = 1.2, easy vertices, q max = 59022, but does not use the BDFS nor any clique.For σ ≥ 0.25, no solution better than 248 colors is found.\nFigure 6: Number of colors over time (in hours) for the instance vispecn13806.\nSeveral CG:SHOP 2022 results.We compare the size of the largest known clique to the smallest coloring found by each team on a selection of 14 CG:SHOP 2022 instances.\n[20][21][22][23][24][25] with state-of-the-art graph coloring algorithms.The conflict optimizer underperforms except on the geometric graphs r* and dsjr*.CE39-0007), SEVERITAS (ANR-20-CE39-0005) and by the French government IDEX-ISITE initiative 16-IDEX-0001 (CAP[20][21][22][23][24][25].The work of Luc Libralesso is supported by the French ANR PRC grant DECRYPT (ANR-18-CE39-0007).\n\nabstract\n\nCG:SHOP is an annual geometric optimization challenge and the 2022 edition proposed the problem of coloring a certain geometric graph defined by line segments. Surprisingly, the top three teams used the same technique, called conflict optimization. This technique has been introduced in the 2021 edition of the challenge, to solve a coordinated motion planning problem.\nIn this paper, we present the technique in the more general framework of binary constraint satisfaction problems (binary CSP). Then, the top three teams describe their different implementations of the same underlying strategy. We evaluate the performance of those implementations to vertex color not only geometric graphs, but also other types of graphs.\n\nIntroduction\n\nThe CG:SHOP challenge (Computational Geometry: Solving Hard Optimization Problems) is an annual geometric optimization competition, whose first edition took place in 2019. The 2022 edition proposed a problem called minimum partition into plane subgraphs. The input is a graph G embedded in the plane with edges drawn as straight line segments, and the goal is to partition the set of edges into a small number of plane graphs (Fig. ) .\nThis goal can be formulated as a vertex coloring problem on a graph G defined as follows. The vertices of G are the segments defining the edges of G, and the edges of G correspond to pairs of crossing segments (segments that intersect only at a common endpoint are not considered crossing). The three top-ranking teams (Lasa, Gitastrophe, and Shadoks) on the CG:SHOP 2022 challenge all used a common approach called conflict optimization while the fourth team used a SAT-Boosted Tabu Search .\nConflict optimization is a technique used by Shadoks to obtain the first place in the CG:SHOP 2021 challenge for low-makespan coordinated motion planning , and the main ideas of the technique lent themselves well to the 2022 challenge. Next, we describe the conflict optimizer as a metaheuristic to solve constraint satisfaction problems (CSP) .\nWe start by describing a CSP. A CSP is a triple of • variables X = (x 1 , . . . , x n ), Each of the 57 colors is then presented in small figures. • domains D = (D 1 , . . . , D n ), and • constraints R. Each variable x i must be assigned a value in the corresponding domain D i such that all constraints are satisfied.\nIn general, the constraints may forbid arbitrary subsets of values. We restrict our attention to a particular type of constraints (binary CSP ), which only involve pairs of assignments. A partial evaluation is an assignment of a subset of the variables, called evaluated, with the remaining variables called non-evaluated.\nAll constraints involving a non-evaluated variable are satisfied by default. We only consider assignments and partial assignments that satisfy all constraints. The conflict optimizer iteratively modifies a partial evaluation with the goal of emptying the set S of non-evaluated variables, at which point it stops.\nAt each step, a variable x i is removed from S. If there exists a value x ∈ D i that satisfies all constraints, then we assign the value x to the variable x i . Otherwise, we proceed as follows. For each possible value x ∈ D i , we consider the set K(i, x) of variables (other than x i ) that are part of constraints violated by the assignment x i = x.\nWe assign to x i the value x that minimizes where w(j) is a weight function to be described later. The variables x j ∈ K(i, x) become non-evaluated and added to S. The weight function should be such that w(j) increases each time x j is added to S, in order to avoid loops that keep moving the same variables back and forth from S. Let q(j) be the number of times x j became non-evaluated.\nA possible weight function is w(j) = q(j). More generally, we can have w(j) = q(j) p for some exponent p (typically between 1 and 2). Of course, several details of the conflict optimizer are left open. For example, which element to choose from S, whether some random noise should be added to w, and the decision to restart the procedure from scratch after a certain time.\nThe CSP as is, does not apply to optimization problems. However, we can, impose a maximum value k of the objective function in order to obtain a CSP. The conflict optimizer was introduced in a low makespan coordinated motion planning setting. In that setting, the variables are the robots, the domains are their paths (of length at most k) and the constraints forbid collisions between two paths.\nIn the graph coloring setting, the domains are the k colors of the vertices and the constraints forbid adjacent vertices from having the same color. The conflict optimizer can be adapted to non-binary CSP, but in that case multiple variables may be unassigned for a single violated constraint. The strategy has some resemblance to the similarly named min-conflicts algorithm , but notable differences are that a partial evaluation is kept instead of an invalid evaluation and the weight function that changes over time.\nWhile the conflict optimization strategy is simple, there are different ways to apply it to the graph coloring problem. The goal of the paper is to present how the top three teams applied it or complemented it with additional strategies. We compare the relative benefits of each variant on the instances given in the CG:SHOP 2022 challenge.\nWe also compare them to baselines on some instances issued from graph coloring benchmarks. The paper is organized as follows. Section 2 presents the details of the conflict optimization strategy applied to graph coloring. In the three sections that follow, the three teams Lasa, Gitastrophe, and Shadoks present the different parameters and modified strategies that they used to make the algorithm more efficient for the CG:SHOP 2022 challenge.\nThe last section is devoted to the experimental results.\n\nLiterature Review\n\nThe study of graph coloring goes back to the 4-color problem (1852) and it has been intensively studied since the 1970s (see for surveys). Many heuristics have been proposed , as well as exact algorithms . We briefly present two classes of algorithms: greedy algorithms and exact algorithms. Greedy algorithms.\nThese algorithms are used to find good quality initial solutions in a short amount of time. The classic greedy heuristic considers the vertices in arbitrary order and colors each vertex with the smallest non-conflicting color. The two most famous modern greedy heuristics are DSATUR and Recursive Largest First (RLF ) .\nAt each step (until all vertices are colored), DSATUR selects the vertex v that has the largest number of different colors in its neighbourhood. Ties are broken by selecting a vertex with maximum degree. The vertex v is colored with the smallest non-conflicting color. RLF searches for a large independent set I, assigns the vertices I the same color, removes I from G , and repeats until all vertices are colored.\nExact algorithms. Some exact methods use a branch-and-bound strategy, for example extending the DSATUR heuristic by allowing it to backtrack . Another type of exact method (branch-and-cut-and-price) decomposes the vertex coloring problem into an iterative resolution of two sub-problems . The \"master problem\" maintains a small set of valid colors using a set-covering formulation.\nThe \"pricing problem\" finds a new valid coloring that is promising by solving a maximum weight independent set problem. Exact algorithms are usually able to find the optimal coloring for graphs with a few hundred vertices. However, even the smallest CG:SHOP 2022 competition instances involve at least a few thousands vertices.\n\nConflict Optimization for Graph Coloring\n\nHenceforth, we will only refer to the intersection conflict graph G induced by the instance. Vertices will refer to the vertices V (G ), and edges will refer to the edges E(G ). Our goal is to partition the vertices using a minimum set of k color classes C = {C 1 , . . . , C k }, where no two vertices in the same color class C i are incident to a common edge.\n\nConflict Optimization\n\nTABUCOL inspired neighbourhood One classical approach for the vertex coloring involves allowing solutions with conflicting vertices (two adjacent vertices with the same color). It was introduced in 1987 and called TABUCOL. It starts with an initial solution, removes a color (usually the one with the least number of vertices), and assigns uncolored vertices with a new color among the remaining ones.\nThis is likely to lead to some conflicts (i.e. two adjacent vertices sharing a same color). The local search scheme selects a conflicting vertex, and tries to swap its color, choosing the new coloring that minimises the number of conflicts. If it reaches a state with no conflict, it provides a solution with one color less than the initial solution.\nThe process is repeated until the stopping criterion is met. While the original TABUCOL algorithm includes a \"tabu-list\" mechanism to avoid cycling, it is not always sufficient, and requires some hyper-parameter tuning in order to obtain a good performance on a large variety of instances. To overcome this issue, we use a neighbourhood, but replace the \"tabu-list\" by the conflict optimizer scheme presented above.\nPARTIALCOL inspired neighbourhood PARTIALCOL another local search algorithm solving the vertex coloring problem was introduced in 2008. This algorithm proposes a new local search scheme that allows partial coloring (thus allowing uncolored vertices). The goal is to minimize the number of uncolored vertices.\nSimilarly to TABUCOL, PARTIALCOL starts with an initial solution, removes one color (unassigning its vertices), and performs local search iterations until no vertex is left uncolored. When coloring a vertex, the adjacent conflicting vertices are uncolored. Then, the algorithm repeats the process until all vertices are colored, or the stopping criterion is met.\nThis neighbourhood was also introduced alongside a tabu-search procedure. The tabu-search scheme is also replaced by a conflict-optimization scheme. Note that this neighbourhood was predominantly used by the other teams.\n\nFinding Initial Solutions\n\nLasa team used two approaches to find initial solutions: 1. DSATUR is the classical graph coloring algorithm presented in Section 1. 2. Orientation greedy is almost the only algorithm where the geometry of the segments is used. If segments are almost parallel, it is likely that they do not intersect (thus forming an independent set).\nThis greedy algorithm first sorts the segments by orientation, ranging from − π 2 to π 2 . For each segment in this order, the algorithm tries to color it using the first available color. If no color has been found, a new color is created for coloring the considered segment. This algorithm is efficient, produces interesting initial solutions and takes into account the specificities of the competition.\n\nSolution Initialization\n\nThe gitastrophe team uses the traditional greedy algorithm of Welsh and Powell to obtain initial solutions: order the vertices in decreasing order of degree, and assign each vertex the minimum-label color not used by its neighbors. During the challenge Gitastrophe attempted to use different orderings for the greedy algorithm, such as sorting by the slope of the line segment associated with each vertex (as the orientation greedy initialization presented in Section 3), and also tried numerous other strategies.\nUltimately, after running the solution optimizer for approximately the same amount of time, all initializations resulted in an equal number of colors.\n\nModifications to the Conflict Optimizer\n\nTaking inspiration from memetic algorithms, which alternate between an intensification and a diversification stage, the algorithm continually switched between a phase using the above conflict score, and one minimizing only the number of conflicts. Thus during the conflict-minimization phase, the random variables f (C j ) and w(u) are both fixed equal to 1 leading to a conflict score\nEach phase lasted for 10 5 iterations. Adding the conflict-minimization phase gave minor improvements to some of the challenge instances.\n\nShadoks\n\nIn this section, we describe the choices used by the Shadoks team for the options described in Section 2.1. The Shadoks generally chose to eliminate the color with the smallest number of elements. However, if the multistart option is toggled on, then a random color is used each time. The conflict set S is stored in a queue.\nThe Shadoks tried other strategies, but found that the queue gives the best results. The weight function used is w(u) = 1 + q(u) p , mostly with p = 1.2. The effect of the parameter p is shown in Fig. . Notice that in all figures, the number of colors shown is the average of ten executions of the code using different random seeds.\nThe algorithm uses σ = 0.15, easy vertices, q max = 59022, but does not use the BDFS nor any clique. If q(u) is larger than a threshold q max , the Shadoks set w(u) = ∞ so that the vertex u never reenters S. If at some point an uncolored vertex v is adjacent to some vertex u of infinite weight in every color class, then the conflict optimizer is restarted.\nWhen restarting, the initial coloring is shuffled by moving some vertices from their initial color class to a new one. Looking at Fig. , the value of q max does not seem to have much influence as long as it is not too small. Throughout the challenge the Shadoks almost exclusively used q max = 2000 • (75000/m) 2 , where m is the number of vertices.\nThis value roughly ensures a restart every few hours. q max =0.5k q max =5k q max =50k q max =100k q max =250k The Shadoks use the function f as a Gaussian random variable of mean 1 and variance σ. A good default value is σ = 0.15. The effect of the variance is shown in Fig. . Notice that setting σ = 0 gives much worse results.\nOption (e) The goal of BDFS is to further optimize very good solutions that the conflict optimizer is not able to improve otherwise. Fig. shows the influence of BDFS. While on this figure, the advantages of BDFS cannot be noticed, its use near the end of the challenge improved about 30 solutions. The bounded depth-first search (BDFS) algorithm tries to improve the dequeuing process.\nThe goal is to prevent a vertex in conflict with some adjacent colored vertices from entering in the conflict set. At the first level, the algorithm searches for a recoloring of some adjacent vertices which allows us to directly recolor the conflict vertex. If no solution is found, the algorithm  In both figures the algorithm uses p = 1.2, easy vertices, q max = 59022, but does not use the BDFS nor any clique.\nFor σ ≥ 0.25, no solution better than 248 colors is found. could recolor some vertices at larger distances from the conflict vertex. To do so, a local search is performed by trying to recolor vertices at a bounded distance from the conflict vertex in the current partial solution. The BDFS algorithm has two parameters: adjacency bound a max and depth d.\nIn order to recolor a vertex v, BDFS gets the set C of color classes with at most a max neighbors of v. If a class in C has no neighbor of v, v is assigned to C. Otherwise, for each class C ∈ C, BDFS tries to recolor the vertices in C which are adjacent to v by recursively calling itself with depth d − 1.\nAt depth d = 0 the algorithm stops trying to color the vertices. During the challenge the Shadoks used BDFS with parameters a max = 3 and d = 3. The depth was increased to 5 (resp. 7) when the number of vertices in the queue was 2 (resp. 1). Degeneracy order Given a target number of colors k, we call easy vertices a set of vertices Y such that, if the remainder of the vertices of G are colored using k colors, then we are guaranteed to be able to color all vertices of G with k colors.\nThis is obtained using the degeneracy order Y . To obtain Y we iteratively remove from the graph a vertex v that has at most k − 1 neighbors, appending v to the end of Y . We repeat until no other vertex can be added to Y . Notice that, once we color the remainder of the graph with at least k colors, we can use a greedy coloring for Y in order from last to first without increasing the number of colors used.\nRemoving the easy vertices reduces the total number of vertices, making the conflict optimizer more effective. The Shadoks always toggle this option on (the challenge instances contain from 0 to 23% easy vertices).\n\nResults\n\nWe provide the results of the experiments performed with the code from the three teams on two classes of instances. First, we present the results on some selected CG:SHOP 2022 instances. These instances are intersection graphs of line segments. Second, we execute the code on graphs that are not intersection graphs, namely the classic DIMACS graphs , comparing the results of our conflict optimizer implementations to previous solutions.\nThe source code for the three teams is available at: • Lasa: https://github.com/librallu/dogs-color • Gitastrophe: https://github.com/jacketsj/cgshop2022-gitastrophe • Shadoks: https://github.com/gfonsecabr/shadoks-CGSHOP2022\n\nCG:SHOP 2022 Instances\n\nWe selected 14 instances (out of 225) covering the different types of instances given in the CG:SHOP 2022 challenge. The results are presented in Table . For comparison, we executed the HEAD code on some instances using the default parameters. The table shows the smallest number of colors for which HEAD found a solution.\nWe ran HEAD for 1 hour of repetitions for each target number of colors on a single CPU core (the HEAD solver takes the target number of colors as a parameter and we increased this parameter one by one). At the end of the challenge, 8 colorings computed by Lasa, 11 colorings computed by Gitastrophe, and 23 colorings computed by Shadoks over 225 instances have been proved optimal (their number of colors is equal to the size of a clique).\nIn order to compare the efficiency of the algorithms, we executed the different implementations on the CG:SHOP instance vispecn13806. The edge density of this graph is 19%, the largest clique that we found has 177 vertices and the best coloring found during the challenge uses 218 colors. Notice that vispecn13806 is the same instance used in other Shadoks experiments in Section 5. Notice also that HEAD algorithm provides 283 colors after one hour compared to less than 240 colors for the conflict optimizers.\nWe ran the three implementations on three different servers and compared the results shown in Figure . For each implementation, the x coordinate is the running time in hours, while the y coordinate is the smallest number of colors found at that time.\n\nResults on DIMACS Graphs\n\nWe tested the implementation of each team on the DIMACS instances to gauge the performance of the conflict optimizer on other classes of graphs. We compared our results to the best known bounds and to the state of the art coloring algorithms HEAD and QACOL . The time limit for Lasa's algorithms is 1 hour.\nCWLS is Lasa's conflict optimizer with the neighbourhood presented in TABUCOL , while PWLS is the optimizer with the neighbourhood presented in PARTIALCOL . Gitastrophe algorithm ran 10 minutes after which the number of colors no longer decreases. Shadoks algorithm ran for 1 hour without the BDFS option (results with BDFS are worse).\nResults are presented in Table . We only kept the difficult DIMACS instances. For the other instances, all the results match the best known bounds. The DIMACS instances had comparatively few edges (on the order of thousands or millions); the largest intersection graphs considered in the CG:SHOP challenge had over 1.5 billion edges.\nWe notice that the conflict optimizer works extremely poorly on random graphs, but it is fast and appears to perform well on geometric graphs (r250.5, r1000.1c, r1000.5, dsjr500.1c and dsjr500.5), matching the best-known results . Interestingly, these geometric graphs are not intersection graphs as in the CG:SHOP challenge, but are generated based on a distance threshold.\nOn the DIMACS graphs, Lasa implementation shows better performance than the other implementations."
  },
  {
    "id": "f20114170ce5bb518f17950edb4ef828d980bba0bba01077",
    "content": "Margaret Way (b. Brisbane d. Cleveland, Queensland, Australia ) was an Australian writer of romance novels and women's fiction. A prolific author, Way wrote more than 120 novels since 1970, many through Mills & Boon, a romance imprint of British publisher Harlequin UK Ltd., owned by Harlequin Enterprises.\n\nBiography\nBefore her marriage, she was a well-known pianist, teacher, vocal coach and accompanist. She began writing when her son, Laurence Way, was born, a friend took a pile of Mills & Boon books to her, she read all and decided that she also could write these types of novels. She began to write and promote her country with her stories set in Australia. She sold her first novels in 1970. Margaret Way lives with her family in her native Brisbane. Beginning in 2013, Margaret began to self-publish, releasing her first \"e-book\" mid-July.\n\nMargaret died on the 10th of August 2022 in Cleveland, Queensland.\n\nBibliography\n\nSingle Novels\nKing Country (1970)\nBlaze of Silk (1970)\nThe Time of the Jacaranda (1970)\nBauhinia Junction (1971)\nMan from Bahl Bahla (1971)\nSummer Magic (1971)\nReturn to Belle Amber (1971)\nRing of Jade (1972)\nCopper Moon (1972)\nRainbow Bird (1972)\nMan Like Daintree (1972)\nNoonfire (1972)\nStorm Over Mandargi (1973)\nWind River (1973)\nLove Theme (1974)\nMcCabe's Kingdom (1974)\nSweet Sundown (1974)\nReeds of Honey (1975)\nStorm Flower (1975)\nLesson in Loving (1975)\nFlight into Yesterday (1976)\nRed Cliffs of Malpara (1976)\nMan on Half-moon (1976)\nSwan's Reach (1976)\nMutiny in Paradise (1977)\nOne Way Ticket (1977)\nPortrait of Jaime (1977)\nBlack Ingo (1977)\nAwakening Flame (1978)\nWild Swan (1978)\nRing of Fire (1978)\nWake the Sleeping Tiger (1978)\nValley of the Moon (1979)\nWhite Magnolia (1979)\nWinds of Heaven (1979)\nBlue Lotus (1979)\nButterfly and the Baron (1979)\nGolden Puma (1980)\nTemple of Fire (1980)\nLord of the High Valley (1980)\nFlamingo Park (1980)\nNorth of Capricorn (1981)\nSeason for Change (1981)\nShadow Dance (1981)\nMcIvor Affair (1981)\nHome to Morning Star (1981)\nBroken Rhapsody (1982)\nThe Silver Veil (1982)\nSpellbound (1982)\nHunter's Moon (1982)\nGirl at Cobalt Creek (1983)\nNo Alternative (1983)\nHouse of Memories (1983)\nAlmost a Stranger (1984)\nA place called Rambulara (1984)\nFallen Idol (1984)\nHunt the Sun (1985)\nEagle's Ridge (1985)\nThe Tiger's Cage (1986)\nInnocent in Eden (1986)\nDiamond Valley (1986)\nMorning Glory (1988)\nDevil Moon (1988)\nMowana Magic (1988)\nHungry Heart (1988)\nRise of an Eagle (1988)\nOne Fateful Summer (1993)\nThe Carradine Brand (1994)\nHolding on to Alex (1997)\nThe Australian Heiress (1997)\nClaiming His Child (1999)\nThe Cattleman's Bride (2000)\nThe Cattle Baron (2001)\nThe Husbands of the Outback (2001)\nSecrets of the Outback (2002)\nWith This Ring (2003)\nInnocent Mistress (2004)\nCattle Rancher, Convenient Wife (2007)\nOutback Marriages (2007)\nPromoted: Nanny to Wife (2007)\nCattle Rancher, Secret Son (2007)\nGenni's Dilemma (2008)\nBride At Briar Ridge (2009)\nOutback Heiress, Surprise Proposal (2009)\nCattle Baron, Nanny Needed (2009)\n\nLegends of the Outback Series\nMail Order Marriage (1999)\nThe Bridesmaid's Wedding (2000)\nThe English Bride (2000)\nA Wife at Kimbara (2000)\n\nKoomera Crossing Series\nSarah's Baby (2003)\nRunaway Wife (2003)\nOutback Bridegroom (2003)\nOutback Surrender (2003)\nHome to Eden (2004)\n\nMcIvor Sisters Series\nThe Outback Engagement (2005)\nMarriage at Murraree (2005)\n\nMen Of The Outback Series\nThe Cattleman (2006)\nThe Cattle Baron's Bride (2006)\nHer Outback Protector (2006)\nThe Horseman (2006)\n\nOutback Marriages Series\nOutback Man Seeks Wife (2007)\nCattle Rancher, Convenient Wife (2007)\n\nBarons of the Outback Series Multi-Author\nWedding At Wangaree Valley (2008)\nBride At Briar's Ridge (2008)\n\nFamily Ties Multi-Author\nOnce Burned (1995)\n\nHitched! Multi-Author\nA Faulkner Possession (1996)\n\nSimply the Best Multi-Author\nGeorgia and the Tycoon (1997)\n\nThe Big Event Multi-Author\nBeresford's Bride (1998)\n\nGuardian Angels Multi-Author\nGabriel's Mission (1998)\n\nAustralians Series Multi-Author\n7. Her Outback Man (1998)\n17. Master of Maramba (2001)\n19. Outback Fire (2001)\n22. Mistaken Mistress (2002)\n24. Outback Angel (2002)\n33. The Australian Tycoon's Proposal (2004)\n35. His Heiress Wife (2004)\n\nMarrying the Boss Series Multi-Author\nBoardroom Proposal (1999)\n\nContract Brides Series Multi-Author\nStrategy for Marriage (2002)\n\nEverlasting Love Series Multi-Author\nHidden Legacy (2008)\n\nDiamond Brides Series Multi-Author\nThe Australian's Society Bride (2008)\n\nCollections\nSummer Magic / Ring of Jade / Noonfire (1981)\nWife at Kimbara / Bridesmaid's Wedding (2005)\n\nOmnibus in Collaboration\nPretty Witch / Without Any Amazement / Storm Over Mandargi (1977) (with Lucy Gillen and Margaret Malcolm)\nDear Caliban / Heart of the Eagle / Swans' Reach (1978) (with Jane Donnelly and Elizabeth Graham)\nThe Bonds of Matrimony / Dragon Island / Reeds of Honey (1979) (with Elizabeth Hunter and Henrietta Reid)\nThe Man Outside / Castles in Spain / McCabe's Kingdom (1979) (with Jane Donnelly and Rebecca Stratton)\nWinds From The Sea / Island of Darkness / Wind River (1979) (with Margaret Pargeter and Rebecca Stratton)\nMoorland Magic / Tree of Idleness / Sweet Sundown (1980) (with Elizabeth Ashton and Elizabeth Hunter)\nThe Shifting Sands / Portrait of Jaime / Touched by Fire (1982) (with Jane Donnelly and Kay Thorpe)\nHead of Chancery / Wild Heart / One-Way Ticket (1986) (with Betty Beaty and Doris Smith)\nHeart of the Scorpion / The Winds of Heaven / Sweet Compulsion (1987) (with Janice Gray and Victoria Woolf)\nOne Brief Sweet Hour / Once More With Feeling / Blue Lotus (1990) (with Jane Arbor and Natalie Sparks)\nMarry Me Cowboy (1995) (with Janet Dailey, Susan Fox and Anne McAllister)\nHusbands on Horseback (1996) (with Diana Palmer)\nWedlocked (1999) (with Day Leclaire and Anne McAllister)\nMistletoe Magic (1999) (with Betty Neels and Rebecca Winters)\nThe Australians (2000) (with Helen Bianchin and Miranda Lee)\nWeddings Down Under (2001) (with Helen Bianchin and Jessica Hart)\nOutback Husbands (2002) (with Marion Lennox)\nThe Mother's Day Collection (2002) (with Helen Dickson and Kate Hoffmann)\nAustralian Nights (2003) (with Miranda Lee)\nOutback Weddings (2003) (with Barbara Hannay)\nAustralian Playboys (2003) (with Helen Bianchin and Marion Lennox)\nAustralian Tycoons (2004) (with Emma Darcy and Marion Lennox)\nA Mother's Day Gift (2004) (with Anne Ashley and Lucy Monroe)\nWhite Wedding (2004) (with Judy Christenberry and Jessica Steele)\nA Christmas Engagement (2004) (with Sara Craven and Jessica Matthews)\nA Very Special Mother's Day (2005) (with Anne Herries)\nAll I Want for Christmas... (2005) (with Betty Neels and Jessica Steele)\nThe Mills and Boon Collection (2006) (with Caroline Anderson and Penny Jordan)\nOutback Desire (2006) (with Emma Darcy and Carol Marinelli)\nTo Mum, with Love (2006) (with Rebecca Winters)\nAustralian Heroes (2007) (with Marion Lennox and Fiona McArthur)\nTall, Dark and Sexy (2008) (with Caroline Anderson and Helen Bianchin)\nThe Boss's Proposal (2008) (with Jessica Steele and Patricia Thayer)\nIsland Heat / Outback Man Seeks Wife / Prince's Forbidden Virgin / One Night Before Marriage / Their Lost-and-found Family / Single Dad's Marriage Wish (2008) (with Robyn Donald, Marion Lennox, Carol Marinelli, Sarah Mayberry and Anne Oliver)\nAustralian Billionaires (2009) (with Jennie Adams and Amy Andrews)\nCattle Baron : Nanny Needed / Bachelor Dad on Her Doorstep (2009) (with Michelle Douglas)\n\nExternal links\nMargaret Way at Harlequin Enterprises Ltd\n\nAustralian romantic fiction writers\nAustralian women novelists\nLiving people\nYear of birth missing (living people)\nWomen romantic fiction writers"
  },
  {
    "id": "ee53f01eb9a44de54723ab03918b7361f2eb630a35ce7b81",
    "content": "'用户指南 * User Guide 02 CN 11 EN * 包装内含 使用前注意事项 快速引导 产品部件详情说明 操作说明 02 02 03 06 08 01 \n•本产品支持在系统设置中进行瞳距调节 , 调节时请务必注意，最小瞳距可能会碰触鼻梁。当您佩戴头盔后，您 “显示”中进行手动调节，请注意设置使用不合适的瞳距，可能会引起视觉重影或者眼睛疲劳。 可在“设置” ► •本产品“护眼模式”经德国 TÜV Rheinland 低蓝光认证，通过软件算法降低三色通道中的蓝光量，来达到保护 “护眼” “色彩调节” 眼睛的作用，该模式下画面颜色偏黄，您可根据个人喜好在“设置” 中激活或关闭此功能。 ““显示” ► ► ► 包装内含： VR 头盔 / 手柄 × 2 / 1.5V AA 碱性干电池 × 4/ 眼镜支架 / 遮光鼻托 / 手柄挂绳 × 2 / USB-C 电源适配器 / USB-Cto C 2.0 数据线 / 快速指南 / 用户指南 / 安全与质保指南使用前注意事项 •本产品在开阔的室內环境使用体验最佳，建议至少预留 2×2 米 的空间。使用前请确认身体没有不适且周围环 境安全，特别是佩戴头盔在室内行走移动时，要尽量避免发生意外。 •不建议 12 岁及以下儿童使用本产品，建议将头盔、手柄和配件置于儿童够不到的位置，13 岁以上青少年须在 成人监护下使用，以免发生意外。 •本产品无近视调节功能，近视用户请佩戴眼镜使用并尽量避免近视眼镜被头盔的光学镜片磨伤或刮伤。建议在 使用和收纳时注意防护光学镜片，避免尖锐物体划伤镜片，擦拭清洁时请使用柔软的眼镜布，否则可能划伤镜片， 影响视觉效果。 •长时间使用可能引发轻微的昡晕或者眼疲劳，建议使用 30 分钟后适当休息，可通过眼保健操或观看远处物体缓 解眼疲劳。如果您的身体感到任何不适，请立即停止使用。如果不适持续，请咨询医生。 •当头盔镜片被阳光或紫外线照射时（尤其在户外、阳台、窗台及汽车内存放时），可能导致屏幕出现永久性黄斑。 请尽量避免该情况发生，此种屏幕损坏不在产品的质保范围内。 *本产品最终外观及功能以实物为准，部分地区包装内含物品有所差异，本说明仅供参考。 02 CN\n六自由度 VR 体验 本产品可以追踪头盔和手柄前、后、左、右、上、下和旋转的运动状态，您在现实中的肢体运动会实时反映在虚 拟世界中。 由于没有任何线缆的束缚，您在虚拟世界自由探索时请确保游玩区域的安全。 1. 建议准备一个整洁安全的体验空间：至少 2×2 米；保持房间明亮，避免在只有单色的墙或大面积玻璃、镜子类 反射物以及许多移动画面和物体的空间中使用。 2. 撕下 VR 头盔前端摄像头上的保护膜，并佩戴手柄挂绳。 3. 根据开机后的画面提示进行游玩区域的设定。 ❶ 安装电池 按箭头方向拔出电池盖侧边的绝缘纸 快速引导 提示：本产品虚拟的安全区提醒功能，不能完全保证您在设定好的游戏区域中的安全，请时刻注意周围的安全情况。 提示：建议使用 1.5V AA 碱性电池。 按照图示拨动电池盖拨钮打开电池盖更换电池。 03 CN\n❷ 手柄开机 ❸ 头盔开机 ❹ 佩戴头盔，调节至清晰舒适的位置 首次开机：拔出绝缘纸，手柄自动开机（蓝灯闪烁） 非首次开机：短按手柄 Home 键开机（蓝灯闪烁） 长按头盔电源键 2 秒（蓝灯常亮） 调节旋钮转动绑带，使后脑垫套在头上，微调绑带长度及佩戴位置至视野清晰 04 提示：近视用户请佩戴眼镜或镜片插件使用，本产品不具备近视调节功能。 CN\n❺ 微调顶绑带 微调顶绑带使其受力以减少额头压力 ❻ 瞳 距 调 节 在系统设置：“设置” ► “显示”界面中进行瞳距调节，点击“+”或“-”按钮可微调瞳距直至画面清晰 64mm 请勿 强行 掰动镜 筒，以 免造 成损坏 ！ 请注 意设 置使用 不合适 的瞳 距，可 能 会引起 视 觉重影 或 者眼睛 疲 劳。准 确 的瞳距 设 置有助 于 获得清 晰 的图像 并 减少眼睛 疲劳。 05 CN\n产品部件详情说明 头盔状态指示灯 蓝灯常亮：开机进行中或工作状态 黄灯常亮：充电中，电量低于 98% 红灯常亮：充电中，电量低于 20% 绿灯常亮：充电完毕，电量大于 98% 或 充满 蓝灯闪烁：关机进行中 红灯闪烁：电量低于 20% 指示灯熄灭：休眠或关机 06 ① 电源键 开机：长按 2 秒 关机：长按 5 秒 复位：长按 10 秒 开机时，短按休眠 ② ③ ④ ⑤ 状态指示灯 贴脸泡棉 音量键 彩色透视摄像头 使用时请勿遮挡 ⑥ ⑦ ⑧ 顶部绑带 可拆卸 绑带旋钮 环境追踪摄像头 使用时请勿遮挡 ⑨ ⑩ ⑪ USB-C 接口 左 / 右喇叭 距离传感器 佩戴头盔后，系统自动唤醒 摘下头盔后，系统自动休眠 ⑫ ⑬ 眼球追踪摄像头 此功能仅 Pro 版支持 使用时请勿遮挡 面部追踪摄像头 此功能仅 Pro 版支持 使用时请勿遮挡 CN\n手柄状态指示灯 熄灭：已连接或者关机 蓝灯常亮：固件升级模式 蓝灯闪烁：连接中 红蓝灯交替慢速闪烁：等待配对 ① ② 摇杆 菜单键 ③ Home 键 开机 : 短按关机 : 长按 6 秒退出应用 : 短按屏幕中心校正 : 长按 1 秒④ ⑤ ⑥ ⑦ 状态指示灯 抓握键 截屏键 扳机键 ⑧ ⑨ 电池盒 打开：拨动拨钮，电池盒弹出 安装：按压直至自动锁紧 追踪光环 使用时请勿遮挡 注：手柄挂绳可按图示将粗绳穿过细绳并锁紧在手柄尾端 07 CN\n手柄硬件复位 如果手柄出现按 Home 键和任何按键均无反应或者头盔中虚拟手柄卡死不动的问题可拆装电池重新启动手柄。 近视用户配戴 本设备不具备近视调节功能，头盔可支持佩戴镜框宽度小于 150mm 的大多数标准眼镜。 操作说明 头控模式 未连接手柄的情况下，您可通过转动头部光标及点击头盔音量加减按键进行操作。 切换主控手柄射线 在主控菜单下，短按对应手柄的扳机键可以切换主控手柄的射线。 屏幕中心校正 戴着头盔直视前方，按住手柄 Home 键（或头控模式下头盔上的音量减键）1 秒以上，进行屏幕中心的校正将菜 单拉到当前视野朝向位置。 断开手柄 长按手柄 Home 键直至手柄状态指示灯红灯亮起并伴随振动产生时即可松手，此时手柄关机并断开与头盔的连接。 您无需刻意进行手柄关机操作，在以下状态下手柄会自动关机省电： •头盔进入深度休眠时（摘下头盔后一段时间） •头盔手柄管理界面解绑手柄时 •头盔关机时 添加新手柄 如需添加新手柄（头盔最多可同时连接一对手柄，即左右手柄各一只），或解绑手柄后再次连接 , 可进入“设置” “手 柄”，点击“配对”，同时按住手柄 Home 键和扳机键直至手柄状态指示灯红蓝交替闪烁时即可松开，然后根据 头盔画面提示操作。 ► 休眠 / 唤醒 方式一：摘下头盔一段时间后，系统自动休眠；戴上头盔时，系统自动唤醒。 方式二：短按头盔电源键也可以进行休眠或唤醒操作。 硬件复位 头盔硬件复位 如果头盔出现短按头盔电源键没有反应或头盔的画面卡死等问题，可以长按头盔电源键 10 秒以上重新启动头盔。 08 CN\n安装眼镜支架 安装遮光鼻托 如您存在眼镜摩擦光学镜片或者压迫鼻梁的问题，请按照图示安装眼镜支架以增加间隔空间。 您可根据佩戴的舒适度选择是否安装。 如您感觉鼻子处漏光影响体验，请按照图示安装遮光鼻托配件。 由于眼睛空间密闭可能加剧起雾及出汗问题，您可根据喜好选择是否安装。 ❶ 摘下贴脸泡棉 ❷ 将眼镜支架按照图示安装在产品上 ❸ 将贴脸泡棉按照图示安装眼镜支架上 ❶ 摘下贴脸泡棉 ❸ 安装贴脸泡棉❷ 将遮光鼻托按照图示方式安装在贴脸泡棉上 注：按照图示拆卸眼镜支架 09 CN\n更换贴脸泡棉 贴脸泡棉多次清洁和长时间使用后会变色和质地变软，您可酌情更换新泡棉。 更换顶绑带 摘下贴脸泡棉 ❸ 安装贴脸泡棉 按照图示捏住顶绑带金属扣，往下压到底然后抽出 ❷ •购买优质热门应用 •畅 聊 社 区， 与 众 多 PICO 玩 家 一起探索 VR 世界 •管理设备更便捷 •参与丰富互动活动 •更多精彩内容等你来发现 ❶ 微 信公 众 号：PICO VR抖音：PICO官 方 旗 舰 店哔 哩 哔 哩：PICO-VR官 方微 博：PICO-VR ❶ ❷ 10 CN\nIn The Box: VR Headset / 2 Controllers / 4 1.5V AA Alkaline Batteries / Glasses Spacer / Nose Pad / 2 Controller Lan- yards / USB-C Power Adapter / USB-C to C 2.0 Data Cable / Quick Guide / User Guide / Safety and WarrantyGuide Important Health & Safety Notes • This product is designed and intended to be used in an open and safe indoor area, free of anytripping or slipping hazards. To avoid accidents, remain conscious to the potential confines ofyour physical area and respect the boundary of your virtual area whenever you see it. Be sure towear the lanyards when using the Controllers. Make sure that there is enough space around yourhead and body (at least 2 meters by 2 meters) to stretch your arms to avoid damage or injury toyourself, others, and your surroundings. • This product is not recommended for children aged 12 and under. It is recommended to keep headsets,controllers and accessories out of the reach of children. Teenagers aged 13 and over must use it underadult supervision to avoid accidents. • This product is designed to accommodate most prescription glasses. Make sure to wear the VR Headsetin a manner in which the VR Headset lenses do not rub or impair your prescription lenses. • Prolonged use may cause dizziness or eye fatigue. It is recommended to take a break every 30 minutes.Try relieving your eyestrain by looking at distant objects. If you feel any discomfort, stop using the prod- uct immediately. If the discomfort persists, seek medical advice.• Do not expose the optical lenses to direct sunlight or other strong light sources. Exposure to directsunlight may cause permanent yellow spot damage on the screen. Screen damage caused by sunlightexposure or other strong sources of light is not covered by the warranty. • This product supports interpupillary distance (IPD) adjustment in system settings. When adjusting,please be aware that with the minimum IPD, it may touch the bridge of the nose. You can adjust the IPDaccording to your actual interpupillary distance in \"Settings\"►\"Display\". Please note that using inap- propriate IPD may increase the risk of discomfort. • This product has an “Eye Protection Mode”, certified by TÜV Rheinland (Germany), which can protectyour eyes by reducing blue light in the three color channels using software algorithms. The screen ap- pears yellowish in this mode and you can turn this feature on/off in \"Settings\"►\"Display\"►\"Color\"►“- Eye Protection”. • Protect optical lenses during use and storage to prevent damage, such as scratches or exposure tostrong light or direct sunlight. * Product and packaging are updated regularly, and the functions and contents of the standalone headset may be upgraded in the future.Therefore, the content, appearance and functionality listed in this manual and product packaging are subject to change and may notreflect the final product. These instructions are for reference only. * Carefully read this user guide before using the product and share this information with any other users, as it contains important safetyinformation. Keep the user guide as reference for the future. 11 EN\n6 Degrees of Freedom VR The device can track your translational and rotational movements in all directions (up/down, left/right,forward/backward, pitch, roll, and yaw). Your movements in the real world will be captured and translatedto what you see in the virtual world when using the appropriate content. Ensure a safe environment before you start your VR experience. 1. Clear a safe indoor area of at least 2 meters by 2 meters. Keep the room bright, avoid spaces with main- ly single-colored walls, glass, mirrors, moving pictures or other similar objects. 2. Remove the protective film that covers the headset front cameras. Wear the lanyards connected to theControllers. 3. Set up your environment by following instructions on the VR Headset screen. Install Batteries ❶Pull the tab to remove the insulating paper. Quick Guide 2 m 2m This product can not guarantee your safety with guardian system, you will need to always pay attention to the surrounding safety. * Note: 1.5V AA alkaline batteries should be used.Slide the toggle according to arrow direction toopen the battery case. 12 EN\nPower on the Controller ❷ First Start: The Controller will start automaticallyafter removing the insulating paper. Others: Short press the Home button for 1second until the status indicator flashes blue.Power on the VR Headset ❸ Long press the Power button for 2 seconds untilthe status indicator turns blue.Wear Your Headset for a Comfortable Fit and View ❹ Adjust the strap dial to turn the strap so that the back of your head rests on the padding. Fine-tune thelength and position of the strap to give a clear view. * Note: You can use this product with prescription glasses or lenses insert. 13 EN\nFine-tune the Top Strap ❺ Fine-tune the head strap to reduce pressure on the forehead. Interpupillary Distance (IPD) Adjustment ❻ In System Setting, go to “Setting” ► “Display” to adjust IPD, tap “+” or “-” button to slightly adjust IPDuntil the picture is clear. 14 64mm Please note that inappropriate IPD setting may cause ghosting or eyestrain.Accurate IPD setting helps you get a clear imaging and ease eyestrain. EN\nProduct Details VR Headset Status Indicator Legend Blue: Powered on with battery over 20% Yellow: Charging: Battery is less than 98% Red: Charging: Battery is less than 20% Green: Charging: Battery is more than 98% or charge complete Blue flashing: Shutting down Red flashing: Battery is less than 20% Off: Sleeping or Powered off Power Power on: Long press for 2 seconds Power off: Long press for 5 seconds Hardware reset: Long press for 10 seconds Short press to enter sleep or wake up Status Indicator Face Cushion Volume ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ RGB See Through Camera Do not block during use. Top Strap Removable Strap Dial Tracking Cameras Do not block during use. ⑨ ⑩ ⑪ USB-C Interface Left/Right Speaker Proximity Sensor The system wakes upwhen the VR headset isput on, sleeps when VRheadset is taken off. ⑫ ⑬ Eye Tracking Cameras Pro version only. Do not block during use. Face Tracking Camera Pro version only. Do not block during use. 15 EN\nController Status Indicator Legend Off: Connected or Powered off Blue: Firmware updating in progress Blue flashing: Searching for connection Red and blue flashing alternately: Pairing in progress 16 Joystick Menu ③ ① ② Home Power on: Short pressPower off: Long press for 6 secondsReturn home screen: Short pressScreen recentering: Press for 1 secondStatus Indicator Grip Capture Trigger ④ ⑤ ⑥ ⑦ ⑧ ⑨ Battery Case Open: Slide down the toggle andpop up the battery case. Lock: Push the battery case to lock. Tracking Ring Do not block during use. * Note: Pass the Controller Lanyardthrough the string as shown andlock at the end of the Controller EN\nOperating Instructions Headset Control Mode If the Controller is not connected, you can interact with the home screen by moving your head to directthe crosshairs over your intended selection and clicking the Volume Up/Down button on the VR Headset. Switch the pointer of the master Controller In the home screen, short press the Trigger of the corresponding Controller to switch the pointer of themaster Controller. Screen re-centering Wear the VR Headset and look straight ahead, press and hold the Home button of the Controller or VRHeadset ( or the Volume Down button of the VR Headset in head control mode) for more than 1 second tore-center the screen. Disconnect the Controller Press and hold the Home button until the status indicator turns red and the Controller vibrates.Controllers will automatically shut down to save power in the following cases:When the VR Headset enters deep sleep (a while after the VR Headset is taken off)When the Controller is unpairedWhen the VR Headset is powered off Add a new Controller If you need to add a new Controller (the VR Headset can only connect one left Controller and one rightController) or reconnect with an unpaired Controller. Go to “Settings” ► “Controller”, click on “Pair”.Press and hold the Home button and the Trigger of the Controller at the same time until the red and bluelights of the Controller flashing alternately, and then follow the instructions on the VR Headset screen. Sleep / Wake up Option 1 (Proximity Sensor) Take off VR Headset for automatic sleeping: wear the VR Headset for automat- ic waking up. Option 2 (POWER Button) Press the Power button of the VR Headset for manual sleeping or waking up. Hardware reset VR Headset reset If the visual in the VR Headset freezes, or the VR Headset does not respond after short press the Powerbutton, you can press the Power button of the VR Headset for more than 10 seconds to reboot the VRHeadset. Controller reset If the virtual Controller, the Home button or any buttons of the Controller doesn\\'t respond, remove andreinstall the battery case to restart the Controller. The VR Headset Adjustment This device has no myopia adjustment function. The VR Headset allows wearing most standard glasseswith a frame width of less than 150mm. to install Glasses Spacer to increase the space. You can install or not according to your situation. 17 EN\nInstall Glasses Spacer Install Nose Pad If you have glasses collision with headset lens or pressure on the bridge of nose, please follow the pictureto install Glasses Spacer to increase the space. You can install or not according to your situation. If you feel light leaking from your nose, please follow the picture to install Nose Pad to block the light.You can consider having it installed at your own discretion. Disassemble the Face Cushion. Install the Glasses Spacer on the Headset. ❸ ❶ ❷ Install the Face Cushion on the Glasses Spacer. Disassemble the Face Cushion. Install the Nose Pad on the Face Cushion. ❶ ❷ Install the Face Cushion on the Headset. ❸ * Note: Disassemble the Glasses Spacer 18 EN\nReplace Face Cushion The Face Cushion will have the following phenomena such as color change, surface fluff, soft texture afterlong-term use and repeated cleaning. You can replace a new Face Cushion as needed. Replace Top Strap ❶ ❷ Disassemble the Face Cushion. Pinch the metal buckle of the top strap asshown, press it down and pull it out.Install the Face Cushion on. ❸ ❷ ❶ • Purchase high-quality and trending apps • Join PICO Community and explore the VR worldwith other PICO players• Manage your device with ease • Engage in diverse and interactive activities • More exciting features waiting for you 19 EN\n'"
  },
  {
    "id": "df53ff06fbc99dbae6bfe13a946420321d4542480eeb56f3",
    "content": "Thalassaemia minor | definition of Thalassaemia minor by Medical dictionary\nThalassaemia minor | definition of Thalassaemia minor by Medical dictionary\nhttps://medical-dictionary.thefreedictionary.com/Thalassaemia+minor\n(redirected from Thalassaemia minor)\nRelated to Thalassaemia minor: thalassaemia major\nThalassemia describes a group of inherited disorders characterized by reduced or absent amounts of hemoglobin, the oxygen-carrying protein inside the red blood cells. There are two basic groups of thalassemia disorders: alpha thalassemia and beta thalassemia. These conditions cause varying degrees of anemia, which can range from insignificant to life threatening.\nAll types of thalassemias are considered quantitative diseases of hemoglobin, because the quantity of hemoglobin produced is reduced or absent. Usual adult hemoglobin is made up of three components: alpha globin, beta globin, and heme. Thalassemias are classified according to the globin that is affected, hence the names alpha and beta thalassemia. Although both classes of thalassemia affect the same protein, the alpha and beta thalassemias are distinct diseases that affect the body in different ways.\nBeta thalassemia may be the most best-known type of thalassemia and is also called Cooley's anemia. It is caused by a change in the gene for the beta globin component of hemoglobin. Beta thalassemia causes variable anemia that can range from moderate to severe, depending in part on the exact genetic change underlying the disease. Beta thalassemia can be classified based on clinical symptoms. Beta thalassemia major usually causes severe anemia that can occur within months after birth. If left untreated, severe anemia can result in insufficient growth and development, as well as other common physical complications that can lead to a dramatically decreased life-expectancy. Fortunately, in developed countries beta thalassemia is usually identified by screening in the newborn period, before symptoms have developed. Children who are identified early can be started on ongoing blood transfusion therapy as needed. Although transfusion therapy prevents many of the complications of severe anemia, the body is unable to eliminate the excess iron contained in the transfused blood. Over time, the excess iron deposits in tissues and organs, resulting in damage and organ failure. Another medication must be administered to help the body eliminate the excess iron and prevent iron-over-load complications. Beta thalassemia intermedia describes the disease in individuals who have moderate anemia that only requires blood transfusions intermittently, if at all.\nAlpha thalassemia is the result of changes in the genes for the alpha globin component of hemoglobin. There are two main types of alpha thalassemia disease: hemoglobin H disease and alpha thalassemia major. The two diseases are quite different from beta thalassemia as well as from one another. Individuals with hemoglobin H disease can experience events of hemolytic anemia—anemia caused by the rapid breakdown of the red blood cells. These events are thought to be triggered by various environmental causes, such as infection and/or exposure to certain chemicals. Hemoglobin H disease is in most cases milder than beta thalassemia. It does not generally require transfusion therapy. Alpha thalassemia major is a very serious disease that results in severe anemia that begins even before birth. Most affected babies do not survive to be born or die shortly after birth.\nThe thalassemias are among the most common genetic diseases worldwide. Both alpha and beta thalassemia have been described in individuals of almost every ancestry, but the conditions are more common among certain ethnic groups. Unaffected carriers of all types of thalassemia traits do not experience health problems. In fact, the thalassemia trait is protective against malaria, a disease caused by blood-borne parasites transmitted through mosquito bites. According to a widely accepted theory, most genetic changes—mutations—that cause thalassemia occurred multiple generations ago. Coincidentally, these mutations increased the likelihood that carriers would survive malaria infection. Survivors passed the mutation onto their offspring, and the trait became established throughout areas where malaria is common. As populations migrated, so did the thalassemia traits.\nBeta thalassemia trait is seen most commonly in people with the following ancestry: Mediterranean (including North African, and particularly Italian and Greek), Middle Eastern, Indian, African, Chinese, and Southeast Asian (including Vietnamese, Laotian, Thai, Singaporean, Filipino, Cambodian, Malaysian, Burmese, and Indonesian). Alpha-thalassemia trait is seen with increased frequency in the same ethnic groups. However, there are different types of alpha thalassemia traits within these populations. The frequency of hemoglobin H disease and alpha thalassemia major depends on the type of alpha thalassemia trait. The populations in which alpha thalassemia diseases are most common include Southeast Asians and Chinese (particularly Southern Chinese).\nIt is difficult to obtain accurate prevalence figures for various types of thalassemia within different populations. This difficulty arises due to testing limitations in determining exact genetic diagnoses, as well as the fact that many studies have focused on small, biased hospital populations.\nTwo studies reflect prevalence figures that can be helpful counseling families and determining who to screen for beta thalassemia. Between the years of 1990 and 1996, the State of California screened more than 3.1 million infants born in the state for beta thalassemia. Approximately 1 in 114,000 infants had beta thalassemia major, with prevalence rates being highest among Asian Indians (about one in 4,000), Southeast Asians (about one in 10,000), and Middle Easterners (about one in 7,000). Another type of beta thalassemia disease, E/beta thalassemia, was represented in approximately one in 110,000 births, all of which occurred in families of Southeast Asian ancestry. Among Southeast Asians, the prevalence of E/beta thalassemia was approximately one in 2,600 births. This is in keeping with the observation that hemoglobin E trait carrier rates are relatively high within the Southeast Asian population: 16% in a study of 768 immigrants to California, and up to 25% in some specific Southeast Asian populations such as Cambodians. While these California studies address some of the limitations of earlier population studies, the pattern observed in California is expected to be different in other areas of the United States and the world. For example, Italians are underrepresented in this population when compared to the population of the East Coast of the United States.\nDetermining prevalence figures for alpha thalassemia is even more difficult due to increased limitations in diagnostic testing. All types of alpha thalassemia disease are most common among people of Southeast Asian and Chinese descent, for reasons that become clearer with an understanding of the underlying genetics of alpha thalassemia. One study of 500 pregnant women in Northern Thailand estimated a frequency of one in 500 pregnancies affected by alpha thalassemia major, for example. Prevalence of alpha thalassemia disease is significantly lower in the United States primarily because of immigration patterns; although at least one state, California, has observed growing hemoglobin H disease incidence rates that are high enough to justify universal newborn screening for the condition.\nHumans normally make several types of the oxygen-carrying protein hemoglobin. An individual's stage in development determines whether he or she makes primarily embryonic, fetal, or adult hemoglobins. All types of hemoglobin are made of three components: heme, alpha (or alpha-like) globin, and beta (or beta-like) globin. All types of thalassemia are caused by changes in either the alpha- or beta-globin gene. These changes cause little or no globin to be produced. The thalassemias are, therefore, considered quantitative hemoglobin diseases. All types of thalassemias are recessively inherited, meaning that a genetic change must be inherited from both the mother and the father. The severity of the disease is influenced by the exact thalassemia mutations inherited, as well as other genetic and environmental factors. There are rare exceptions, notably with beta thalassemia, where globin gene mutations exhibit a dominant pattern of inheritance in which only one gene needs to be altered in order to see disease expression. Scientists continue to study the causes. For instance, a new mutation for alpha-thalassemia was discovered for the first time among Iranian patients in 2004.\nBETA-THALASSEMIA. Most individuals have two normal copies of the beta globin gene, which is located on chromosome 11 and makes the beta globin component of normal adult hemoglobin, hemoglobin A. There are approximately 100 genetic mutations that have been described that cause beta thalassemia, designated as either beta0 or beta + mutations. No beta globin is produced with a beta0 mutation, and only a small fraction of the normal amount of beta globin is produced with a beta + mutation.\nWhen an individual has one normal beta globin gene and one with a beta thalassemia mutation, he or she is said to carry the beta thalassemia trait. Beta thalassemia trait, like other hemoglobin traits, is protective against malaria infection. Trait status is generally thought not to cause health problems, although some women with beta thalassemia trait may have an increased tendency toward anemia during pregnancy.\nWhen two members of a couple carry the beta thalassemia trait, there is a 25% chance that each of their children will inherit beta thalassemia disease by inheriting two beta thalassemia mutations, one from each parent. The clinical severity of the beta thalassemia disease—whether an individual has beta thalassemia intermedia or beta thalassemia major—will depend largely on whether the mutations inherited are beta0 thalassemia or beta + thalassemia mutations. Two beta0 mutations generally lead to beta thalassemia major, and two beta+ thalassemia mutations generally lead to beta thalassemia intermedia. Inheritance of one beta0 and one beta + thalassemia mutation tends to be less predictable.\nAlthough relatively uncommon, there are other thalassemia-like mutations that can affect the beta globin gene. Hemoglobin E is the result of a substitution of a single nucleotide. This change results in a structurally altered hemoglobin that is produced in decreased amounts. Therefore, hemoglobin E is unique in that it is both a quantitative (i.e. thalassemia-like) and qualitative trait. When co-inherited with a beta thalassemia trait, it causes a disease that is almost indistinguishable from beta thalassemia disease. Large deletions around and including the beta globin gene can lead to delta/beta thalassemia or hereditary persistence of fetal hemoglobin (HPFH). Interestingly, delta/beta thalassemia trait behaves very similarly to beta thalassemia trait in its clinical manifestations. However, HPFH trait does not tend to cause hemoglobin disease when co-inherited with a second thalassemia or other beta globin mutation.\nALPHA-THALASSEMIA. Most individuals have four normal copies of the alpha globin gene, two copies on each chromosome 16. These genes make the alpha globin component of normal adult hemoglobin, which is called hemoglobin A. Alpha globin is also a component of fetal hemoglobin and the other major adult hemoglobin called hemoglobin A2. Mutations of the alpha globin genes are usually deletions of the gene, resulting in absent production of alpha globin. Since there are four genes (instead of the usual two) to consider when looking at alpha globin gene inheritance, there are several alpha globin types that are possible.\nAbsence of one alpha globin gene leads to a condition known as silent alpha thalassemia trait. This condition causes no health problems and can be detected only by special genetic testing. Alpha thalassemia trait occurs when two alpha globin genes are missing. This can occur in two ways. The genes may be deleted from the same chromosome, causing the 'cis' type of alpha thalassemia trait. Alternately, they may be deleted from different chromosomes, causing the 'trans' type of alpha thalassemia trait. In both instances, there are no associated health problems, although the trait status may be detected by more routine blood screening.\nHemoglobin H disease results from the deletion of three alpha globin genes, such that there is only one functioning gene. Typically, this can occur when one parent carries the silent alpha thalassemia trait, and the other parent carries the 'cis' type of the alpha thalassemia trait. In this situation, there is a 25% chance for hemoglobin H disease in each of such a couple's children.\nHemoglobin H disease-like symptoms can also be a part of a unique condition called alpha thalassemia mental retardation syndrome. Alpha thalassemia mental retardation syndrome can be caused by a deletion of a significant amount of chromosome 16, affecting the alpha globin genes. This is usually not inherited, but rather occurs sporadically in the affected individual. Affected individuals have mild hemoglobin H disease, mild-to-moderate mental retardation, and characteristic facial features. This syndrome can also occur as a sex-linked form in which a mutation is inherited in a particular gene on the X-chromosome. This gene influences alpha globin production, as well as various other developmental processes. Individuals affected with this form of the syndrome tend to have more severe mental retardation, delayed development, nearly absent speech, characteristic facial features, and genital-urinary abnormalities. The remaining discussion will focus only on aspects of hemoglobin H disease.\nAlpha thalassemia major results from the deletion of all four alpha globin genes, such that there are no functioning alpha globin genes. This can occur when both parents carry the 'cis' type of the alpha thalassemia trait. In this situation, there is a 25% chance for alpha thalassemia major in each of such a couple's children.\nBeta thalassemia major is characterized by severe anemia that can begin months after birth. In the United States and other developed countries beta thalassemia is identified and treated early and effectively. Therefore, the following discussion of symptoms applies primarily to affected individuals in the past and unfortunately in some underdeveloped countries now. If untreated, beta thalassemia major can lead to severe lethargy, paleness, and delays in growth and development. The body attempts to compensate by producing more blood, which is made inside the bones in the marrow. However, this is ineffective without the needed genetic instructions to make enough functioning hemoglobin. Instead, obvious bone expansion and changes occur that cause characteristic facial and other changes in appearance, as well as increased risk of fractures. Severe anemia taxes other organs in the body—such as the heart, spleen, and liver—which must work harder than usual. This can lead to heart failure, as well as enlargement and other problems of the liver and spleen. When untreated, beta thalassemia major generally results in childhood death, usually due to heart failure. In 2004, the first known heart attack associated with beta thalassemia major was reported. Fortunately, in developed countries diagnosis is usually made early, often before symptoms have begun. This allows for treatment with blood transfusion therapy, which can prevent most of the complications of the severe anemia caused by beta thalassemia major. Individuals with beta thalassemia intermedia have a more moderate anemia that may only require treatment with transfusion intermittently, such as when infections occur and stress the body. As a person with beta thalassemia intermedia gets older, however, the need for blood transfusions may increase to the point that they are required on a regular basis. When this occurs their disease becomes more similar to beta thalassemia major. Other genetic and environmental factors can influence the course of the disease as well. For example, co-inheritance of one or two alpha thalassemia mutations can tend to ameliorate some of the symptoms of beta thalassemia disease, which result in part from an imbalance in the amount of alpha- and beta-globin present in the red blood cells.\nHemoglobin h disease\nAbsence of three alpha globin genes causes an imbalance of alpha and beta globin proteins in the red blood cells. The excess beta globin proteins tend to come together to form hemoglobin H, which is unable to release oxygen to the tissues. In addition, hemoglobin H tends to precipitate out in the cells, causing damage to the red blood cell membrane. When affected individuals are exposed to certain drugs and chemicals known to make the membrane more fragile, the cells are thought to become vulnerable to breakdown in large numbers, a complication called hemolytic anemia. Fever and infection are also considered to be triggers of hemolytic anemia in hemoglobin H disease. This can result in fatigue, paleness, and a yellow discoloration of the skin and whites of eyes called jaundice. Usually, the anemia is mild enough not to require treatment. Severe anemia events may require blood transfusion, however, and are usually accompanied by such other symptoms as dark feces or urine and abdominal or back pain. These events are uncommon in hemoglobin H disease, although they occur more frequently in a more serious type of hemoglobin H disease called hemoglobin H/Constant Spring disease. Individuals effected with this type of hemoglobin H disease are also more likely to have enlargement of and other problems with the spleen.\nAlpha thalassemia major\nBecause alpha globin is a necessary component of all major hemoglobins and some minor hemoglobins, absence of all functioning alpha globin genes leads to serious medical consequences that begin even before birth. Affected fetuses develop severe anemia as early as the first trimester of pregnancy. The placenta, heart, liver, spleen, and adrenal glands may all become enlarged. Fluid can begin collecting throughout the body as early as the start of the second trimester, causing damage to developing tissues and organs. Growth retardation is also common. Affected fetuses usually miscarry or die shortly after birth. In addition, women carrying affected fetuses are at increased risk of developing complications of pregnancy and delivery. Up to 80% of such women develop toxemia, a disturbance of metabolism that can potentially lead to convulsions and coma. Other maternal complications include premature delivery and increased rates of delivery by cesarean section, as well as hemorrhage after delivery.\nThalassemia may be suspected if an individual shows signs that are suggestive of the disease. In all cases, however, laboratory diagnosis is essential to confirm the exact diagnosis and to allow for the provision of accurate genetic counseling about recurrence risks and testing options for parents and affected individuals. Screening is likewise recommended to determine trait status for individuals of high-risk ethnic groups.\nThe following tests are used to screen for thalassemia disease and/or trait:\nhemoglobin electrophoresis with quantitative hemoglobin A2 and hemoglobin F\nfree erythrocyte-protoporphyrin (or ferritin or other studies of serum iron levels)\nA complete blood count will identify low levels of hemoglobin, small red blood cells, and other red blood cell abnormalities that are characteristic of a thalassemia diagnosis. Since thalassemia trait can sometimes be difficult to distinguish from iron deficiency, tests to evaluate iron levels are important. A hemoglobin electrophoresis is a test that can help identify the types and quantities of hemoglobin made by an individual. This test uses an electric field applied across a slab of gel-like material. Hemoglobins migrate through this gel at various rates and to specific locations, depending on their size, shape, and electrical charge. Isoelectric focusing and high-performance liquid chromatography (HPLC) use similar principles to separate hemoglobins and can be used instead of or in various combinations with hemoglobin electrophoresis to determine the types and quantities of hemoglobin present. Hemoglobin electrophoresis results are usually within the normal range for all types of alpha thalassemia. However, hemoglobin A2 levels and sometimes hemoglobin F levels are elevated when beta thalassemia disease or trait is present. Hemoglobin electrophoresis can also detect structurally abnormal hemoglobins that may be co-inherited with a thalassemia trait to cause thalassemia disease (i.e., hemoglobin E) or other types of hemoglobin disease (i.e., sickle hemoglobin). Sometimes DNA testing is needed in addition to the above screening tests. This can be performed to help confirm the diagnosis and establish the exact genetic type of thalassemia.\nDiagnosis of thalassemia can occur under various circumstances and at various ages. Several states offer thalassemia screening as part of the usual battery of blood tests done for newborns. This allows for early identification and treatment. Thalassemia can be identified before birth through the use of prenatal diagnosis. Chorionic villus sampling (CVS) can be offered as early as 10 weeks of pregnancy and involves removing a sample of the placenta made by the baby and testing the cells. CVS carries a risk of causing a miscarriage that is between 0.5%-1%. Amniocentesis is generally offered between 15 and 22 weeks of pregnancy, but can sometimes be offered earlier. Two to three tablespoons of the fluid surrounding the baby is removed. This fluid contains fetal cells that can be tested. The risk of miscarriage associated with amniocentesis ranges from 0.33-0.5%. Pregnant woman and couples may choose prenatal testing in order to prepare for the birth of a baby that may have thalassemia. Alternately, knowing the diagnosis during pregnancy allows for the option of pregnancy termination. Preimplantation genetic diagnosis (PGD) is a relatively new technique that involves in-vitro fertilization followed by genetic testing of one cell from each developing embryo. Only the embryos unaffected by sickle cell disease are transferred back into the uterus. PGD is currently available on a research basis only and is relatively expensive.\nIndividuals with beta thalassemia major receive regular blood transfusions, usually on a monthly basis. This helps prevent severe anemia and allows for more normal growth and development. Transfusion therapy does have limitations, however. Individuals can develop reactions to certain proteins in the blood—called a transfusion reaction. This can make locating appropriately matched donor blood more difficult. Although blood supplies in the United States are very safe, particularly relative to the past and to other areas of the world, there remains an increased risk of exposure to such blood-borne infections as hepatitis. Additionally, the body is not able to get rid of the excess iron that accompanies each transfusion. An additional medication called desferoxamine is administered, usually five nights per week over a period of several hours, using an automatic pump that can be used during sleep or taken anywhere the person goes. This medication is able to bind to the excess iron, which can then be eliminated through urine. If desferoxamine is not used regularly or is unavailable, iron overload can develop and cause tissue damage and organ damage and failure. The heart, liver, and endocrine organs are particularly vulnerable. Desferoxamine itself may rarely produce allergic or toxic side effects, including hearing damage. Signs of desferoxamine toxicity are screened for and generally develop in individuals who overuse the medication when body iron levels are sufficiently low. Overall, however, transfusion and desferoxamine therapy have increased the life expectancy of individuals with the most severe types of beta thalassemia major to the 4th or 5th decade. This can be expected to improve with time and increased developments in treatment, as well as for those with more mild forms of the disease.\nNew treatments offer additional options for some individuals with beta thalassemia major. There are various medications that target the production of red blood cells (i.e. erythropoeitin) or fetal hemoglobin (i.e. hydroxyurea and butyrate). Their effectiveness in ameliorating the severity of beta thalassemia is currently being investigated. Another promising new treatment is bone marrow transplantation, in which the bone marrow of an affected individual is replaced with the bone marrow of an unaffected donor. If successful, this treatment can provide a cure. However, there is an approximately 10-15% chance the procedure could be unsuccessful (i.e. the thalassemia returns); result in complications (i.e. graft-versus-host disease); or result in death. The risk for specific individuals depends on current health status, age, and other factors. Because of the risks involved and the fact that beta thalassemia is a treatable condition, transplant physicians require a brother or sister donor who has an identically matched tissue type, called HLA type. HLA type refers to the unique set of proteins present on each individual's cells, which allows the immune system to recognize \"self\" from \"foreign.\" HLA type is genetically determined, so there is a 25% chance for two siblings to be a match. Transplant physicians and researchers are also investigating ways to improve the safety and effectiveness of bone marrow transplantation. Using newborn sibling umbilical cord blood—the blood from the placenta that is otherwise discarded after birth but contains cells that can go on to make bone marrow—seems to provide a safer and perhaps more effective source of donor cells. Donors and recipients may not have to be perfect HLA matches for a successful transplant using cord blood cells. Trials are also underway to determine the effectiveness of \"partial transplants,\" in which a safer transplant procedure is used to replace only a percentage of the affected individual's bone marrow. Other possible treatments on the horizon may include gene therapy techniques aimed at increasing the amount of normal hemoglobin the body is able to make.\nHemoglobin H disease is a relatively mild form of thalassemia that may go unrecognized. It is not generally considered a condition that will reduce one's life expectancy. Education is an important part of managing the health of an individual with hemoglobin H disease. It is important to be able to recognize the signs of severe anemia that require medical attention. It is also important to be aware of the medications, chemicals, and other exposures to avoid due to the theoretical risk they pose of causing a severe anemia event. When severe anemia occurs, it is treated with blood transfusion therapy. For individuals with hemoglobin H disease, this is rarely required. For those with the hemoglobin H/Constant Spring form of the disease, the need for transfusions may be intermittent or ongoing, perhaps on a monthly basis and requiring desferoxamine treatment. Individuals with this more severe form of the disease may also have an increased chance of requiring removal of an enlarged and/or overactive spleen.\nAnemia — A blood condition in which the level of hemoglobin or the number of red blood cells falls below normal values. Common symptoms include paleness, fatigue, and shortness of breath.\nBilirubin — A yellow pigment that is the end result of hemoglobin breakdown. This pigment is metabolized in the liver and excreted from the body through the bile. Bloodstream levels are normally low; however, extensive red cell destruction leads to excessive bilirubin formation and jaundice.\nBone marrow — A spongy tissue located in the hollow centers of certain bones, such as the skull and hip bones. Bone marrow is the site of blood cell generation.\nBone marrow transplantation — A medical procedure used to treat some diseases that arise from defective blood cell formation in the bone marrow. Healthy bone marrow is extracted from a donor to replace the marrow in an ailing individual. Proteins on the surface of bone marrow cells must be identical or very closely matched between a donor and the recipient.\nDesferoxamine — The primary drug used in iron chelation therapy. It aids in counteracting the life-threatening buildup of iron in the body associated with long-term blood transfusions.\nGlobin — One of the component protein molecules found in hemoglobin. Normal adult hemoglobin has a pair each of alpha-globin and beta-globin molecules.\nHeme — The iron-containing molecule in hemoglobin that serves as the site for oxygen binding.\nHemoglobin — Protein-iron compound in the blood that carries oxygen to the cells and carries carbon dioxide away from the cells.\nHemoglobin A — Normal adult hemoglobin that contains a heme molecule, two alpha-globin molecules, and two beta-globin molecules.\nHemoglobin electrophoresis — A laboratory test that separates molecules based on their size, shape, or electrical charge.\nHepatomegaly — An abnormally large liver.\nHLA type — Refers to the unique set of proteins called human leukocyte antigens. These proteins are present on each individual's cell and allow the immune system to recognize 'self' from 'foreign'. HLA type is particularly important in organ and tissue transplantation.\nHydroxyurea — A drug that has been shown to induce production of fetal hemoglobin. Fetal hemoglobin has a pair of gamma-globin molecules in place of the typical beta-globins of adult hemoglobin. Higher-than-normal levels of fetal hemoglobin can ameliorate some of the symptoms of thalassemia.\nIron overload — A side effect of frequent blood transfusions in which the body accumulates abnormally high levels of iron. Iron deposits can form in organs, particularly the heart, and cause life-threatening damage.\nJaundice — Yellowing of the skin or eyes due to excess of bilirubin in the blood.\nMutation — A permanent change in the genetic material that may alter a trait or characteristic of an individual, or manifest as disease, and can be transmitted to offspring.\nPlacenta — The organ responsible for oxygen and nutrition exchange between a pregnant mother and her developing baby.\nRed blood cell — Hemoglobin-containing blood cells that transport oxygen from the lungs to tissues. In the tissues, the red blood cells exchange their oxygen for carbon dioxide, which is brought back to the lungs to be exhaled.\nScreening — Process through which carriers of a trait may be identified within a population.\nSplenomegaly — Enlargement of the spleen.\nBecause alpha thalassemia major is most often a condition that is fatal in the prenatal or newborn period, treatment has previously been focused on identifying affected pregnancies in order to provide appropriate management to reduce potential maternal complications. Pregnancy termination provides one form of management. Increased prenatal surveillance and early treatment of maternal complications is an approach that is appropriate for mothers who wish to continue their pregnancy with the knowledge that the baby will most likely not survive. In recent years, there have been a handful of infants with this condition who have survived long-term. Most of these infants received experimental treatment including transfusions before birth, early delivery, and even bone marrow transplantation before birth, although the latter procedure has not yet been successful. For those infants that survive to delivery, there seems to be an increased risk of developmental problems and physical effects, particularly heart and genital malformations. Otherwise, their medical outlook is similar to a child with beta thalassemia major, with the important exception that ongoing, life-long blood transfusions begin right at birth.\nAs discussed above, the prognosis for individuals with the most serious types of thalassemia has improved drastically in the last several years following recent medical advances in transfusion, chemo-, and transplantation therapy. Advances continue and promise to improve the life expectancy and quality of life further for affected individuals.\n\"First Known Heart Attack Associated With Beta-thalassemia Major Reported.\" Heart Disease Weekly February 22, 2004: 10.\n\"Novel Alpha-thalassemia Mutations Identified.\" Hematology Week January 26, 2004: 19.\nChildren's Blood Foundation. 333 East 38th St., Room 830, New York, NY 10016-2745. (212) 297-4336. cfg@nyh.med.cornell.edu.\nCooley's Anemia Foundation, Inc. 129-09 26th Ave. #203, Flushing, NY 11354. (800) 522-7222 or (718) 321-2873. http://www.thalassemia.org.\nMarch of Dimes Birth Defects Foundation. 1275 Mamaroneck Ave., White Plains, NY 10605. (888) 663-4637. resourcecenter@modimes.org. http://www.modimes.org.\nNational Heart, Lung, and Blood Institute. PO Box 30105, Bethseda, MD 20824-0105. (301) 592-8573. nhlbiinfo@rover.nhlbi.nih.gov. http://www.nhlbi.nih.gov.\nNational Organization for Rare Disorders (NORD). PO Box 8923, New Fairfield, CT 06812-8923. (203) 746-6518 or (800) 999-6673. Fax: (203) 746-6481. http://www.rarediseases.org.\nBojanowski J. \"Alpha Thalassemia Major: The Possibility of Long-Term Survival.\" Pamphlet from the Northern California Comprehensive Thalassemia Center. (1999).\nChildren's Hospital Oakland, Northern California Comprehensive Thalassemia Center website. http://www.thalassemia.com.\nCooley's Anemia Foundation, Inc. website. http://www.thalassemia.org/gohome.html.\nJoint Center for Sickle Cell and Thalassemic Disorders website. http://cancer.mgh.harvard.edu/medOnc/sickle.htm.\n[thal″ah-se´me-ah]\na heterogeneous group of hereditary hemolytic anemias marked by a decreased rate of synthesis of one or more hemoglobin polypeptide chains, classified according to the chain involved (α, β, δ); the two major categories are α- and β-thalassemia.\nα-thalassemia (alpha-thalassemia) that caused by diminished synthesis of alpha chains of hemoglobin. The homozygous form is incompatible with life, the stillborn infant displaying severe hydrops fetalis. The heterozygous form may be asymptomatic or marked by mild anemia.\nβ-thalassemia (beta-thalassemia) that caused by diminished synthesis of beta chains of hemoglobin. The homozygous form is called t. major and the heterozygous form is called t. minor.\nthalassemia ma´jor the homozygous form of β-thalassemia, in which hemoglobin A is completely absent; it appears in the newborn period and is marked by hemolytic, hypochromic, microcytic anemia; hepatosplenomegaly; skeletal deformation; mongoloid facies; and cardiac enlargement.\nthalassemia mi´nor the heterozygous form of β-thalassemia; it is usually asymptomatic, but there may be mild anemia.\nsickle cell–thalassemia a hereditary anemia involving simultaneous heterozygosity for hemoglobin S and thalassemia.\nthal·as·se·mi·a\n, thalassanemia (thal'ă-sē'mē-ă, thă-las-ă-nē'mē-ă),\nAny of a group of inherited disorders of hemoglobin metabolism in which there is impaired synthesis of one or more of the polypeptide chains of globin; several genetic types exist, and the corresponding clinical picture may vary from barely detectable hematologic abnormality to severe and fatal anemia.\n[G. thalassa, the sea, + haima, blood]\n/thal·as·se·mia/ (thal″ah-se´me-ah) a heterogeneous group of hereditary hemolytic anemias marked by a decreased rate of synthesis of one or more hemoglobin polypeptide chains, classified according to the chain involved (α, β, δ); the two major categories are α- and β-thalassemia.\nα-thalassemia that caused by diminished synthesis of alpha chains of hemoglobin. The homozygous form is incompatible with life, the stillborn infant displaying severe hydrops fetalis. The heterozygous form may be asymptomatic or marked by mild anemia.\nβ-thalassemia that caused by diminished synthesis of beta chains of hemoglobin. The homozygous form is called t. major and the heterozygous form is called t. minor.\nthalassemia ma´jor the homozygous form of &#x03B2;, in which hemoglobin A is completely absent; it appears in the newborn period and is marked by hemolytic, hypochromic, microcytic anemia, hepatosplenomegaly, skeletal deformation, mongoloid facies, and cardiac enlargement.\nthalassemia mi´nor the heterozygous form of &#x03B2;, usually asymptomatic, although there is sometimes mild anemia.\n(thăl′ə-sē′mē-ə)\nAn inherited form of anemia occurring chiefly among people of Mediterranean descent, caused by faulty synthesis of part of the hemoglobin molecule. Also called Mediterranean anemia.\nthal′as·se′mic adj.\n[thal′əsē′mē·ə]\nEtymology: Gk, thalassa, sea, a + haima, without blood\nproduction and hemolytic anemia characterized by microcytic, hypochromic red blood cells. Thalassemia is caused by inherited deficiency of alpha- or beta-globin synthesis. See also hemochromatosis, hemosiderosis.\nBeta thalassemia, clinical thalassemia, Cooley's anemia, Mediterranean anemia, thalassemia major Hematology A group of genetic diseases by underproduction of hemoglobin due to mutations in the beta globin gene, which is more common in Mediterraneans Heredity Parents are carriers–heterozygotes; one in 4 children is homozygous for the mutation and thus has full-blown disease Clinical See Anemia. Cf Sickle cell anemia.\nα-thalassemia\nHemoglobin Barts Hematology An inherited condition caused by a defect in the synthesis of the Hb α chain; Hb Barts hemoglobinopathy is characterized by the presence of 4 gamma chains; it is more common in southeast Asians; the most severe form of alpha thalassemia causes stillbirth due to hydrops fetalis Heredity Parents are carriers–heterozygotes; one in 4 children is homozygous for the mutation and thus has full-blown disease Clinical Pallor, fatiguability, FTT, fever, infections, diarrhea Management Transfusions\nThalassemia major Hematology A hemoglobinopathy caused by a defect in the synthesis of Hb β chain Clinical Pallor, fatigability, FTT, fever due to infections, diarrhea, bone deformities, hepatosplenomegaly Management Transfusions, but iron overload can damage the heart, liver, and endocrine systems, ergo iron chelation–early use of deferiprone, deferoxamine ↓ transfusion-related iron overload and may protect against DM, cardiac disease, early death\nδ-thalassemia\nHematology A condition characterized by a defect of Hb A2–α2δ2; because Hb A2 comprises only 3% of the circulating Hb, even its complete absence; δ-thalassemia has little clinical or hematologic impact\nγ-thalassemia\nHematology A condition characterized by a defect of gamma–γ Hb chains found in Hb F–α2γ2; because Hb F is present primarily in the fetus and newborns, it is rarely seen outside of the neonatal period, but may cause transient neonatal hemolytic anemia.\n, thalassanemia (thal'ă-sē'mē-ă, -ă-să-nē'mē-ă)\nAny of a group of inherited disorders of hemoglobin metabolism in which there is impaired synthesis of one or more of the polypeptide chains of globin; several genetic types exist, and the corresponding clinical picture may vary from barely detectable hematologic abnormality to severe and fatal anemia. People of Mediterranean, extraction are more often affected than others by this type of anemia.\nSynonym(s): thalassaemia, thalassanaemia.\nAny of a group of inherited disorders of hemoglobin metabolism with impaired synthesis of one or more polypeptide chains of globin; several genetic types exist.\n<a href=\"https://medical-dictionary.thefreedictionary.com/Thalassaemia+minor\">thalassemia</a>\nBarts hemoglobin\nbeta hemoglobinopathy\nbeta-delta thalassemia\nbeta-thalassaemia\nBite Cell\nblack gallstone\nI know of a couple, totally unrelated and unbeknownst to them, who are silent carriers of Thalassaemia minor.\nPakistan: Genetic factor: All in the genes\nBut, unfortunately, when one person with thalassaemia minor carrier happens to marry another with the same diagnosis, there is a strong possibility that their child would be thalassaemia major, as happened in the case of Taneja.\n' My life depends upon a monthly blood transfusion '\n0] thalassaemia demonstrates variable severity, ranging from a condition similar to [beta] thalassaemia minor to something approaching thalassaemia major.\nA retrospective review of homozygous haemoglobin E patients\nThal, Alan P.\nthalame\nthalamencephalic\nthalamencephalon\nthalamic\nthalamic fasciculus\nthalamic nucleus\nthalamic pain syndrome\nthalamic peduncle\nthalamic radiation\nthalamo-\nthalamocoele\nthalamocortical\nthalamocortical fibers\nthalamogeniculate artery\nthalamolenticular\nthalamoperforating artery\nthalamostriate radiation\nthalamotuberal artery\nThalassaemia minor\nthalassaemiaor Cooley's disease\nthalassemic facies\nthalasso-\nThalassobacter\nThalassobacter utilis\nthalassoplankton\nthalassoposia\nthalidomide neuropathy\nThalidomider\nthallium poisoning\nThalarctos\nTHALAS\nThalasaemia\nThalassaemia Association of Malaysia\nthalassaemia major\nThalassaemias\nthalassaemic\nthalassanaemia\nThalassemia Action Group\nThalassemia Clinical Research Network\nthalassemia syndrome"
  },
  {
    "id": "71902c8027dca26b265f12709ec21224b5abe8b9ef5750fd",
    "content": "McPherson County (standard abbreviation: MP) is a county located in the U.S. state of Kansas.  As of the 2020 census, the county population was 30,223. The largest city and county seat is McPherson. The county is named for Civil War General James B. McPherson.\n\nHistory\n\nEarly history\n\nFor many millennia, the Great Plains of North America was inhabited by nomadic Native Americans. From the 16th century to 18th century, the Kingdom of France claimed ownership of large parts of North America. In 1762, after the French and Indian War, France secretly ceded New France to Spain, per the Treaty of Fontainebleau. In 1802, Spain returned most of the land to France, but keeping title to about 7,500 square miles.\n\nIn 1803, most of the land for modern day Kansas was acquired by the United States from France as part of the 828,000 square mile Louisiana Purchase for 2.83 cents per acre. In 1848, after the Mexican–American War, the Treaty of Guadalupe Hidalgo with Spain brought into the United States all or part of land for ten future states, including southwest Kansas. In 1854, the Kansas Territory was organized, then in 1861 Kansas became the 34th U.S. state.\n\n19th century\n\nFrom the 1820s to 1870s, the Santa Fe Trail passed through, what is now McPherson County. The trail entered the county, east of Canton, then south of Galva, then north of Inman, and west towards Lyons. In 1855, Charles O. Fuller established a ranch adjacent to the Running Turkey Creek Crossing about two miles south and one mile east of Galva. Fuller's Ranch provided accommodations for travelers on the Santa Fe Trail and was probably the first white settlement in McPherson County.\n\nPeketon County was established in 1860, by the passage of a bill by S. N. Wood:  An act to establish Peketon County. Section 1. - That all that territory west of the sixth principal meridian and south of Township 16, in Kansas Territory, be and the same is hereby erected into a county, to be known by the name of Peketon County. On February 17, 1865, Peketon County was abolished, and McPherson County was made a part of Marion County, which extended from the west line of Chase County to the present western boundary of Kansas.\n\nIn 1868, Solomon Stephens and L. N. Holmberg were appointed Justices of the Peace—the first officers in what is now McPherson County. The next year (1869) occurred the first election for the township, now the county of McPherson. McPherson was regularly organized as a county in the spring of 1870, a mass meeting being held at Sweadal. Sweadal, the county seat thus selected, was located about one mile and a half southwest of the present site of Lindsborg. In September, however, the County Commissioners resolved to meet at the latter place, McPherson which had already been located some two years.\n\nIn April, 1873, a petition was filed for the county seat re-location. It was signed by 483 voters, and a special election was accordingly ordered for June 10. Upon that day, McPherson received 605 votes, New Gottland 325, King City 3 and Lindsborg 1; McPherson's majority over all, 276. In May the McPherson Town Company had offered, as an inducement for the location of the county seat at this point, the free use of rooms for ten years, and the donation of two squares of land on the town site. The offer was accepted the next month, the County Commissioners selecting blocks 56 and 65. Thus the county seat was established at McPherson and has remained since.\n\nAs early as 1875, city leaders of Marion held a meeting to consider a branch railroad from Florence. In 1878, Atchison, Topeka and Santa Fe Railway and parties from Marion County and McPherson County chartered the Marion and McPherson Railway Company.  In 1879, a branch line was built from Florence to McPherson, in 1880 it was extended to Lyons, in 1881 it was extended to Ellinwood. The line was leased and operated by the Atchison, Topeka and Santa Fe Railway. The line from Florence to Marion, was abandoned in 1968. In 1992, the line from Marion to McPherson was sold to Central Kansas Railway. In 1993, after heavy flood damage, the line from Marion to McPherson was abandoned. The original branch line connected Florence, Marion, Canada, Hillsboro, Lehigh, Canton, Galva, McPherson, Conway, Windom, Little River, Mitchell, Lyons, Chase, then connected with the original AT&SF main line at Ellinwood.\n\nIn 1887, the Chicago, Kansas and Nebraska Railway extended its main line from Herington to Pratt.  This main line connected Herington, Ramona, Tampa, Durham, Waldeck, Canton, Galva, McPherson, Groveland, Inman, Medora, Hutchinson, Whiteside, Partridge, Arlington,  Langdon, Turon, Preston, Natrona, Pratt.  In 1888, this main line was extended to Liberal.  Later, this line was extended to Tucumcari, New Mexico and Santa Rosa, New Mexico, where it made a connection with the Southern Pacific from El Paso, Texas.  The Chicago, Kansas and Nebraska Railway was absorbed by the Chicago, Rock Island and Pacific Railway. This line is also called the \"Golden State Route\".\n\n20th century\nThe National Old Trails Road, also known as the Ocean-to-Ocean Highway, was established in 1912, and was routed through Windom, Conway, McPherson.\n\nGeography\n\nAccording to the U.S. Census Bureau, the county has a total area of , of which  is land and  (0.3%) is water.\n\nAdjacent counties\n Saline County (north)\n Dickinson County (northeast)\n Marion County (east)\n Harvey County (southeast)\n Reno County (southwest)\n Rice County (west)\n Ellsworth County (northwest)\n\nMajor highways\n  Interstate 135\n  U.S. Route 56\n  U.S. Route 81\n  K-4\n  K-61\n  K-153\n\nDemographics\n\nThe McPherson Micropolitan Statistical Area includes all of McPherson County.\n\n2000 census\nAs of the census of 2000, there were 29,554 people, 11,205 households, and 7,966 families residing in the county.  The population density was 33 people per square mile (13/km2).  There were 11,830 housing units at an average density of 13 per square mile (5/km2).  The racial makeup of the county was 96.53% White, 0.81% Black or African American, 0.34% Native American, 0.32% Asian, 0.06% Pacific Islander, 0.79% from other races, and 1.16% from two or more races.  1.94% of the population were Hispanic or Latino of any race. 37.1% were of German, 12.9% Swedish, 12.1% American, 6.7% English and 6.3% Irish ancestry according to Census 2000.\n\nThere were 11,205 households, out of which 33.00% had children under the age of 18 living with them, 62.50% were married couples living together, 6.00% had a female householder with no husband present, and 28.90% were non-families. 25.50% of all households were made up of individuals, and 11.80% had someone living alone who was 65 years of age or older.  The average household size was 2.49 and the average family size was 2.99.\n\nIn the county, the population was spread out, with 25.40% under the age of 18, 10.30% from 18 to 24, 25.20% from 25 to 44, 21.80% from 45 to 64, and 17.30% who were 65 years of age or older.  The median age was 38 years. For every 100 females there were 95.90 males.  For every 100 females age 18 and over, there were 92.90 males.\n\nThe median income for a household in the county was $41,138, and the median income for a family was $48,243. Males had a median income of $33,530 versus $21,175 for females. The per capita income for the county was $18,921.  About 4.20% of families and 6.60% of the population were below the poverty line, including 5.20% of those under age 18 and 8.10% of those age 65 or over.\n\nGovernment\n\nPresidential elections\nMcPherson county is often carried by Republican candidates. The last time a Democratic candidate has carried this county was in 1964 by Lyndon B. Johnson.\n\nLaws\nFollowing amendment to the Kansas Constitution in 1986, the county remained a prohibition, or \"dry\", county until 1996, when voters approved the sale of alcoholic liquor by the individual drink with a 30 percent food sales requirement.\n\nEducation\n\nColleges\n McPherson College in McPherson\n Bethany College in Lindsborg\n Central Christian College in McPherson\n\nUnified school districts\n Smoky Valley USD 400\n McPherson USD 418\n Canton-Galva USD 419\n Moundridge USD 423\n Inman USD 448\n\nSchool district office in neighboring county\n Goessel USD 411\n Little River-Windom USD 444\n\nMuseums\n Birger Sandzén Memorial Gallery in Lindsborg\n McCormick-Deering Days Museum in Inman\n McPherson Museum in McPherson\n Lindsborg Old Mill & Swedish Heritage Museum in Lindsborg\n Kansas Motorcycle Museum in Marquette\n\nCommunities\n\nCities\n\n Canton\n Galva\n Inman\n Lindsborg\n Marquette\n McPherson (county seat) \n Moundridge\n Windom\n\nUnincorporated communities\n† means a Census-Designated Place (CDP) by the United States Census Bureau.\n Conway\n Elyria†\n Groveland\n Johnstown\n New Gottland\n Roxbury†\n\nGhost towns\n Alta Mills\n Battle Hill\n Christian\n Doles Park\n Elivon\n King City\n Sweadal\n\nTownships\nMcPherson County is divided into twenty-five townships.  The cities of Lindsborg and McPherson are considered governmentally independent and are excluded from the census figures for the townships.  In the following table, the population center is the largest city (or cities) included in that township's population total, if it is of a significant size.\n\nSee also\n List of people from McPherson County, Kansas\n National Register of Historic Places listings in McPherson County, Kansas\n McPherson Valley Wetlands\n Maxwell Wildlife Refuge\n\nReferences\n\nNotes\n\nFurther reading\n\n Wheeler, Wayne Leland. \"An Analysis of Social Change in a Swedish-Immigrant Community: The Case of Lindsborg, Kansas.\" (PhD dissertation, University of Missouri-Columbia; ProQuest Dissertations Publishing, 1959. 5905657).\n\nCounty\n Through the Years: A Pictorial History of McPherson County; McPherson Sentinel' Heritage House Publishing Co; 1992.\n McPherson County First Courthouse Built About 1869 or 1870; Lindsborg News-Record; March 30, 1959.\n Pioneer Life and Lore of McPherson County, Kansas; Edna Nyquist; Democratic-Opinion Press; 1932.\n A History of the Church of the Brethren in Kansas (includes McPherson College history); Elmer LeRoy Craik; McPherson Daily; Republican Press; 397 pages; 1922.\n Portrait and Biographical Record of Dickinson, Saline, McPherson, and Marion Counties, Kansas; Chapman Bros; 614 pages; 1893.\n Standard Atlas of McPherson County, Kansas; Geo. A. Ogle & Co; 82 pages; 1921.\n Plat Book of McPherson County, Kansas; North West Publishing Co; 50 pages; 1903.\n Edwards' Atlas of McPherson County, Kansas; John P. Edwards; 51 pages; 1884.\n\nTrails\n The Story of the Marking of the Santa Fe Trail by the Daughters of the American Revolution in Kansas and the State of Kansas; Almira Cordry; Crane Co; 164 pages; 1915. (Download 4MB PDF eBook)\n The National Old Trails Road To Southern California, Part 1 (LA to KC); Automobile Club Of Southern California; 64 pages; 1916. (Download 6.8MB PDF eBook)\n\nMennonite Settlements\n Impact of Mennonite settlement on the cultural landscape of Kansas; Brenda Martin; Kansas State University; 1985/1988. \n Mennonite settlement : the relationship between the physical and cultural environment; Susan Movle; University of Utah; 1975/1886.\n Status of Mennonite women in Kansas in their church and home relationships; Eva Harshbarger; Bluffton College; 1925/1945.\n\nExternal links\n\nCounty\n \n McPherson County - Directory of Public Officials\nHistorical\n , from Hatteberg's People'' on KAKE TV news\nMaps\n McPherson County Maps: Current, Historic, KDOT\n Kansas Highway Maps: Current, Historic, KDOT\n Kansas Railroad Maps: Current, 1996, 1915, KDOT and Kansas Historical Society\n\n \nKansas counties\n1867 establishments in Kansas\n"
  },
  {
    "id": "2d48bbc2bd6052c5e67824752a9296cbbc26351616cd6d7e",
    "content": "Brooksley Elizabeth Born (born August 27, 1940) is an American attorney and former public official who, from August 26, 1996, to June 1, 1999, was chair of the Commodity Futures Trading Commission (CFTC), the federal agency which oversees the U.S. futures and commodity options markets. During her tenure on the CFTC, Born lobbied Congress and the President to give the CFTC oversight of off-exchange markets for derivatives, in addition to its role with respect to exchange-traded derivatives, but her warnings were ignored or dismissed, and her calls for reform resisted by other regulators.<ref name=\"nytimes\">Goodman, Peter S. The Reckoning - Taking Hard New Look at a Greenspan Legacy, The New York Times, October 9, 2008.</ref> Born resigned as chairperson on June 1, 1999, shortly after Congress passed legislation prohibiting her agency from regulating derivatives.\n\nIn 2009, Born received the John F. Kennedy Profiles in Courage Award, along with Sheila Bair of the Federal Deposit Insurance Corporation, in recognition of the \"political courage she demonstrated in sounding early warnings about conditions that contributed\" to the 2007-08 financial crisis.\n\nEarly life and education\nBorn graduated from Abraham Lincoln High School (San Francisco, California) at the age of 16. She then attended Stanford University, where she majored in English and was graduated with the class of 1961.  She initially wanted to become a doctor, but a guidance counsellor at Stanford advised her against medicine, so she majored in English literature instead.\n\nShe then attended Stanford Law School, one of only seven women in her class.  She was the first female student ever to be named president of the Stanford Law Review. She received the \"Outstanding Senior\" award and graduated as valedictorian of the class of 1964.\n\nLegal career\nImmediately after law school Born was selected as a law clerk to judge Henry Edgerton of the U.S. Court of Appeals for the District of Columbia Circuit. It was during this time that she met her first husband, Jacob C. Landau, who was a journalist covering the Federal courts at the time. Following her clerkship, she became an associate at the Washington, D.C.-based international law firm of Arnold & Porter. Born was attracted to Arnold & Porter because it was one of the few major law firms to have a woman partner at that time, Carolyn Agger, who was the head of the tax practice. Born took a two-year leave of absence from Arnold & Porter to accompany her first husband to Boston, where he had received a fellowship. During that time she worked as a research assistant to law professor Alan Dershowitz.\n\nBorn's early career at Arnold & Porter focused on international trade law, in which she represented a number of Swiss industries and the government of Switzerland. She developed a practice representing clients in numerous complex litigation and arbitration cases involving financial market transactions. Among her high-profile cases was the matter of the Hunt Brothers attempt to corner the market in silver in the 1970s. She made partner at Arnold & Porter, after moving to a three-day schedule to help raise her second child, and eventually rose to be the head of the firm's derivatives practice.\n\nBorn was among the first female attorneys to systematically address inequities regarding how the laws treated women. Born and another female lawyer, Marna Tucker, taught what is considered to have been the first \"Women and the Law\" course at Catholic University’s Columbus School of Law. The class exclusively concerned prejudicial treatment of women under the laws of the United States, past and present.  Born and Tucker were surprised to discover that there was no textbook on the issue at the time.  Born is also one of the co-founders of the National Women's Law Center. Born also helped rewrite the American Bar Association rules to make it possible for more women and minorities to sit on federal bench.\n\nDuring her long legal career, and into her retirement, Born did much pro bono and other types of volunteer work. She was active in the American Bar Association, the largest professional organization of lawyers in the United States.  Initially Born was named a member of the governing council of the ABA's Individual Rights Section, eventually becoming chairperson.  Born and Tucker founded the ABA Women's Caucus, the first organization of female lawyers in the ABA.  She held several other senior positions in the ABA, including being named the first woman member of the ABA's Standing Committee on the Federal Judiciary.  As a member of the Judiciary Committee, Born provided testimony and opinion on persons nominated for federal judgeships. In 1980 she was named chair of the committee.  As chair of the committee, Born was invited to address the U.S. Congress regarding the nomination of Judge Sandra Day O'Connor to the U.S. Supreme Court.\n\nIn 1993, Born's name was floated as a possible candidate for Attorney General of the United States, but Janet Reno was nominated.\n\nIn July 2009, Nancy Pelosi appointed Brooksley Born as a commissioner to the Financial Crisis Inquiry Commission (FCIC).\n\nBorn and the OTC derivatives market\nBorn was appointed to the CFTC on April 15, 1994, by President Bill Clinton. Due to litigation against Bankers Trust Company by Procter and Gamble and other corporate clients, Born and her team at the CFTC sought comments on the regulation of over-the-counter derivatives, a first step in the process of writing CFTC regulations to supplement the existing regulations of the Federal Reserve System,  the Options Clearing Corporation, and the National Association of Insurance Commissioners. Born was particularly concerned about swaps, financial instruments that are traded over the counter between banks, insurance companies or other funds or companies, and thus have no transparency except to the two counterparties and the counterparties' regulators, if any.  CFTC regulation was strenuously opposed by Federal Reserve chairman Alan Greenspan, and by Treasury Secretaries Robert Rubin and Lawrence Summers. On May 7, 1998, former SEC Chairman Arthur Levitt joined Rubin and Greenspan in objecting to the issuance of the CFTC's concept release. Their response dismissed Born's analysis and focused on the hypothetical possibility that CFTC regulation of swaps and other OTC derivative instruments could create a \"legal uncertainty\" regarding such financial instruments,  hypothetically reducing the value of the instruments. They argued that the imposition of regulatory costs would \"stifle financial innovation\" and encourage financial capital to transfer its  transactions offshore. The disagreement between Born and the Executive Office's top economic policy advisors has been described not only as a classic Washington turf war, but also a war of ideologies,  insofar as it is possible to argue that Born's actions were consistent with Keynesian and neoclassical economics while Greenspan, Rubin, Levitt, and Summers consistently espoused neoliberal, and neoconservative policies.\n\nIn 1998, a trillion-dollar hedge fund called Long Term Capital Management (LTCM) was near collapse.  Using mathematical models to calculate debt risk, LTCM used derivatives to leverage $5 billion into more than $1 trillion, doing business with fifteen of Wall Street's largest financial institutions.  The derivative transactions were not regulated, nor were investors able to evaluate LTCM's exposures.  Born stated, \"I thought that LTCM was exactly what I had been worried about\".  In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance.  After intervention by the Federal Reserve, the crisis was averted.  In congressional hearings into the crisis, Greenspan acknowledged that language had been introduced into an agriculture bill that would prevent CFTC from regulating the derivatives which were at the center of the crisis that threatened the US economy.  U.S. Representative Maurice Hinchey (D-NY) asked \"How many more failures do you think we'd have to have before some regulation in this area might be appropriate?\"  In response, Greenspan brushed aside the substance of Born's warnings with the simple assertion that \"the degree of supervision of regulation of the over-the-counter derivatives market is quite adequate to maintain a degree of stability in the system\". Born's warning was that there wasn't any regulation of them.  Born's chief of staff, Michael Greenberger summed up Greenspan's position this way: \"Greenspan didn't believe that fraud was something that needed to be enforced, and he assumed she probably did. And of course, she did.\"  Under heavy pressure from the financial lobby, legislation prohibiting regulation of derivatives by Born's agency was passed by the Congress.  Born resigned on June 1, 1999.\n\nThe derivatives market continued to grow yearly throughout both terms of George W. Bush's administration. On September 15, 2008, the bankruptcy of Lehman Brothers forced a broad recognition of a financial crisis in both the US and world capital markets.  As Lehman Brothers' failure temporarily reduced financial capital's confidence, a number of newspaper articles and television programs suggested that the failure's possible causes included the conflict between the CFTC and the other regulators.Faiola, Anthony, Nakashima, Ellen and Drew, Jill. The Crash: Risk and Regulation - What Went Wrong, The Washington Post, October 15, 2008.\n\nBorn declined to publicly comment on the unfolding 2008 crisis until March 2009, when she said: \"The market grew so enormously, with so little oversight and regulation, that it made the financial crisis much deeper and more pervasive than it otherwise would have been.\" She also lamented the influence of Wall Street lobbyists on the process and the refusal of regulators to discuss even modest reforms.\n\nAn October 2009 Frontline documentary titled \"The Warning\"  described Born's thwarted efforts to regulate and bring transparency to the derivatives market, and the continuing opposition thereto. The program concluded with an excerpted interview with Born sounding another warning: \"I think we will have continuing danger from these markets and that we will have repeats of the financial crisis -- may differ in details but there will be significant financial downturns and disasters attributed to this regulatory gap, over and over, until we learn from experience.\"\n\nIn 2009 Born, along with Sheila Bair of the FDIC, was awarded the John F. Kennedy Profiles in Courage Award in recognition of the \"political courage she demonstrated in sounding early warnings about conditions that contributed\" to the 2007-08 financial crisis.  According to Caroline Kennedy, \"Brooksley Born recognized that the financial security of all Americans was being put at risk by the greed, negligence and opposition of  powerful and well connected interests.... The catastrophic financial events of recent months have  proved them [Born and Sheila Bair] right.\"  One member of the President's working group had a change of heart about Brooksley Born.  SEC Chairman Arthur Levitt stated \"I've come to know her as one of the most capable, dedicated, intelligent and committed public servants that I have ever come to know\", adding that \"I could have done much better. I could have made a difference\" in response to her warnings.\n\nIn 2010, a documentary film Inside Job further alleged that derivatives regulation was ineffective from the Clinton administration on. Along with fellow whistleblower, former IMF Chief Economist Raghuram Rajan, who was also scorned by the economic establishment, Brooksley Born was cited as one of the authorities arguing that financial derivatives increase economic risk.\n\n Personal life \nBorn is married to Alexander E. Bennett (also retired from Arnold & Porter).  She has five adult children - two from a previous marriage to Jacob Landau and three stepchildren. Notably, Born was named a partner at Arnold & Porter while working part-time so she could raise her two young children.  When both of her children were school-age, Born returned to practice full-time.\n\nReferences\n\nExternal links\nAttorney profile at Arnold & Porter\nBrooksley Born (2009 Winner) of the Profiles in Courage Award, with acceptance speech transcript and NECN video\n\nProfile at MarketsWiki\nSpeeches and statements\n\"Testimony Of Brooksley Born Chairperson of the CFTC Concerning The Over-The-Counter Derivatives Market\", before the House Committee On Banking And Financial Services, July 24, 1998.\n\"The Lessons of Long Term Capital Management L.P.\", Remarks of Brooksley Born, Chairperson of the CFTC, Chicago-Kent-IIT Commodities Law Institute, Chicago, Illinois, October 15, 1998.\n Interview: Brooksley Born for \"PBS Frontline: The Warning\", PBS, (streaming VIDEO 1 hour), October 20, 2009.\nArticles\nManuel Roig-Franzia. \"Credit Crisis Cassandra:Brooksley Born's Unheeded Warning Is a Rueful Echo 10 Years On\", The Washington Post, May 26, 2009\n Taibbi, Matt. \"The Great American Bubble Machine\", Rolling Stone'', July 9–23, 2009\n\n1940 births\nAmerican women lawyers\nArnold & Porter people\nClinton administration personnel\nColumbus School of Law faculty\nCommodity Futures Trading Commission personnel\nHeads of United States federal agencies\nLawyers from San Francisco\nLiving people\nStanford Law School alumni\n21st-century American women\nStanford University alumni"
  },
  {
    "id": "47b9dbb6d85243ffdaf6d95e842986b30d49d95913f21b6a",
    "content": "Paper Info\n\nTitle: Interpretable reduced-order modeling with time-scale separation Interpretable reduced-order modeling with time-scale separation\nPublish Date: 7 March 2023\nAuthor List: Sebastian Kaltenbach (from CSE-Lab, ETH Zurich, Harvard SEAS), Phaedon-Stelios Koutsourelakis (from CSE-Lab, ETH Zurich, Harvard SEAS), Petros Koumoutsakos (from CSE-Lab, ETH Zurich, Harvard SEAS), Harvard Seas (from CSE-Lab, ETH Zurich, Harvard SEAS)\n\nFigure\n\nFIG. 5. Comparison between the phase-space of the reference solution (left) and the phase-space of the predictions\nFIG. 7. Comparison between predictions and reference solutions for a new initial condition fort = 1.25, 3.75, 7.5, 12.5, 20, 30  (from left to right and top to down).We note that with longer prediction time the uncertainty bounds increases.Despite the chaotic nature of the KS equation, the predictive posterior mean is close to the reference solution for t ≤ 12.5\n\nabstract\n\nPartial Differential Equations (PDEs) with high dimensionality are commonly encountered in computational physics and engineering. However, finding solutions for these PDEs can be computationally expensive, making model-order reduction crucial. We propose such a data-driven scheme that automates the identification of the time-scales involved and, can produce stable predictions forward in time as well as under different initial conditions not included in the training data.\nTo this end, we combine a non-linear autoencoder architecture with a time-continuous model for the latent dynamics in the complex space. It readily allows for the inclusion of sparse and irregularly sampled training data. The learned, latent dynamics are interpretable and reveal the different temporal scales involved.\nWe show that this data-driven scheme can automatically learn the independent processes that decompose a system of linear ODEs along the eigenvectors of the system's matrix. Apart from this, we demonstrate the applicability of the proposed framework in a hidden Markov Model and the (discretized) Kuramoto-Shivashinsky (KS) equation.\nAdditionally, we propose a probabilistic version, which captures predictive uncertainties and further improves upon the results of the deterministic framework.\n\nINTRODUCTION\n\nHigh-fidelity simulations of critical phenomena such as ocean dynamics and epidemics have become essential for decision-making. They are based on physically-motivated PDEs expressing system dynamics that span multiple spatiotemporal scales and which necessitate cumbersome computations . In recent years there is increased attention to the development of data-driven models that can accelerate the solution of these PDEs as well as reveal salient, lower-dimensional features that control the long-term evolution.\nIn most cases, data-driven reduced-order models are not interpretable. In particular, models based on neural networks despite good predictive capabilities , they offer a black-box description of the system dynamics. A possible remedy is applying a symbolic regression to the learned neural network representation , but this adds additional computational cost due to the two-step procedure.\nA number of frameworks such as SINDy allows to learn interpretable dynamics but it relies on the a-priori availability of lower-dimensional descriptors and of time-derivatives which can be very noisy for both simulation and experimental data. Other frameworks are tailored to specific problems such as molecular dynamics .\nHere, we present a framework that only needs the value of the observables, and not their derivatives, as training data and is capable of identifying interpretable latent dynamics. The deployment of interpretable latent dynamics ensures that conservation of important properties of that are reflected in the reduced-order model .\nThe present method is related to approaches based on the Koopman-operator extended Dynamic Mode Decomposition (eDMD) but uses continuous complex-valued latent space dynamics and only requires one scalar variable per latent dimension to describe the latent space dynamics. Therefore we do not have to enforce any parametrizations on the Koopman matrix .\nThe time-continuous formulation moreover allows to incorporate sparse and irregularly sampled training data and fast generation of predictions after the training phase. By using a complex-valued latent space we can also incorporate harmonic effects and reduce the number of latent variables needed. Linear and non-linear autoencoders are used to map the observed, high-dimensional time-series to the lower-dimensional, latent representation and we identify simultaneously the autoencoder as well as the latent dynamics by optimizing a combined loss function.\nHence the to tasks of dimensionality reduction and discovery of the reduced dynamics are unified while other frameworks treat the two parts separately . Apart from using an architecture based on autoencoders to identify the latent space, projection-based methods could also be employed . We are also proposing a probabilistic version of our algorithm ) that makes use of probabilistic Slow Feature Analysis .\nThis allows for a latent representation that arart from being time-continuous, can quantify the predictive uncertainty and hierarchically decompose the dynamics into their pertinent scales while promoting the discovery of slow processes that control the system's evolution over long time horizons. The rest of the paper is structured as follows: We introduce the methodological framework as well as algorithmic details in section II.\nParticular focus is paid on the interpretability of the inferred lower-dimensional dynamics. In section III we present three numerical illustrations, i.e. a system of linear ODEs, a hidden Markov Model and the discretized KS-equation. We then present in section IV the probabilistic extension of the framework and apply it to the KS-equation.\nWe conclude with a summary and a short discussion about possible next steps. We introduce the autoencoders deployed in this work, followed by the interpretable latent space dynamic and discuss the training process. We consider data from high-dimensional time series x n ∈ R f with n = 1, ..., T . We remark that the intervals between the different states do not need to be uniformly spaced.\n\nAutoencoder\n\nA core assumption of the method is that each high-dimensional state x n can be compressed to a lower-dimensional representation z n ∈ C c with c << f . We identify this lower-dimensional representation by an autoencoder consisiting of a parameterized encoder and decoder. The encoder maps the high-dimensional representation to the latent space as:\nThe latent space is complex-valued. The decoder reconstructs the high-dimensional representation based on the latent variables as: We denote the parameters of the encoder as well as the decoder by θ. As discussed later in Section II C, both set of parameters are optimized simultaneously during training and therefore there is no need for differentiating them.\n\nInterpretable Latent Space Dynamics\n\nWe employ a propagator in the latent space to capture the reduced-order dynamics of the system. In contrast to other time-extended variational autoencoder frameworks, our representation uses complex valued latent variables. In addition the latent variables are treated independently. The latter feature enables us to have an interpretable latent dynamics as well as a model that is especially suitable for being trained in the Small Data regime due to the small number of required parameters.\nThis is in contrast to temporal propagators such as LSTMs . For each dimension i of the latent variable z we are using the following continuous ODE in the complex plane: By solving this ODE, we can define the operator: Interpretable reduced-order modeling with time-scale separation Here, λ is a vector containing all the individual λ's and ∆t n indicates the time-step between the latent states.\nThe symbol is used to indicate a component-wise multiplication. We remark that the latent variables and the parameter governing the temporal evolution are complex numbers and their role in describing the system dynamics is similar to that of an eigenvalue. The real part is associated with growth and decay whereas the imaginary part is representing the periodic component.\nThis approach has similarities with the Koopman-operator based methods and the extended dynamic mode decomposition . In contrast to the methods mentioned before we are using a continuous formulation in the latent space that allows us to incorporate scarce and irregularly sampled training data and directly rely on complex numbers in the latent space.\n\nTraining and Predictions\n\nWe optimize a loss function that combines both a reconstruction loss as well as a loss associated with the error of our learned propagator in the latent space: (5) We note that we could directly incorporate mini-batch training by only taking the summation over a subset of the N available training data.\nFor new predictions of unseen states, we use the encoder to generate a latent representations which is then advanced in time by the learned propagator. At a designated time step we are using the decoder to reconstruct the high-dimensional solution. We applied our algorithm to three systems. First, we show that the algorithm is capable of exactly reproducing the solution of a linear ODE and to identify its eigenvalues.\nAfterwards we are applying the framework to a high-dimensional process generated by a complex latent dynamics, which is correctly identified. As a final test case, we are applying the algorithm to a Kuramoto Shivashinski (KS) equation. Interpretable reduced-order modeling with time-scale separation\n\nLinear ODE\n\nWe are considering a two-dimensional ODE system for x = y 1 y 2 : Based on the obtained training data we run our algorithm using a linear encoder and decoder structure as well as two latent variables z. The loss function was optimized using the Adam algorithm . As we consider a linear ODE we can analytically compute the eigenvalues involved and compare it with the parameters λ identified by our algorithm.\nWe observe in Figure that the algorithm was able to recover the correct values, i.e. the eigenvalues 7 and 3 of the given linear ODE. The system does not have a periodic component and the two imaginary parts correctly go to zero, whereas the real parts converge to the reference value. Moreover we are also able to identify for the linear mapping between our latent variables z and the training data a matrix consisting of a multiple of the eigenvectors (1,1) and (1,-1) and thus the correct solution.\nThis example was chosen to show that the algorithm is able to quickly identify the exact solution of a linear ODE in terms of its linearly independent components.\n\nHidden multiscale dynamics\n\nWe consider eight-dimensional synthetic time series data produced by an underlying twodimensional complex valued process. In particular, the data points x are generated by first solving for the temporal evolution for the two complex-valued processes p 1 and p 2 and than mapping to the eight-dimensional space by using a randomly sampled linear mapping W .\nOne of the two processes used to generate the data is chosen to be much slower than the other one and both processes have a periodic component. dp 2 dt = (−0.9 + 1.5i)p 2 (8) As training data we consider 40 time series with 150 data points each, obtained by simulating the described processes for a maximum of t = 15 s and then sampling from the obtained data points.\nHence the training data consists of: • 40 time-series • with each consisting 150 observations of the x at a uniform time-step ∆t = 0.0025 The autoencoder obtained consists of one linear layer for both the decoder as well as the encoder. The model is trained for 5000 iterations using the Adam optimizer and a learning rate of 10 −3 .\nThe results for the convergence of the parameters λ 1 and λ 2 can be found in Figure . We note that the process which is slower decaying and thus more responsible for the long-term evolution of the system has a higher convergence rate than the faster process. With the obtained parameters λ as well as the trained autoencoder, we compute predictions based on the last time step used for training, i.e. we apply the encoder to obtain our latent representation and than use the latent dynamics to advance the latent representation in time.\nAfterwards, we employ the decoder to reconstruct the full high-dimensional system. The results can be found in Figure and show very good agreement between predictions and reference data. This example shows that our model is successfully able to carry out dimensionality reduction and moreover indicates that the convergence rate between latent processes can be different.\nThe latter is relevant when training models as for accurate predictions all latent processes and their dynamics should be converged.\n\nKuramoto-Sivashinsky\n\nFinally, we applied our algorithm to the KS equation and aim to identify a reduced-order model for the solution u(y, t): We employed periodic boundary conditions, µ = 1 and a domain size y ∈ [0, 22]. For this domain-size, the KS-equation exhibits a structurally stable chaotic attractor as discussed in The black lines divides the area for which training data was given from the area without raining data.\n; . The equation is discretized in space using a discretization step of 22 64 resulting in a state vector x of dimension 64 and a nonlinear system of coupled ODEs. This is solved using a stiff fourth-order solver  We employed a non-linear encoder and decoder with four fully-connected layers each and ReLU-activation functions as well as Dropout Layers between the fully-connected layers.\nWe trained the model for 200000 iterations using Adam and a learning rate of 5 • 10 4 and assuming a five-dimensional latent space. We obtained the λ's in Figure . Four latent variables have λ's close to zero and thus a slow temporal dynamic that is responsible for the long-term evolution whereas one latent variable is quickly decaying.\nBased on the obtained parameters, we do predictions based on an unseen initial condition not contained in the training data. We are able to reconstruct the correct phase space based on our predictions despite only using a very limited amount of training data. The results for the phase space can be seen in Figure .\nAlthough the small-scale fluctuations in the temporal dynamics are not well captured, the model identifies the correct manifold which has a good accuracy compared to the reference solution. All phase-spaces were obtained by using a finite-difference operator on the data or predictions. These results are in accordance Interpretable reduced-order modeling with time-scale separation with whose LSTM-based temporal dynamic model was also able to find the correct phase space but not to track the actual dynamics for long-term predictions.\nOur model is not able to account for noise in the temporal evolution and thus dealing with chaotic, small-scale fluctuations is challenging. We believe that a probabilistic version of our algorithm could be advantageous here. This section contains a fully probabilistic formulation for the deterministic model discussed before.\nWe replace the Autoencoder with a Variational Autoencoder and the ODE in the latent space with a SDE. The loss function which we optimize is the Evidence-Lower Bound (ELBO).\n\nModel Structure\n\nWe postulate the following relations for our probabilistic model using an Ornstein-Uhlenbeck (OU) for each dimension i of the latent space and a Wiener process W t in the latent space: We again assume that the latent variables z t are complex-valued and a priori independent. Complex variables were chosen as their evolution includes a harmonic components which are observed in many physical systems.\nWe assume an initial conditions z 0,i ∼ CN (0, σ 2 0,i ). The total parameters associated with the latent space dynamics of our model are thus {σ 2 0,i , σ 2 i , λ i } c i=1 and will be denoted by θ together with all parameters responsible for the decoder mapping G (see next section). These parameters along with the state variables z t have to be inferred from the data x t .\nBased on probabilistic Slow Feature Analysis (SFA) , we set σ 2 i = 2; (λ j ) and σ 2 0,i = 1. As a consequence, a priori, the latent dynamics are stationary. A derivation and reasoning for this choice can be found in Appendix A. Hence the only independent parameters are the λ i , the imaginary part of which can account for periodic effects in the latent dynamics.\n\nVariational Autoencoder\n\nWe employ a variational autoencoder to account for a probabilistic mappings from the lower-dimensional representation z n to the high-dimensional system x n . In particular we are employing a probabilistic decoder The encoder is used to infer the state variables z based on the given data and thus defined in the inference and learning section.\n\nInference and Learning\n\nGiven the probabilistic relations , our goal is to infer the latent variables z 0:T as well as all model parameters θ. We follow a hybrid Bayesian approach in which the posterior of the state variables is approximated using amortized Variational Inference and Maximum-A-Posteriori (MAP) point-estimates for θ are computed.\nThe application of Bayes' rule for each data sequence x 0:T leads to the following posterior: where p(θ) denotes the prior on the model parameters. In the context of variational inference, we use the following factorization of the approximate posterior i.e. we infer only the mean µ and variance σ for each state variable based on the given data points.\nThis conditional density used for inference is the encoder-counterpart to the probabilistic decoder defined in the section before. It can be readily shown that the optimal parameter values are found by maximizing the Evidence Lower Bound (ELBO) F(q φ (z 0:T ), θ) which is derived in Appendix B. We compute Monte Carlo estimates of the gradient of the ELBO with respect to φ and θ with the help of the reparametrization trick and carry out stochastic optimization with the ADAM algorithm .\n\nResults for the probabilistic extension\n\nWe applied our probabilistic version to the KS-equation. We used the same settings as for the deterministic approach but considered up to 10 complex latent variables. The obtained λ's are in Figure . The probabilistic model allows us to quantify the uncertainty in predictions. In Figure predictions for various time-steps and the respective uncertainty bounds are shown for an unseen initial condition.\nDue to the chaotic nature of the KS-equation and the small amount of training data, the underlying linear dynamic of our model is only able to capture the full dynamics for a limited time horizon. Fortunately, due to the probabilistic approach the model is capable of capturing chaotic fluctuations with increasingly wide uncertainty bounds.\nWe also computed the phase space representation for the KS-equation based on the predictions obtained by our model and compare it with the reference solution. The probabilistic model identifies the correct manifold with a better accuracy than the deterministic model. As some of the small-scale fluctuations are accounted as noise, the resulting manifold is more concentrated at the origin and the obtained values are slightly smaller than the reference manifold although their shape is very similar."
  },
  {
    "id": "94ccf4d54b36e7e9d4aa594a7bc426528630a5b8aff296a4",
    "content": "Sir Simon William English  (born 30 December 1961) is a New Zealand former National Party politician who served as the 39th prime minister of New Zealand from 2016 to 2017. He had previously served as the 17th deputy prime minister of New Zealand and minister of finance from 2008 to 2016 under John Key and the Fifth National Government.\n\nA farmer and public servant before entering politics, English was elected to the New Zealand Parliament in  as the National Party's candidate in the Wallace electorate. He was elevated to Cabinet in 1996 and in 1999 was made minister of finance, although he served for less than a year due to his party's loss at the 1999 general election. In October 2001, English replaced Jenny Shipley as the leader of the National Party (and consequently as Leader of the Opposition). He led the party to its worst defeat at the 2002 general election, and as a consequence, in October 2003 he was replaced as leader by Don Brash.\n\nIn November 2006, after Brash's resignation, English became deputy leader under John Key. After National's victory at the 2008 general election, he became deputy prime minister and was also made minister of finance for the second time. Under English's direction New Zealand's economy maintained steady growth during National's three terms of government. He became a list-only MP after stepping down as an electorate MP at the 2014 general election.\n\nJohn Key resigned as leader of the National Party and prime minister in December 2016. English won the resulting leadership election unopposed and was sworn in as prime minister on 12 December 2016. His tenure was only ten months, and included a three-month election campaign. In the 2017 general election, National won the largest number of seats but fell short of a majority. The parties holding the balance of power declined to support the existing government, and English was subsequently replaced as prime minister by Jacinda Ardern, leader of the Labour Party. English initially continued on as Leader of the Opposition, but resigned as leader of the National Party on 27 February 2018 and left parliament two weeks later.\n\nEarly life\nEnglish was born on 30 December 1961 at Lumsden Maternity Centre in Lumsden. He is the eleventh of twelve children of Mervyn English and Norah (née O'Brien) English. His parents purchased Rosedale, a mixed sheep and cropping farm in Dipton, Southland from Mervyn's uncle, Vincent English, a bachelor, in 1944. English was born in the maternity unit at Lumsden.\n\nEnglish attended St Thomas's School in Winton, then boarded at St. Patrick's College in Upper Hutt, where he became head boy. He played in the first XV of the school's rugby team. English went on to study commerce at the University of Otago, where he was a resident at Selwyn College, and then completed an honours degree in English literature at Victoria University of Wellington.\n\nAfter finishing his studies, English returned to Dipton and farmed for a few years. From 1987 to 1989, he worked in Wellington as a policy analyst for the New Zealand Treasury, at a time when the free market policies favoured by Labour's finance minister Roger Douglas (known collectively as \"Rogernomics\") were being implemented.\n\nEnglish joined the National Party in 1980, while at Victoria University. He served for a period as chairman of the Southland branch of the Young Nationals, and became a member of the Wallace electorate committee. After moving to Wellington, he served for periods on the Island Bay and Miramar electorate committees, respectively.\n\nFourth National Government (1990–1999)\n\nAt the 1990 general election, English stood as the National candidate in Wallace, replacing the retiring Derek Angus, and was elected with a large majority. He would hold this seat, renamed Clutha-Southland in 1996, until 2014. He and three other newly elected National MPs (Tony Ryall, Nick Smith, and Roger Sowry) were soon identified as rising stars in New Zealand politics, and at various points were dubbed the \"brat pack\", the \"gang of four\", and the \"young Turks\". In his first term in parliament, English chaired a select committee into social services. He was made a parliamentary under-secretary in 1993, serving under the Minister of Health.\n\nFirst period in cabinet (1996–1999)\nIn early 1996, English was elevated to cabinet by Prime Minister Jim Bolger, becoming the Minister for Crown Health Enterprises and Associate Minister of Education (to Wyatt Creech). He was 34 at the time, becoming the cabinet's youngest member. After the 1996 general election, the National Party was forced into a coalition with New Zealand First to retain government. In the resulting cabinet reshuffle, English emerged as Minister of Health. However, as a condition of the coalition agreement, NZ First's Neil Kirton (a first-term MP) was made Associate Minister of Health, effectively becoming English's deputy. This arrangement was described in the press as a \"shotgun marriage\", and there were frequent differences of opinion between the two ministers. After their relationship became unworkable, Kirton was sacked from the role in August 1997, with the agreement of NZ First leader Winston Peters.\n\nAs Minister of Health, English was responsible for continuing the reforms to the public health system that National had begun after the 1990 general election. The reforms were unpopular, and health was perceived as one of the government's weaknesses, with the health portfolio consequently being viewed as a challenge. English believed that the unpopularity of the reforms was in part due to a failure in messaging, and encouraged his National colleagues to avoid bureaucratic and money-focused language (such as references to \"balance sheets\" and \"user charges\") and instead talk about the improvements to services the government's reforms would bring. He also rejected the idea that public hospitals could be run as commercial enterprises, a view which some of his colleagues had previously promoted.\n\nBy early 1997, as dissatisfaction with Bolger's leadership began to grow, English was being touted as a potential successor, along with Jenny Shipley and Doug Graham. His age (35) was viewed as the main impediment to a successful leadership run. National's leadership troubles were resolved in December 1997, when Bolger resigned and Shipley was elected to the leadership unopposed. English had been a supporter of Bolger as leader, but Shipley reappointed him Minister of Health in her new cabinet.\n\nEnglish was promoted to Minister of Finance in a reshuffle in January 1999, a position which was at the time subordinate to the Treasurer, Bill Birch. After a few months, the pair switched positions as part of Birch's transition to retirement, with English assuming the senior portfolio. In early interviews, he emphasised his wish to be seen as a pragmatist rather than an ideologue, and said that the initiatives of some of his predecessors (Roger Douglas's \"Rogernomics\" and Ruth Richardson's \"Ruthanasia\") had focused on \"fruitless, theoretical debates\" when \"people just want to see problems solved\".\n\nOpposition (1999–2008)\n\nAfter the National Party lost the 1999 election to Helen Clark's Labour Party, English continued on in the shadow cabinet as National's spokesperson for finance. He was elected deputy leader of the party in February 2001, following the resignation of Wyatt Creech, with Gerry Brownlee being his unsuccessful opponent.\n\nLeader of the Opposition\nIn October 2001, after months of speculation, Jenny Shipley resigned as leader of the National Party after being told she no longer had the support of the party caucus. English was elected as her replacement unopposed (with Roger Sowry as his deputy), and consequently became Leader of the Opposition. However, he did not openly organise against Shipley, and according to The Southland Times \"there was almost an element of 'aw, shucks, I'll do it then' about Mr English's ascension\".\n\nAged 39 when he was elected, English became the second-youngest leader in the National Party's history, after Jim McLay (who was 38 when elected in 1984). He also became only the third Southlander to lead a major New Zealand political party, after Joseph Ward and Adam Hamilton. However, English failed to improve the party's performance. In the 2002 election, National suffered its worst electoral defeat ever, gaining barely more than twenty percent of the vote. English described it as \"the worst day of my political life\". Both party insiders and the general public were split as to how much to blame him for the loss, but most of the party believed that English would be able to rebuild National's support.\n\nBy late 2003, however, National's performance in opinion polls remained poor. The party had briefly increased its popularity in the year following the election, but by October its support had fallen to levels only slightly better than what it achieved in the last ballot. English also appeared in a boxing match for a charity against entertainer Ted Clarke. This did not boost his polling or that of the National party either, with suggestions that it devalued his image as a serious politician. Don Brash, former governor of the Reserve Bank and a relative newcomer to politics, began to build up support to replace English. On 28 October, Brash gained sufficient backing in Caucus to defeat English in a leadership contest.\n\nShadow cabinet roles and deputy leader\nOn 2 November 2003, when Brash changed responsibilities for certain MPs, English became National's spokesman for education, ranked at fifth place in the party's parliamentary hierarchy. He remained in parliament after the 2005 election. In his new shadow education portfolio, English performed strongly, and remained a party favourite despite his election defeat as leader in 2002, eventually being returned to the finance portfolio in August 2004 as deputy spokesman (while still retaining responsibility for education).\n\nIn November 2006, Brash resigned as leader. English was considered as a potential replacement leader (running against John Key) or deputy leader (against incumbent Gerry Brownlee) in the ensuing leadership election. However, a contest was avoided when the MPs agreed a Key/English ticket would run unopposed in a display of party unity. English took over the deputy leadership and the finance portfolio in the Key shadow cabinet.\n\nFifth National Government (2008–2017)\n\nDeputy Prime Minister and Minister of Finance (2008–2016)\n\nAt the 2008 election, English was re-elected by his electorate, winning by a margin of about 15,500 votes. He became Deputy Prime Minister of New Zealand and Minister of Finance in the fifth National Government, being sworn into office on 19 November 2008 and continued to serve in those roles until becoming Prime Minister on 12 December 2014. He was also made Minister of Infrastructure in National's first term of government and Minister responsible for Housing New Zealand Corporation and minister responsible for the New Zealand flag consideration process in its third.\n\nHe was comfortably re-elected in Clutha-Southland in the 2011 election but opted to run as a party-list candidate in 2014. \n\nThe pairing of John Key as leader of the National Party and English as his deputy has been compared to that of Bob Hawke and Paul Keating (in Australia) and Tony Blair and Gordon Brown (in the UK).\n\nEnglish acceded to the role of Finance Minister in the continuing wake of the financial crisis. In response to New Zealand's rising debt, English made budget deficit-reduction his main priority. His first budget outlined three focuses in New Zealand's financial recovery: \"improving the business environment and removing roadblocks to growth; investment in productive infrastructure; and improving the way government works\". One of his first acts was creating the National Infrastructure Unit, charged with formulating a plan for infrastructure projects and investments. He commissioned a government-wide spending review, with an aim to reducing government expenditure—with the exceptions of a two-year stimulus package and long-term increases on infrastructure spending.\n\nIn April 2011, the Opposition criticised English for suggesting that New Zealand businesses could use New Zealand's low wages to help it compete with Australia. The National Government campaigned for re-election in 2011 on its economic record. The Government boasted growth for five consecutive quarters up to mid-2010, totalling 1.6% of real GDP.\n\nStrong growth resulted in a surplus of $473 million for the 2015/16 financial year, projected to rise to $8.5 billion by 2020/21. In his 2016 Economic and Fiscal Update address, English stated that reducing debt and tackling the costs of the 2016 Kaikōura earthquake were higher priorities than reducing rates of tax.\n\nAllowances issue\nIn 2009, the media, including TVNZ and TV3 revealed that English was receiving about NZ$900 a week as part of a living allowance for ministers, to live in his own NZ$1.2 million Wellington home. At the time, English also received $276,200 in his annual salary as Deputy Prime Minister. It was also revealed other ministers with homes in the capital city were also claiming accommodation allowances. On 3 August 2009, Prime Minister John Key started a review of the housing allowances claimed by cabinet ministers. English subsequently paid back $12,000 and only claimed about $24,000 a year in living allowances. The Auditor-General's office said in September 2009 that they were making \"preliminary enquiries\" into parliamentary housing expenses in response to a letter of complaint from Progressive party leader Jim Anderton. Two days later English stated that he would no longer take up any housing allowance and had paid back all the allowance he had received since the November 2008 election.\n\nPrime Minister (2016–2017)\n\nJohn Key resigned on 12 December, and endorsed English as his successor in the resulting leadership election. Following the drop-out of both Judith Collins and Jonathan Coleman from the leadership election, English was sworn in as the 39th Prime Minister of New Zealand on 12 December 2016.\n\nEnglish appointed his first cabinet on 18 December. In a reshuffle, he appointed Steven Joyce to succeed him as Finance Minister, while most ministerial portfolios remained the same.\n\nIn February 2017, English did not attend Waitangi Day commemorations at the historic treaty grounds, reportedly in response to the Ngāpuhi iwi's decision to stop the Prime Minister from speaking at the marae. Ngāpuhi have protested the Government's negotiation of the Trans Pacific Partnership Agreement (TPPA), which the iwi believe infringes upon Māori sovereignty, and thus does not adhere to the Treaty of Waitangi. English had been invited to attend in an official capacity; his non-attendance was criticised by a Ngāpuhi elder and Opposition leader Andrew Little.\n\nIn his first overseas trip as Prime Minister, English travelled to Europe to discuss trade ties, including a prospective New Zealand–European Union free trade agreement. He first travelled to London on 13 January 2017 to meet British Prime Minister Theresa May. Discussing trade relations, English said the two nations were \"natural partners\" and would \"continue to forge ties\" after the UK's withdrawal from the EU. He also arranged to meet with London Mayor Sadiq Khan, Belgian Prime Minister Charles Michel and German Chancellor Angela Merkel.  In a meeting with Merkel, English received crucial backing from Germany for a trade deal with the EU. On 16 January, English stated that his government would continue to promote TPPA, despite the United States' decision to withdraw from the agreement. He explained that Southeast Asian countries would now be treated as a priority in negotiations—he also asserted that the United States was ceding influence to China by its rejection of the trade pact.\n\nAt a press conference at the Beehive on 1 February 2017, English announced that the 2017 general election would be held on 23 September. The Prime Minister later confirmed that his party would approach ACT, United Future and the Māori Party if confidence and supply agreements were required to form a government following the election. In his second cabinet reshuffle on 24 April, English appointed Gerry Brownlee as his new Foreign Affairs Minister; he also promoted Nikki Kaye to the portfolio of Education Minister, and moved Mark Mitchell into the cabinet to become Defence Minister. The reshuffle was perceived as an election preparation.\n\nOn 13 February 2017, English welcomed Australian Prime Minister Malcolm Turnbull to Wellington. The two leaders reaffirmed their shared trade agenda, and discussed changes to the Australian citizenship pathway which will affect permanent residents originating from New Zealand.\n\nOn 19 June, it was reported that Todd Barclay, who succeeded English as MP for Clutha-Southland, had clandestinely recorded one of his employee's conversations the previous year, and that John Key's leaders' budget was used to pay a confidential settlement after the employee resigned. English admitted that he had been aware of the illegal recording and the settlement, and thus implicated in the scandal.\n\nDuring the 2017 National campaign launch, English introduced a $379 million social investment package including digital learning academies for high school students, more resources for mathematics, and boosting support for teaching second languages in schools, and maintaining National Standards in the school curriculum. Prime Minister English also sought to defend National's financial management and economic track record and claimed that the opposition Labour Party would raise taxes. Early opinion polling had forecast a poor showing in the election for the Labour Party, but in early August 37-year-old Jacinda Ardern took over as Labour leader and seemingly energised younger voters.\n\nAt the 2017 general election, National won the largest share of the party vote (44.4%) and the largest number of seats (56) in the House Representatives. However, National lacked enough seats to govern alone due to two of the party's support partners, the Māori Party and United Future, losing their parliamentary seats. In response, English stated that the party would be entering into talks to form a coalition with New Zealand First. Following talks with the two largest parties, New Zealand First entered a coalition arrangement with the Labour Party. English was succeeded as prime minister by Jacinda Ardern on 26 October.\n\nOpposition (2017–2018)\n\nLeader of the Opposition\nEnglish was re-elected as National Party leader on 24 October 2017. At the time of his re-election, English announced his intention to stay on as leader until the next general election. On 13 February 2018, however, he stood down as National Party leader due to personal reasons, and instructed the party to put into motion the processes to elect a new leader. He also retired from Parliament. English's resignation followed weeks of speculation that he would step aside for a new leader. On 27 February, he was succeeded as party leader by Simon Bridges as the result of the leadership election held that day.\n\nPost-premiership \nIn 2018, English joined the board of Australian conglomerate, Wesfarmers. English serves in Chairmanships of Mount Cook Alpine Salmon, Impact Lab Ltd and Manawanui Support Ltd. He is also a director of The Instillery, Centre for Independent Studies and The Todd Corporation Limited, and is a member of the Impact Advisory Group of Macquarie Infrastructure and Real Assets.\n\nPolitical and social views\n\nEnglish is regarded as more socially conservative than his predecessor, John Key. He has stated his opposition to voluntary euthanasia and physician-assisted suicide, same-sex civil unions, and the decriminalisation of prostitution. As Prime Minister he opposed any \"liberalisation\" of abortion law.\n\nIn 2004, English voted against a bill to establish civil unions for both same-sex and opposite-sex couples. In 2005, he voted for the Marriage (Gender Clarification) Amendment Bill, which would have amended the Marriage Act to define marriage as only between a man and a woman. English voted against the Marriage (Definition of Marriage) Amendment Bill, a bill that legalised same-sex marriage in New Zealand. However, in December 2016 he stated, \"I'd probably vote differently now on the gay marriage issue. I don't think that gay marriage is a threat to anyone else's marriage\".\n\nIn 2009, English voted against the Misuse of Drugs (Medicinal Cannabis) Amendment Bill, a bill aimed at amending the Misuse of Drugs Act so that cannabis could be used for medical purposes.\n\nPersonal life \nEnglish met his future wife, Mary Scanlon, at university. She was studying medicine at the time, and became a general practitioner. Both her parents were immigrants, her father being Samoan and her mother Italian, born on the island of Stromboli. They have six children: a daughter and five sons.\n\nEnglish is a practising Roman Catholic, but has stated that he considers his religious beliefs personal and thus separate from politics.\n\nIn June 2002, English took part in TV3's Fight For Life, a celebrity boxing fundraiser to raise money for the Yellow Ribbon anti-youth-suicide campaign, influenced by the death of a teenage nephew in 1997. He lost a split decision to former university colleague Ted Clarke.\n\nHonours\nIn the 2018 Queen's Birthday Honours, English was appointed a Knight Companion of the New Zealand Order of Merit, for services of over 27 years to the State."
  }
]